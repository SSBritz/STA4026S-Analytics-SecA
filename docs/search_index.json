[["index.html", "Chapter 1 Introduction", " STA4026S  Honours Analytics Section A: Theory and Application of Supervised Learning Stefan S. Britz February 2025 Chapter 1 Introduction Welcome to an introduction to supervised learning! In this set of course notes we will: cover some of the fundamental theoretical principles underpinning supervised statistical and machine learning; explore various models, algorithms, and heuristics to analyse different types of data, both for regression (continuous target variable) and classification (categorical target variable) problems; and apply these methods in R. The aim is to find a balance between breadth of topics, depth of theory, and practical application. Since we will be covering several topics in a relatively short time, the application component will focus largely on the current best practices for implementation in R. Therefore, we will mostly be using existing R packages and will not spend time coding these algorithms from scratch, with one exception in Chapter 6. The fields of statistical learning/AI/machine learning/data science/analytics/data mining/deep learning/[insert new buzzword here] are constantly evolving at a rapid pace. Although the core theory and methodology will (should) always be relevant, adaptations to the methods are regularly being developed, along with more efficient and convenient packages for implementation. Therefore, although these notes attempt to introduce you to up-to-date modern frameworks, note that these things change over time. Also note that this is by no means an exhaustive exploration of either theory, methods, or application, but it will imbue you with a skill set with which to tackle various problems and provide a solid foundation for further learning. These notes draw from various sources, with the theoretical aspects largely relying on An Introduction to Statistical Learning with Applications in R (James et al., 2013) and Elements of Statistical Learning (Hastie et al., 2009), both of which are freely available here and here, respectively. It is recommended that you keep the former on hand, as you will be referred to sections therein for reading. Other sources will be referenced as and when they are used. Happy learning! "],["supervised-learning.html", "Chapter 2 Supervised Learning 2.1 Bias-variance trade-off 2.2 Model validation 2.3 Side note: Statistical learning vs machine learning 2.4 Homework exercises", " Chapter 2 Supervised Learning This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, some of which you should be familiar with already. Whereas there are many good sources that provide a more comprehensive discussion, Chapter 2 of James et al. (2013) is sufficient for the level and scope of this course. Using a set of observations to uncover some underlying process in the real world is the basic premise of learning from data (Abu-Mostafa et al., 2012, p. 11). By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course. The basic distinction is that supervised learning refers to cases where there is some target variable, usually indicated as \\(Y\\), whereas if we are interested in the structures and patterns in explanatory variables1 only, we refer to unsupervised learning. Given a quantitative response \\(Y\\) and a set of \\(p\\) predictors \\(X_1, X_2, \\ldots, X_p\\), we are interested in the assumed, unobserved function that maps the inputs to the outputs: \\[Y = \\underbrace{f(X)}_{systematic} + \\underbrace{\\epsilon}_{random},\\] where \\(f(.)\\) represents the fixed, but unknown function and \\(\\epsilon\\) is a random error term, independent of \\(X\\), with \\(E(\\epsilon) = 0\\). By estimating \\(f\\) such that \\[\\hat{Y} = \\hat{f}(X),\\] we allow for both prediction of \\(Y\\)  which is the primary goal in forecasting  and inference, i.e.Â describing how \\(Y\\) is affected by changes in \\(X\\). Hypothesising \\(\\hat{f}\\) can be done in two ways, namely via a parametric or a non-parametric approach. Parametric approach Here an assumption is made about the functional form of \\(f\\), for example the linear model \\[f(\\boldsymbol{X}) = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j.\\] The best estimate of \\(f\\) is now defined as the set of parameters \\(\\hat{\\boldsymbol{\\beta}}\\) that minimise some specified loss function. Given a set of observations \\(\\mathcal{D}=\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^n\\)  henceforth referred to as the training set  we could use ordinary least squares to minimise the mean squared error (MSE): \\[MSE = \\frac{1}{n}\\sum_{i=1}^n\\left[y_i - \\hat{f}(x_i)\\right]^2 \\] Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters. Non-parametric approach Another option is to make no explicit assumptions regarding the functional form of \\(f\\). This allows one to fit a wide range of possible forms for \\(f\\)  in these notes we consider K-nearest neighbours and tree-based methods  but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation. The objective remains to find \\(f\\) that fits the available data as closely as possible, whilst avoiding overfitting to ensure that the model generalises well to unseen data. Generalisation The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model, referred to as out-of-sample observations. Consider the case where the estimated function \\(\\hat{f}\\) is fixed and out-of-sample observations of the variables are introduced, which we will denote as \\(\\{\\boldsymbol{x}_0, y_0\\}\\). The expected MSE for these test set observations (see Section 2.2) can be deconstructed as follows: \\[\\begin{align} E\\left[y_0 - \\hat{f}(\\boldsymbol{x}_0) \\right]^2 &amp;= E\\left[f(\\boldsymbol{x}_0) + \\epsilon - \\hat{f}(\\boldsymbol{x}_0)\\right]^2 \\\\ &amp;= E\\left[\\left(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right)^2 + 2\\epsilon \\left(f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0) \\right) + \\epsilon^2\\right] \\\\ &amp;= E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right]^2 + 2E[\\epsilon]E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right] + E\\left[\\epsilon^2\\right] \\\\ &amp;= \\underbrace{E\\left[f(\\boldsymbol{x}_0) - \\hat{f}(\\boldsymbol{x}_0)\\right]^2}_{reducible} + \\underbrace{Var(\\epsilon)}_{irreducible} \\tag{2.1} \\end{align}\\] The primary goal of machine learning is to find an \\(\\hat{f}\\) that best approximates the underlying, unknown relationship between the input and output variables by minimising the reducible error. Note that because of the irreducible component (the noise in the data), there will always be some lower bound for the theoretical MSE, and that this bound is almost always unknown in practice (James et al., 2013, p. 19). In order to achieve accurate out-of-sample prediction, we need to specify a model (another term for the estimated function \\(\\hat{f}\\)) that is sufficiently  but not overly  complex. Finding this balance of complexity is referred to as the bias-variance trade-off. The reducible error component can be decomposed further to help illustrate this trade-off. 2.1 Bias-variance trade-off Consider again a fixed \\(\\hat{f}\\) and out-of-sample observations \\(\\{\\boldsymbol{x}_0, y_0\\}\\). For ease of notation, let \\(f = f(\\boldsymbol{x}_0)\\) and \\(\\hat{f} = \\hat{f}(\\boldsymbol{x}_0)\\). Also note that \\(f\\) is deterministic such that \\(E\\left[f\\right] = f\\). Starting with the reducible error in Equation (2.1), we have \\[\\begin{align} E\\left[f - \\hat{f} \\right]^2 &amp;= E\\left[\\hat{f} - f \\right]^2 \\\\ &amp;= E\\left[\\hat{f} - E(\\hat{f}) + E(\\hat{f}) - f \\right]^2 \\\\ &amp;= E\\left\\{ \\left[\\hat{f} - E(\\hat{f})\\right]^2 + 2\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right] + \\left[E(\\hat{f}) - f\\right]^2 \\right\\} \\\\ &amp;= E\\left[\\hat{f} - E(\\hat{f})\\right]^2 + 2E\\left\\{\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right]\\right\\} + E\\left[E(\\hat{f}) - f\\right]^2 \\\\ &amp;= Var\\left[\\hat{f}\\right] + 0 + \\left[E(\\hat{f}) - f\\right]^2 \\\\ &amp;= Var\\left[\\hat{f}\\right] + Bias^2\\left[\\hat{f}\\right] \\tag{2.2} \\end{align}\\] Showing that the crossproduct term equals zero: \\[\\begin{align} E\\left\\{\\left[\\hat{f} - E(\\hat{f})\\right] \\left[E(\\hat{f}) - f\\right]\\right\\} &amp;= E\\left[\\hat{f}E(\\hat{f}) - E(\\hat{f})E(\\hat{f}) - \\hat{f}f + E(\\hat{f})f\\right] \\\\ &amp;= E(\\hat{f})E(\\hat{f}) - E(\\hat{f})E(\\hat{f}) - E(\\hat{f})f + E(\\hat{f})f \\\\ &amp;= 0 \\end{align}\\] Therefore, in order to minimise the expected test MSE we need to find a model that has the lowest combined variance and (squared) bias. The variance represents the extent to which \\(\\hat{f}\\) changes between different randomly selected training samples taken from the same population. The bias of \\(\\hat{f}\\) is simply the error that is introduced by approximating the real-world relationship with a simpler representation. Note that since \\(f\\) is generally unknown, the bias component cannot be directly observed or measured outside of simulations. However, these simulations may help us illustrate how the bias and variance change as model complexity increases. Although the concepts of model complexity and flexibility are not necessarily perfectly defined  depending on the class of model being hypothesised  the following example should provide an intuitive understanding. 2.1.1 Example 1  Simulation To allow for easy visualisation, let us consider a simple function with only one feature: \\[Y = X + 2\\cos(5X) + \\epsilon,\\] where \\(\\epsilon \\sim N(0, 2)\\). Below we simulate \\(n = 100\\) observations from \\(X \\sim U(-2,2)\\), to which we fit cubic smoothing splines of increasing complexity. The details of splines are beyond the scope of this course, but they provide an easy-to-see illustration of flexibility. rm(list = ls()) set.seed(4026) #Simulated data n &lt;- 100 x &lt;- runif(n, -2, 2) y &lt;- x + 2*cos(5*x) + rnorm(n, sd = sqrt(2)) #The true function xx &lt;- seq(-2, 2, length.out = 1000) f &lt;- xx + 2*cos(5*xx) #Fit cubic splines with increasing degrees of freedom for(dof in 2:50){ fhat &lt;- smooth.spline(x, y, df = dof) plot(x, y, pch = 16) lines(xx, f, &#39;l&#39;, lwd = 2) lines(fhat, col = &#39;blue&#39;, lwd = 2) title(main = paste(&#39;Degrees of freedom:&#39;, dof)) legend(&#39;bottomright&#39;, c(&#39;f(x) - True&#39;, expression(hat(f)(x) ~ &#39;- Cubic spline&#39;)), col = c(&#39;black&#39;, &#39;blue&#39;), lty = 1, lwd = 2) } Figure 2.1: Cubic splines with varying degrees of freedom fitted to a sample of 100 datapoints drawn from \\(Y = X + 2\\cos(5X) + \\epsilon\\), with \\(\\epsilon \\sim N(0, 2)\\). This serves to illustrate how the models degrees of freedom are directly proportional to the models complexity. However, to extricate the bias and variance components, we need to observe these models fit on out-of-sample data across many random realisations of training samples. In the following simulation we again observe \\(n=100\\) training observations at a time, to which the same models of varying complexity as above are fitted. Each models fit is then assessed on a set of 100 testing observations, where the \\(\\boldsymbol{x}_0\\) (and, therefore, true \\(f(\\boldsymbol{x}_0)\\)) are fixed, but random noise is added. This process is repeated 1000 times, such that we can keep track of how each test observations predictions vary across the iterations, as well as the errors. set.seed(1) n &lt;- 100 #Sample size num_sims &lt;- 1000 #Number of iterations (could be parallelised) dofs &lt;- 2:25 #Model complexities var_eps &lt;- 2 #Var(epsilon): The irreducible error pred_mat &lt;- matrix(nrow = num_sims, ncol = n) #To store each set of predictions mses &lt;- vector(length = num_sims) #Also want to track the testing MSEs red_err &lt;- vector(length = num_sims) #As well as the reducible error #Herein we will capture the deconstructed components for each model results &lt;- data.frame(Var = NA, Bias2 = NA, Red_err = NA, MSE = NA) #Testing data x_test &lt;- runif(n, -2, 2) f_test &lt;- x_test + 2*cos(5*x_test) #This is the part we don&#39;t know outside sims!! d &lt;- 0 #To keep track of dof iterations, even when changing the range for(dof in dofs) { #Repeat over all model complexities d &lt;- d+1 for(iter in 1:num_sims){ #Training data x_train &lt;- runif(n, -2, 2) y_train &lt;- x_train + 2*cos(5*x_train) + rnorm(n, sd = sqrt(var_eps)) #Add the noise y_test &lt;- f_test + rnorm(n, sd = sqrt(var_eps)) #Fit cubic spline spline_mod &lt;- smooth.spline(x_train, y_train, df = dof) #Predict on OOS data yhat &lt;- predict(spline_mod, x_test)$y #And store pred_mat[iter, ] &lt;- yhat red_err[iter] &lt;- mean((f_test - yhat)^2) mses[iter] &lt;- mean((y_test - yhat)^2) } #Average each component over all iterations var_fhat &lt;- mean(apply(pred_mat, 2, var)) #E[\\hat{f} - E(\\hat{f})]^2 bias2_fhat &lt;- mean((colMeans(pred_mat) - f_test)^2) #E[E(\\hat{f}) - f]^2 reducible &lt;- mean(red_err) #E[f - \\hat{f}]^2 MSE &lt;- mean(mses) #E[y_0 - \\hat{f}]^2 results[d, ] &lt;- c(var_fhat, bias2_fhat, reducible, MSE) } #Plot the results plot(dofs, results$MSE, &#39;l&#39;, col = &#39;darkred&#39;, lwd = 2, xlab = &#39;Model complexity&#39;, ylab = &#39;&#39;, ylim = c(0, max(results))) lines(dofs, results$Bias2, &#39;l&#39;, col = &#39;lightblue&#39;, lwd = 2) lines(dofs, results$Var, &#39;l&#39;, col = &#39;orange&#39;, lwd = 2) lines(dofs, results$Red_err, &#39;l&#39;, lty = 2, lwd = 2) legend(&#39;topright&#39;, c(&#39;MSE&#39;, expression(Bias^2 ~ (hat(f))), expression(Var(hat(f))), &#39;Reducible Error&#39;), col = c(&#39;darkred&#39;, &#39;lightblue&#39;, &#39;orange&#39;, &#39;black&#39;), lty = c(rep(1, 3), 2), lwd = 2) abline(v = dofs[which.min(results$MSE)], lty = 3) #Complexity minimising MSE abline(h = var_eps, lty = 3) #MSE lower bound Figure 2.2: Averaged error components over 1000 simulations of samples of \\(n=100\\). The horizontal dashed line represents the minimum lower bound for the test MSE. The vertical dashed line indicates the point at which both the test MSE and reducible error are minimised. As a quick sanity check before interpreting this result, let us add up the components  which were calculated separately  and see whether we indeed observe that \\(E\\left[f - \\hat{f} \\right]^2 = Var\\left[\\hat{f}\\right] + Bias^2\\left[\\hat{f}\\right]\\) as per Equation (2.1), and \\(\\text{Test MSE} = E\\left[y_0 - \\hat{f}\\right]^2 = E\\left[f - \\hat{f} \\right]^2 + Var(\\epsilon)\\) as per Equation (2.2). Note that we will need to have a small tolerance for discrepancy, since we have approximated the expected values by averaging over only 1000 realisations. This approximation will become more accurate as the number of iterations is increased. #Is reducible error = var(fhat) + bias^2(fhat)? ifelse(isTRUE(all.equal(results$Red_err, results$Var + results$Bias2, tolerance = 0.001)), &#39;Happy days! :D&#39;, &#39;Haibo...&#39;) ## [1] &quot;Happy days! :D&quot; #Is Test MSE = var(fhat) + bias^2(fhat) + var(eps)? ifelse(isTRUE(all.equal(results$MSE, results$Var + results$Bias2 + var_eps, tolerance = 0.01)), &#39;Happy days! :D&#39;, &#39;Haibo...&#39;) ## [1] &quot;Happy days! :D&quot; Figure 2.2 illustrates the general error pattern when learning from data: As model complexity/flexibility increases, the variance across multiple training samples increases, whilst the (squared) bias decreases as the estimated function gets closer to the true pattern on average. Note that \\(E(\\epsilon^2) = Var(\\epsilon)\\) remains constant. This decrease in bias\\(^2\\) initially offsets the increase in variance such that the test MSE initially decreases. However, from some complexity/flexibility of \\(\\hat{f}\\), the decrease in bias\\(^2\\left(\\hat{f}\\right)\\) is offset by the increase in \\(Var\\left(\\hat{f}\\right)\\), at which point the model starts to overfit and the test MSE starts increasing. This is the bias-variance trade-off. In this particular example, we see that of all the cubic splines, one with 13 degrees of freedom most closely captures the underlying pattern in the data, as measured by the test MSE. The fundamental challenge in statistical learning is to postulate a model of the data that yields both a low bias and variance, whilst policing the model complexity such that the sum of these error components are minimised. In the above example, we knew what the underlying function was as well as the residual variance. However, when modelling data generated in some real-world environment, we do not observe \\(f\\) and therefore cannot explicitly compute the test MSE. In order to estimate the test MSE, we make use of model validation procedures. 2.2 Model validation Imagine there are two students who have been subjected to the same set of lectures, notes, and homework exercises, which you can view as their training data used to learn the true subject knowledge. When studying for the test  which is designed to test this knowledge, i.e.Â the test set in our analogy  they take two different approaches: Student A, a model student, tries to master the subject matter by focusing on the course material, testing themself with new homework exercises after studying some completed ones first. Student B, however, managed to obtain a copy of the test in advance through some nefarious means, and plans to prove their knowledge of the subject matter by preparing only for this specific set of questions. Even though student Bs test score will in all likelihood be better, does this mean that they have obtained and retained more knowledge? Certainly not! Suppose the lecturer catches wind of this cheating and swaps the initial test with a new set of randomised questions. Which of the two approaches would you expect to yield better results on such a test, on average? When comparing different statistical models, we would like to select the one that we think will work best on unseen test data. But if we use the test data to make this decision, this will also be cheating, and we will be no better off for it. Like student A though, we can leave out some exercises in the training data and use these to validate our learning, i.e.Â gauge how well we would do in the test. 2.2.1 Validation set One way to create a validation set (or hold-out set) is to just leave aside, in a randomised way, a portion of the training data, say 30%. We then train models on the other 70% of the data only, test them on the validation set, and select the model that yields the lowest validation MSE, which serves as an estimate of test set performance. Although there are some situations in which this approach is merited, it has two potential flaws: Due to the single random split, the validation estimate of the test error can be highly variable. Since we are reducing our training data, the model sees less information, generally leading to worse performance. Therefore, the validation error may overestimate the test error. We will not go into any more detail than this on the validation set approach, but rather focus on cross-validation (CV) strategies, which addresses these two issues. 2.2.2 \\(k\\)-fold CV With this approach, the training set is randomly divided into \\(k\\) groups, or folds, of (approximately) the same size. Each fold gets a turn to act as the validation set, with the model trained on the remaining \\(k-1\\) folds. Therefore, the training process is repeated \\(k\\) times, each yielding an estimate of the test error, denoted as \\(MSE_1,\\ MSE_2,\\ldots,\\ MSE_k\\). These values are then averaged over all \\(k\\) folds to yield the \\(k\\)-fold CV estimate: \\[CV_{(k)} = \\frac{1}{k}\\sum_{i=1}^k MSE_i\\] The next obvious question is: What value of \\(k\\) should we choose? Start by considering the lowest value, \\(k=2\\). This would be the same as the validation set approach with a 50% split, except that each half of the data will get a chance to act as training and validation set. Therefore, we still expect the validation error to overestimate the test error, or in other words, there will be some bias. As we increase \\(k\\), the estimated error will become more unbiased, since each fold will allow the model to capture more of the underlying pattern. However, just as with model complexity we also need to consider the variance aspect. Consider now the other extreme, when \\(k = n\\) (the number of observations in the training set). Here we have what is referred to as Leave-one-out cross-validation (LOOCV), since we have \\(n\\) folds, each leaving out just one observation for validation. Each of these \\(n\\) training sets will be almost identical, such that there will be very high correlation between them. Now, remember that when we add correlated random variables (note that averaging involves summation), then the correlation affects the resulting variance: \\[Var(X+Y) = \\sigma^2_X + \\sigma^2_Y + 2\\frac{\\rho_{XY}}{\\sigma_X\\sigma_Y}\\] Therefore, larger \\(k\\) implies larger variation of the estimated error. This means that the same bias-variance trade-off applies to \\(k\\)-fold CV! In practice, it has been shown that \\(k = 5\\) or \\(k = 10\\) yields a good balance such that the test error estimate does not suffer from excessively high bias nor variance. Also note that as \\(k\\) increases, the computational cost increases proportionally, since \\(k\\) separate models must be fitted to \\(k\\) different data splits. This could cause unnecessarily long training times for large datasets/complicated models, such that a smaller \\(k\\) might be preferable. To illustrate the implementation, let us return to the earlier example, where we will pretend that we do not know the underlying relationship we are trying to estimate. 2.2.3 Example 1  Simulation (continued) Before using the CV error to determine the ideal model complexity, let us first illustrate the concept of cross-validation for a single model, say a cubic spline with 8 degrees of freedom, with \\(k = 10\\). set.seed(4026) #Simulated data n &lt;- 100 k &lt;- 10 #We will apply 10-fold CV x &lt;- runif(n, -2, 2) y &lt;- x + 2*cos(5*x) + rnorm(n, sd = sqrt(2)) #Here we will collect the out-of-sample and in-sample errors cv_k &lt;- c() train_err &lt;- c() # We don&#39;t actually need to randomise, since x&#39;s are random, but one should in general ind &lt;- sample(n) x &lt;- x[ind] y &lt;- y[ind] folds &lt;- cut(1:n, breaks = k, labels = F) #Create indices for k folds #10-fold CV for(fld in 1:k){ x_train &lt;- x[folds != fld] #Could streamline this code, (see next block) x_valid &lt;- x[folds == fld] #but this is easier to follow y_train &lt;- y[folds != fld] y_valid &lt;- y[folds == fld] fit &lt;- smooth.spline(x_train, y_train, df = 8) valid_pred &lt;- predict(fit, x_valid)$y train_pred &lt;- predict(fit, x_train)$y cv_k &lt;- c(cv_k, mean((valid_pred - y_valid)^2)) train_err &lt;- c(train_err, mean((train_pred - y_train)^2)) #One should rather write the above into a function... #But the plotting needs to be inside the loop for the notes&#39; rendering par(mfrow = c(1, 2)) plot(x_train, y_train, pch = 16, xlab = &#39;x&#39;, ylab = &#39;y&#39;, xlim = c(min(x), max(x)), ylim = c(min(y), max(y))) points(x_valid, y_valid, pch = 16, col = &#39;gray&#39;) segments(x_valid, y_valid, x_valid, valid_pred, col = &#39;red&#39;, lty = 3, lwd = 2) lines(fit, col = &#39;blue&#39;, lwd = 2) title(main = paste(&#39;Fold:&#39;, fld)) legend(&#39;bottomright&#39;, c(&#39;Training&#39;, &#39;Validation&#39;, &#39;Errors&#39;), col = c(&#39;black&#39;, &#39;gray&#39;, &#39;red&#39;), pch = c(16, 16, NA), lty = c(NA, NA, 3), lwd = c(NA, NA, 2)) plot(1:fld, cv_k, &#39;b&#39;, pch = 16, col = &#39;red&#39;, lwd = 2, xlab = &#39;Fold&#39;, ylab = &#39;MSE&#39;, xlim = c(1, 10), ylim = c(1, 5.5)) lines(1:fld, train_err, &#39;b&#39;, pch = 16, lwd = 2) legend(&#39;topright&#39;, c(&#39;Training&#39;, &#39;Validation&#39;), col = c(&#39;black&#39;, &#39;red&#39;), lwd = 2) } Figure 2.3: Left: The training (black) and validation (grey) portions of the dataset across 10 folds, with the fitted cubic spline with 8 degrees of freedom in blue. Right: The resulting training (black) and validation (red) MSEs across 10 folds. Here we see that, as expected, the validation error is noticeably more variable than the training error across the folds. We can calculate2 the CV MSE as 2.77, although on its own this value is not particularly insightful. When comparing it to that of other models, though, we can determine which model complexity is estimated to yield the lowest test error. #Keep track of MSE per fold, per model fold_mses &lt;- matrix(nrow = 10, ncol = length(dofs)) for(fld in 1:k){ d &lt;- 0 for(dof in dofs){ #Using the same dofs as earlier d &lt;- d + 1 fit &lt;- smooth.spline(x[folds != fld], y[folds != fld], df = dof) valid_pred &lt;- predict(fit, x[folds == fld])$y fold_mses[fld, d] &lt;- mean((valid_pred - y[folds == fld])^2) } } #Average over all folds cv_mses &lt;- colMeans(fold_mses) # Compare the true MSE from earlier plot(dofs, results$MSE, &#39;l&#39;, col = &#39;darkred&#39;, lwd = 2, xlab = &#39;Model complexity&#39;, ylab = &#39;&#39;, ylim = c(0, max(cv_mses))) lines(dofs, cv_mses, &#39;l&#39;, col = &#39;grey&#39;, lwd = 2) legend(&#39;bottomright&#39;, c(&#39;CV MSE&#39;, &#39;True test MSE&#39;), col = c(&#39;gray&#39;, &#39;darkred&#39;), lty = 1, lwd = 2) abline(v = dofs[which.min(results$MSE)], lty = 3) points(dofs[which.min(cv_mses)], min(cv_mses), pch = 13, cex = 2.5, lwd = 2) Figure 2.4: 10-fold cross-validation error curve (grey) for cubic splines with varying degrees of freedom, with the minimum point indicated by the crossed circle. The red line indicates the true test MSE being estimated. Because we simulated these data, we know that the cubic spline yielding the lowest expected test MSE is one with 13 degrees of freedom. Applying 10-fold CV to our 100 training data points resulted in an estimated optimal model with 12 degrees of freedom, indicated by the crossed square in Figure 2.4. It is interesting to note that the CV error consistently overestimated the true error. This is likely due to the relatively small dataset; remember that we only tested on 10 observations per fold! The shape of the true MSE curve was captured relatively well by the CV process in this example. This section provided a succinct illustration of model validation. For a detailed discussion, see Section 5.1 of James et al. (2013). In the following chapters we will move beyond simulated data and apply these methods to various datasets as we encounter different classes of models and other techniques. Although there is much value in coding the CV procedure from scratch, it is built into various R packages, which we will leverage going forward. 2.3 Side note: Statistical learning vs machine learning It may seem that we use the terms statistical learning and machine learning interchangeably, so is there a difference? The distinction between these two concepts can sometimes be blurred with the paradigms largely overlapping, and some might argue that the difference is mostly semantic. In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance. However, the principles discussed in this chapter form the core of both approaches  both are concerned with extracting insights from data and making predictions, although they may approach these goals with slightly different philosophical and methodological perspectives. Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. While understanding the underlying data-generating process is still important in machine learning, the focus is often more on achieving optimal predictive performance. Statistical learning approaches are also characterised by explicit assumptions about the underlying statistical distributions and relationships between variables, whereas machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions. For the purposes of our study throughout this course, these distinctions are not of consequence and we will adopt both perspectives throughout. 2.4 Homework exercises For Example 1, edit the provided code such that LOOCV is applied. Does this method suggest a different model complexity? Now do the same for 5-fold CV. What changes in the CV curve do you observe as \\(k\\) changes? Often referred to as features in the learning context, or predictors in supervised learning specifically. Since there were 100 observations and 10 folds, each fold had an equal number of observations and we can calculate the CV MSE as the average of the 10 folds average errors. However, in cases where there are unequal numbers of observations across the folds, it would be more accurate to average over all of the individual observations squared errors. "],["linear-model-selection-regularisation.html", "Chapter 3 Linear Model Selection &amp; Regularisation 3.1 Linear regression models 3.2 \\(L_1\\) and \\(L_2\\) regularisation 3.3 Elastic-net 3.4 Homework exercises", " Chapter 3 Linear Model Selection &amp; Regularisation In the previous chapter we discussed cross-validation (CV) as a procedure for estimating the out-of-sample performance of models of the same form, but different complexity, where each model was considered a separate hypothesised representation of the underlying function \\(f\\) mapping all the explanatory variables (features) to the dependent (target) variable. In the following sections we will start by fitting a linear model, with the focus then on variable selection, i.e.Â deciding which features to include in the model. Instead of deciding on the model settings beforehand  which we will in later chapters come to know as hyperparameters  we will rather adjust the fitted model parameters by means of regularisation, also referred to as shrinkage methods. Following that, we will cover dimension reduction methods. This chapter is loosely based on chapter 6 of James et al. (2013) and chapter 3 of Hastie et al. (2009) and assumes some basic knowledge of linear regression models. 3.1 Linear regression models Although few real-world relationships can be considered truly linear, the linear model offers some distinct advantages, most notably in the clear interpretation of features3. Furthermore, they often perform surprisingly well on a range of problems. For some real-valued output \\(Y\\) and input vector \\(\\boldsymbol{X}&#39; = [X_1, X_2, \\ldots, X_p]\\), the model is defined as: \\[\\begin{equation} Y = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j + \\epsilon, \\tag{3.1} \\end{equation}\\] where \\(\\epsilon \\sim N(0, \\sigma^2)\\). The most popular method of estimating the regression parameters based on the training set \\(\\mathcal{D}=\\{\\boldsymbol{x}_i, y_i\\}_{i=1}^n\\), is ordinary least squares (OLS), where we find the coefficients \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_p]&#39;\\) to minimise the residual sum of squares \\[\\begin{equation} RSS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2, \\tag{3.2} \\end{equation}\\] noting that this does not imply any assumptions on the validity of the model. To minimise \\(RSS(\\boldsymbol{\\beta})\\), let us first write (3.1) in matrix form: \\[\\begin{equation} _n\\boldsymbol{Y}_1 = {}_n\\boldsymbol{X}_{(p+1)} \\boldsymbol{\\beta}_1 + {}_n\\boldsymbol{\\epsilon}_1, \\tag{3.3} \\end{equation}\\] where the first column of \\(\\boldsymbol{X}\\) is \\(\\boldsymbol{1}:n\\times1\\). Now we can write \\[\\begin{equation} RSS(\\boldsymbol{\\beta}) = \\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right)&#39;\\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right), \\tag{3.4} \\end{equation}\\] which is a quadratic function in the \\(p+1\\) parameters. Differentiating with respect to \\(\\boldsymbol{\\beta}\\) yields \\[\\begin{align} \\frac{\\partial RSS}{\\partial \\boldsymbol{\\beta}} &amp;= -2\\boldsymbol{X}&#39;\\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right) \\\\ \\frac{\\partial^2 RSS}{\\partial \\boldsymbol{\\beta}\\partial \\boldsymbol{\\beta}&#39;} &amp;= 2\\boldsymbol{X}&#39;\\boldsymbol{X} \\tag{3.5} \\end{align}\\] If \\(\\boldsymbol{X}\\) is of full column rank  a reasonable assumption when \\(n \\geq p\\)  then \\(\\boldsymbol{X}&#39;\\boldsymbol{X}\\) is positive definite. We can then set the first derivative to zero to obtain the unique solution \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}&#39;\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}&#39;\\boldsymbol{y} \\tag{3.6} \\end{equation}\\] These coefficient estimates define a fitted linear regression model. The focus of this section is on methods for improving predictive accuracy, therefore we will not cover inference on the regression parameters or likelihood ratio tests here. The following section instead answers the question: How can one simplify a regression model, either by removing covariates or limiting their contribution, in order to improve predictive performance? Before delving into regularisation methods, we will briefly note the existence of subset selection methods. Subset selection Although subject to much criticism, there are some specific conditions in which subset selection could yield satisfactory results, for instance when \\(p\\) is small and there is little to no multicollinearity. This selection can generally be done in two ways: Best subset selection This approach identifies the best fitting model across all \\(2^p\\) combinations of predictors, by first identifying the best \\(k\\)-variable model \\(\\mathcal{M}_k\\) according to RSS, for all \\(k = 1, 2, \\ldots, p\\). Stepwise selection Starting with either the null (forward stepwise) or saturated (backward stepwise) model, predictors are sequentially added or removed respectively according to some improvement metric. One can also apply a hybrid method, which considers both adding and removing a variable at each step. Typically, either Mallows \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\) is used for model comparison in subset selection. Because subset selection is a discrete process, with variables either retained or discarded, it often exhibits high variance, thereby failing to reduce the test MSE. Regularisation offers a more continuous, general-purpose and usually quicker method of controlling model complexity. Note that although the linear model is used here to illustrate the theory of regularisation, it can be applied to any parametric model. 3.2 \\(L_1\\) and \\(L_2\\) regularisation As an alternative to using least squares, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularises the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero by imposing a penalty on their size. Hence, regularisation is also referred to as shrinkage methods. This approach has the effect of significantly reducing the coefficient estimates variance, thereby reducing the variance component of the total error. The two best-known techniques for shrinking the regression coefficients towards zero, are ridge regression and the lasso. 3.2.1 Ridge regression  \\(L_2\\) Ridge regression was initially developed as a method of dealing with highly correlated predictors in regression analysis. Instead of finding regression coefficients to minimise (3.2), the ridge coefficients minimise a penalised residual sum of squares: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_R = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}}\\left\\{ \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^p\\beta_j^2 \\right\\}. \\tag{3.7} \\end{equation}\\] The complexity parameter \\(\\lambda \\geq 0\\) controls the amount of shrinkage. As \\(\\lambda\\) increases, the coefficients are shrunk towards zero, whilst \\(\\lambda = 0\\) yield the OLS. Neural networks also implement regularisation by means of penalising the sum of the squared parameters; in this context it is referred to as weight decay. The term \\(L_2\\) regularisation, also stylised as \\(\\ell_2\\), arises because the regularisation penalty is based on the \\(L_2\\) norm4 of the regression coefficients. The \\(L_2\\) norm of a vector \\(\\boldsymbol{\\beta}\\) is given by \\(||\\boldsymbol{\\beta}||_2 = \\sqrt{\\sum_{i=1}^p \\beta_i^2}\\). The optimisation problem in (3.7) can also be written as \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_R = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2, \\\\ \\text{subject to } \\sum_{j=1}^p\\beta_j^2 \\leq \\tau, \\tag{3.8} \\end{equation}\\] where \\(\\tau\\), representing the explicit size constraint on the parameters, has a one-to-one correspondence with \\(\\lambda\\) in (3.7). When collinearity exists in a linear regression model the regression coefficients can exhibit high variance, such that correlated predictors, which carry similar information, can have large coefficients with opposite signs. Imposing a size constraint on the coefficients addresses this problem. It is important to note that since ridge solutions are not equivariant under scaling of the inputs5, the inputs are generally standardised before applying this method. Note also the omission of \\(\\beta_0\\) in the penalty term. Whereas the regression coefficients depend on the predictors in the model, the bias term is a constant independent of the predictors, i.e.Â it is a property of the data that does not change as variables are added or removed. Now, if the inputs are standardised such that each \\(x_{ij}\\) is replaced by \\(\\frac{x_{ij} - \\bar{x}_j}{s_{x_j}}\\), then \\(\\beta_0\\) is estimated by \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i\\) and \\(\\beta_1, \\ldots, \\beta_p\\) are estimated by a ridge regression without an intercept, using the centered \\(x_{ij}\\). The same applies to the lasso discussed in the following section. Assuming this centering has been done, the input matrix \\(\\boldsymbol{X}\\) then becomes \\(n\\times p\\), such that the penalised RSS, now viewed as a function of \\(\\lambda\\), can be written as \\[\\begin{equation} RSS(\\lambda) = \\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right)&#39;\\left(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right) + \\lambda\\boldsymbol{\\beta}&#39;\\boldsymbol{\\beta}, \\tag{3.9} \\end{equation}\\] yielding the ridge regression solution \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}&#39;\\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}&#39;\\boldsymbol{y} \\tag{3.10}, \\end{equation}\\] where \\(\\boldsymbol{I}\\) is the \\(p \\times p\\) identity matrix. Equation (3.10) shows that the ridge regression addresses singularity issues that can arise when the predictor variables are highly correlated. The regularisation term ensures that even if \\(\\boldsymbol{X}&#39;\\boldsymbol{X}\\) is singular, the modified matrix \\(\\boldsymbol{X}&#39;\\boldsymbol{X} + \\lambda\\boldsymbol{I}\\) is guaranteed to be non-singular, allowing for stable and well-defined solutions to be obtained. Although all of the above can be neatly explored via simulated examples, our focus will now move away from this controlled environment and towards using R packages to implement this methodology on some real-world data. 3.2.2 Example 2  Prostate cancer This dataset formed part of the now retired ElemStatLearn R package; its details can be found here. The goal is to model the (log) prostate-specific antigen (lpsa) for men who were about to receive a radical prostatectomy, based on eigth clinical measurements. The data only contain 97 observations, 30 of which are set aside for testing purposes. Looking at the correlations displayed in Figure 3.1, we see that all the features are positively correlated with the response variable, ranging from relatively weak (0.23) to relatively strong (0.73) correlation. We also observe some strong correlation between features, which could be of concern for a regression model. Note! svi and gleason are actually binary and ordinal variables respectively, but we will treat them as numeric for the sake of simplicity in this illustration. library(corrplot) #For correlation plot dat_pros &lt;- read.csv(&#39;data/prostate.csv&#39;) # Extract train and test examples and drop the indicator column train_pros &lt;- dat_pros[dat_pros$train, -10] test_pros &lt;- dat_pros[!dat_pros$train, -10] corrplot(cor(train_pros), method = &#39;number&#39;, type = &#39;upper&#39;) Figure 3.1: Correlation plot for the prostate cancer data. Next, we standardise the predictors and fit a saturated linear model. library(kableExtra) library(broom) #For nice tables # Could do the following neatly with tidyverse, this is a MWE y &lt;- train_pros[, 9] #9th column is target variable x &lt;- train_pros[, -9] x_stand &lt;- scale(x) #standardise for comparison train_pros_stand &lt;- data.frame(x_stand, lpsa = y) # Fit lm using all features lm_full &lt;- lm(lpsa ~ ., train_pros_stand) lm_full %&gt;% tidy() %&gt;% kable(digits = 2, caption = &#39;Saturated linear model fitted to the prostate cancer dataset (features standardised).&#39;) Table 3.1: Saturated linear model fitted to the prostate cancer dataset (features standardised). term estimate std.error statistic p.value (Intercept) 2.45 0.09 28.18 0.00 lcavol 0.72 0.13 5.37 0.00 lweight 0.29 0.11 2.75 0.01 age -0.14 0.10 -1.40 0.17 lbph 0.21 0.10 2.06 0.04 svi 0.31 0.13 2.47 0.02 lcp -0.29 0.15 -1.87 0.07 gleason -0.02 0.14 -0.15 0.88 pgg45 0.28 0.16 1.74 0.09 The features gleason, age, and possibly pgg45 and lcp are non-significant in this model, although note that these variables in particular were highly correlated with each other. This example also illustrates the adverse effect that this multicollinearity can have on a regression model. Even though lcp was observed to have a fairly strong positive linear relationship with the response variable (r = 0.49, third highest of all features), the coefficient estimate is in fact negative, relatively significantly (p = 0.07)! Likewise, even though age is positively correlated with lpsa (r = 0.23), its coefficient estimate is negative. Let us now apply \\(L_2\\) regularisation using the glmnet package in R. See section 3.3 for the discussion of the \\(\\alpha\\) parameter. For now, note that \\(\\alpha = 0\\) corresponds to ridge regression. library(glmnet) ridge &lt;- glmnet(x, y, alpha = 0, standardize = T, lambda = exp(seq(-4, 5, length.out = 100))) plot(ridge, xvar = &#39;lambda&#39;, label = T) Figure 3.2: Coefficient profiles for ridge regression on the prostate cancer dataset Here we see how the coefficients vary as \\(\\log(\\lambda)\\) is increased, whilst the labels at the top indicate the number of nonzero coefficients for various values of \\(\\log(\\lambda)\\). Note that none of the coefficients actually equal zero, illustrating that ridge regression does not necessarily perform variable selection per se. In Figure 3.2 we observe that the initially negative coefficient for lcp \\((\\beta_6)\\) becomes both positive and more significant, relative to the other predictors. Therefore, the notion of coefficients shrinking towards zero is a slight misnomer, or perhaps an oversimplification of the effect \\(L_2\\) regularisation has. Eventually, as \\(\\lambda \\to \\infty\\) (or, equivalently, \\(\\tau \\to 0\\) as shown in (3.8)), all coefficients will indeed be forced towards zero. However, the ideal model will usually correspond to a level of \\(\\lambda\\) that allows stronger predictors to be more prominent by diminishing the effect of their correlates. So, how do we determine the appropriate level of \\(\\lambda\\)? By viewing this tuning parameter as a proxy for complexity and applying the same approach as in Chapter 2, we can use CV with the MSE as loss function to identify optimal complexity. #Apply 10-fold CV set.seed(1) ridge_cv &lt;- cv.glmnet(as.matrix(x), y, #this function requires x to be a matrix alpha = 0, nfolds = 10, type.measure = &#39;mse&#39;, standardize = T, lambda = exp(seq(-4, 5, length.out = 100))) #Default lambda range doesn&#39;t cover minimum plot(ridge_cv) abline(h = ridge_cv$cvup[which.min(ridge_cv$cvm)], lty = 2) Figure 3.3: 10-fold CV MSEs as a function of \\(\\log(\\lambda)\\) for ridge regression applied to the prostate cancer dataset Figure 3.3 shows the CV errors (red dots), with the error bars indicating the extent of dispersion of the MSE across folds, the default display being one standard deviation above and below the average MSE. Two values of the tuning parameter are highlighted: the one yielding the minimum CV error (lambda.min), and the one corresponding to the most regularised model such that the error is within one standard error of the minimum (lambda.1se), which has been indicated on this plot with the horizontal dashed line. The choice of \\(\\lambda\\) depends on various factors, including the size of the data set, the length of the resultant error bars, and the profile of the coefficient estimates. In Figure 3.2 we saw that a more reasonable representation of the coefficients is achieved when \\(\\log(\\lambda)\\) is closer to zero, rather than at the minimum CV MSE. Showing this explicitly, below we see that the coefficients corresponding to lambda.min (left) still preserves the contradictory coefficient sign for lcp, whereas lambda.1se (right) rectifies this whilst mostly maintaining the overall relative importance across the features, hence we will use the latter. round(cbind(coef(ridge_cv, s = &#39;lambda.min&#39;), coef(ridge_cv, s = &#39;lambda.1se&#39;)), 3) ## 9 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 s1 ## (Intercept) 0.173 -0.181 ## lcavol 0.515 0.284 ## lweight 0.606 0.470 ## age -0.016 -0.002 ## lbph 0.140 0.099 ## svi 0.696 0.492 ## lcp -0.140 0.038 ## gleason 0.007 0.071 ## pgg45 0.008 0.004 Note that although some predictors have almost been removed, these coefficients are still nonzero. Therefore, the ridge regression will include all \\(p\\) predictors in the final model. The CV MSE for the chosen model, which is an estimate of out-of-sample performance, is 0.645. Before using the ridge regression to predict values for the test set, we will first consider the lasso as an approach for variable selection. 3.2.3 The Lasso  \\(L_1\\) Lasso is an acronym that stands for least absolute shrinkage and selection operator. It is another form of regularisation that, similar to ridge regression, attempts to minimise a penalised RSS. However, the constraint is slightly different: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_L = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2, \\\\ \\text{subject to } \\sum_{j=1}^p|\\beta_j| \\leq \\tau. \\tag{3.11} \\end{equation}\\] Or, written in its Lagrangian form: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_L = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}}\\left\\{ \\sum_{i=1}^n\\left( y_i - \\beta_0 - \\sum_{j=1}^px_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}. \\tag{3.12} \\end{equation}\\] Again we can see that the equivalent name \\(L_1\\) regularisation arises from the fact that the penalty is based on the \\(L_1\\) norm6 \\(||\\boldsymbol{\\beta}||_1 = \\sum_{i=1}^p |\\beta_i|\\). This constraint on the regression parameters makes the solutions nonlinear in the \\(y_i\\), such that there is no closed form expression for \\(\\hat{\\boldsymbol{\\beta}}_L\\) like there is for the ridge estimate, except in the case of orthonormal covariates. Computing the lasso estimate is a quadratic programming problem, although efficient algorithms have been developed to compute the solutions as a function of \\(\\lambda\\) at the same computational cost as for ridge regression. These solutions are beyond the scope of this course. Note that if we let \\(\\tau &gt; \\sum_{j=1}^p|\\hat{\\beta}^{LS}_j|\\), where \\(\\hat{\\beta}^{LS}_j\\) denotes the least squares estimates, then the lasso estimates are exactly equal to the least squares estimates. If, for example, \\(\\tau = \\frac{1}{2} \\sum_{j=1}^p|\\hat{\\beta}^{LS}_j|\\), then the least squares coefficients are shrunk by 50% on average. However, the nature of the shrinkage is not obvious. When comparing ridge regression with the lasso, we see that the nature of the constraints yield different trajectories for \\(\\hat{\\boldsymbol{\\beta}}\\) as \\(\\lambda\\) increases/\\(\\tau\\) decreases: Figure 3.4: Estimation picture for the lasso (left) and ridge regression (right). Shown are contours of the error and constraint functions. The solid blue areas are the constraint regions \\(|\\beta_1| + |\\beta_2| \\leq \\tau\\) and \\(\\beta_1^2 + \\beta_2^2 \\leq \\tau^2\\), respectively, while the red ellipses are the contours of the least squares error function. Source: Hastie et al. (2009), p.Â 71. As the penalty increases, the lasso constraint sequentially forces the coefficients across the p dimensions onto their respective axes. Let us return to the previous example to illustrate this effect. 3.2.4 Example 2  Prostate cancer (continued) Applying \\(L_1\\) regularisation via glmnet follows exactly the same process as for ridge regression, except that we now set \\(\\alpha = 1\\) within the glmnet() function. library(glmnet) lasso &lt;- glmnet(x, y, alpha = 1, standardize = T) plot(lasso, xvar = &#39;lambda&#39;, label = T) Figure 3.5: Coefficient profiles for lasso regression on the prostate cancer dataset Figure 3.5 shows the coefficients shrinking and equaling zero as the regularisation penalty increases, as opposed to gradually decaying as in ridge regression, thereby performing variable selection in the process. Interestingly, here it seems that one of the first variables excluded from the model is lcp, although it is quite difficult to see, even for this small example where \\(p=8\\). In order to determine which variables should be (de)selected, we will again implement CV using the MSE as loss function. #Apply 10-fold CV set.seed(1) lasso_cv &lt;- cv.glmnet(as.matrix(x), y, #this function requires x to be a matrix alpha = 1, nfolds = 10, type.measure = &#39;mse&#39;, standardize = T) plot(lasso_cv) Figure 3.6: 10-fold CV MSEs as a function of \\(\\log(\\lambda)\\) for lasso regression applied to the prostate cancer dataset We can now achieve a notably simpler model where three of the eight coefficients have been shrunk to zero by once again selecting the penalty corresponding to the largest MSE within one standard error of the minimum MSE, as opposed to the minimum MSE where the contradictory estimates will clearly still remain. round(coef(lasso_cv, s = &#39;lambda.1se&#39;), 3) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 0.083 ## lcavol 0.459 ## lweight 0.454 ## age . ## lbph 0.048 ## svi 0.349 ## lcp . ## gleason . ## pgg45 0.001 Once again, the CV MSE for the chosen model is 0.645. At this point we have defined a new \\(\\hat{f}\\), which is ultimately still a linear model with slightly adjusted coefficient estimates. We can now compare these different versions of the proposed linear model  i.e.Â the OLS, ridge and lasso models  by looking at their CV MSEs and selecting the one that performed best. In this instance there is very little difference between the two regularised models, therefore we would favour the more parsimonious model (lasso). As a way of grading our decision, we will now take a peek at each models performance on the test set, shown in Table 4.3. To be clear: the test set should NOT be used to inform the choice of model parameters. This decision should be made before using the test set, which is then used to measure the selected models fit. Below we are calculating the test MSE for each model for illustrative purposes, in essence evaluating three different hypothetical decisions that had already been made. test_y &lt;- test_pros[, 9] test_x &lt;- as.matrix(test_pros[, -9]) #need to extract just the x&#39;s for glmnet predict function test_x_stand &lt;- scale(test_x) #standardise for lm test_pros_stand &lt;- data.frame(test_x_stand, lpsa = test_y) #Yhats ols_pred &lt;-predict(lm_full, test_pros_stand) ridge_pred &lt;- predict(ridge_cv, test_x, s = &#39;lambda.1se&#39;) lasso_pred &lt;- predict(lasso_cv, test_x, s = &#39;lambda.1se&#39;) #Test MSEs ols_mse &lt;- mean((test_y - ols_pred)^2) ridge_mse &lt;- mean((test_y - ridge_pred)^2) lasso_mse &lt;- mean((test_y - lasso_pred)^2) comparison_table &lt;- data.frame( Model = c(&#39;OLS&#39;, &#39;Ridge&#39;, &#39;Lasso&#39;), MSE = c(ols_mse, ridge_mse, lasso_mse) ) kable(comparison_table, digits = 3, caption = &#39;Prostate cancer data test set MSEs for the saturated, L2-regularised, and L1-regularised linear models.&#39;) (#tab:lm_comp)Prostate cancer data test set MSEs for the saturated, L2-regularised, and L1-regularised linear models. Model MSE OLS 0.549 Ridge 0.509 Lasso 0.454 The final results show that the unregularised linear model performed worst, and the lasso performed best in this example. It should be noted that the accuracy of these predictions in context of the application should always be evaluated in consultation with the subject experts, i.e.Â the oncologist in this instance. 3.3 Elastic-net Although both of the regularisation approaches above can be effective in certain situations, each has its limitations: Lasso struggles when predictors are highly correlated, often selecting only one variable from a group while ignoring others. Ridge, on the other hand, retains all predictors but does not perform automatic variable selection. The elastic-net penalty attempts to address these issues by incorporating both \\(L_1\\) and \\(L_2\\) penalties, providing a balance between sparsity and stability. First proposed by Zou &amp; Hastie (2005), the elastic-net penalty term is defined as follows: \\[\\begin{equation} \\text{penalty} = \\lambda \\left[ (1-\\alpha)\\left(\\sum_{j=1}^p \\beta_j^2\\right) + \\alpha\\left(\\sum_{j=1}^p |\\beta_j|\\right) \\right] \\tag{3.13} \\end{equation}\\] Note that the \\(\\alpha\\) terms have actually been swapped around here in order to correspond with the application in glmnet. The elastic-net selects variables like the lasso, and shrinks together the coefficients of correlated predictors like ridge. The effect of \\(\\alpha\\) on the constraint function is illustrated in Figure 3.7. #Set up a range of alphas between 0 and 1 alphas &lt;- seq(0, 1, 0.1) for(alpha in alphas){ beta1 &lt;- beta2 &lt;- seq(-1, 1, 0.01) grid &lt;- expand.grid(beta1 = beta1, beta2 = beta2) #Define elastic-net constraint tau &lt;- sqrt((1 - alpha) * (grid$beta1^2 + grid$beta2^2) + alpha * (abs(grid$beta1) + abs(grid$beta2))) tau_matrix &lt;- matrix(tau, nrow = length(beta1)) contour(beta1, beta2, tau_matrix, levels = 1, drawlabels = FALSE, main = bquote(&#39;Elastic-net with&#39; ~ alpha ~ &#39;=&#39; ~ .(alpha)), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5), lwd = 2, col = &#39;lightblue&#39;, xlab = expression(beta[1]), ylab = expression(beta[2])) abline(h = 0, v = 0) } Figure 3.7: The constraint function of elastic-net regularisation for different values of \\(\\alpha\\). Therefore, there are now two hyperparameters to tune simultaneously, and the choice of \\(\\alpha\\) influences the range of \\(\\lambda\\) values we should consider searching over  compare the x-axis ranges of Figures 3.3 and 3.6. The glmnetUtils package  although with some room for improvement at time of writing  provides a convenient way of doing so. library(glmnetUtils) set.seed(1) elasticnet &lt;- cva.glmnet(lpsa ~ ., train_pros, alpha = seq(0, 1, 0.1)) plot(elasticnet) Figure 3.8: Elastic-net regularisation profiles for the prostate cancer dataset. In Figure 3.8 we see that  for higher values of the regularisation parameter \\(\\lambda\\)  the cross-validated MSE improves as \\(\\alpha\\) decreases, i.e.Â as \\(L_2\\) is weighted more. However, if accuracy of prediction is priority, we are interested in the combination of hyperparameters that yields the lowest CV MSE. It is clear that, for this example at least, there is very little difference in the performance of the best models across the different \\(\\alpha\\) values. Regardless, the best combination can be extracted from the resulting model object: alphas &lt;- elasticnet$alpha #Just extracting the alphas we specified cv_mses &lt;- sapply(elasticnet$modlist, function(mod) min(mod$cvm) #Across the list of models, extract the minimum CV MSE ) best_alpha &lt;- alphas[which.min(cv_mses)] #Alpha corresponding to this minumum It is crucial to reiterate that for this particular example, the difference in CV performance across the \\(\\alpha\\) hyperparameter is close to negligible: plot(alphas, cv_mses, &#39;b&#39;, lwd = 2, pch = 16, col = &#39;navy&#39;, xlab = expression(alpha), ylab = &#39;CV MSE&#39;, ylim = c(0.57, 0.58)) #Scale is crucial, this is still very granular! abline(v = best_alpha, lty = 3, col = &#39;red&#39;) Figure 3.9: Minimum CV MSEs across a range of \\(\\alpha\\) values for the prostate cancer dataset. Nevertheless, the minimum CV MSE is 0.5731454, corresponding to \\(\\alpha =\\) 0, i.e.Â pure ridge regression. A key consideration for the data scientist is whether this slight improvement in estimated test performance is justified. Earlier, we reasoned that the ridge regularisation penalty corresponding to the minimum CV error does not correspond with the correlation patterns in the data. This emphasises the need to fully interrogate the data, and not just apply these methods in a generic way. 3.4 Homework exercises Show that the penalised RSS for the ridge regression yields \\(\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}&#39;\\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}\\boldsymbol{y}\\). In this context, statisticians often prefer the term co-variates. Also called the Euclidean norm, or the length of a vector in Euclidean space. In other words, the big weights are shrunk more than the small weights, and when rescaling the features the relative sizes of the weights change. Also referred to as the Manhattan norm/distance. "],["classification-models.html", "Chapter 4 Classification Models 4.1 Logistic regression 4.2 Model evaluation 4.3 Regularisation 4.4 Homework exercises", " Chapter 4 Classification Models Thus far we have only been considering numeric response variables, generally referred to as regression problems. In this chapter we discuss classification, i.e.Â supervised learning problems where the response variable is categorical. More specifically, we will focus on methods of evaluating a classification models performance. For the sake of illustration in this chapter we will only consider the logistic regression as linear classifier, although other classification models will be introduced in later chapters. The following section is based in part on section 4.3 of James et al. (2013). For now, let us consider a binary classification task, for example: Determining whether a person will test positive or negative for the SARS-CoV-2 virus, based on a range of physiological metrics. Identifying a bank transaction as either fraudulent or legitimate on the basis of the users IP address, past transaction history, etc. Classifying email as spam or legitimate, depending on the features of the content and sender. We will code the target variable as follows: \\[Y = \\begin{cases} 1 &amp; \\text{if the outcome is } `` \\text{class A&quot;}\\\\ 0 &amp; \\text{if the outcome is } `` \\text{class B&quot;}\\\\ \\end{cases}\\] Given data from some set of \\(p\\) features, we would like to define a decision rule based on a p-dimensional decision boundary that splits the feature space into two prediction regions corresponding to classes \\(A\\) and \\(B\\) respectively. When this decision boundary is linear  i.e.Â a straight line in two dimensions; a plane when \\(p = 3\\); and a hyperplane for higher dimensions  then the classifier yielding this decision boundary is considered linear. Some of the most widely applied linear classifiers include linear discriminant analysis, naive Bayes, support vector machine (with linear kernel), the perceptron, and logistic regression. 4.1 Logistic regression In the previous chapter we touched on the benefits provided by the simplicity of a linear model, particularly in interpreting the regression coefficients. Using that same, simple additive structure: \\[\\begin{equation} g(\\boldsymbol{X}) = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j, \\tag{3.1} \\end{equation}\\] we would like to relate the linear combination of input parameters to a classification of 0 or 1. We do so by modelling the probability that an observation belongs to some reference class: \\[\\Pr(Y = j|\\boldsymbol{X} = \\boldsymbol{x}) = \\begin{cases} p(\\boldsymbol{x}) &amp; \\mbox{if }j = 1\\\\ 1 - p(\\boldsymbol{x}) &amp; \\mbox{if }j = 0\\\\ \\end{cases},\\] or in other words, \\(Y \\sim \\text{Bernoulli}(p)\\). Therefore, \\(p(\\boldsymbol{x})\\) represents the probability that an observation is in class A given \\(\\boldsymbol{X} = \\boldsymbol{x}\\). Now we need to define an appropriate function in order to map the linear function \\(g(X)\\) in Equation (3.1) to \\(p(x) \\in [0, 1]\\). One option is the logistic function, yielding a logistic regression model: \\[\\begin{equation} p(\\boldsymbol{X}) = \\frac{e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}. \\tag{4.1} \\end{equation}\\] Also referred to as the logistic map or sigmoidal curve, this S-shaped function has asymptotes at 0 and 1, as later examples will illustrate. Through some simple manipulation, we can rewrite (4.1) as follows: \\[\\begin{equation} \\log \\left( \\frac{p(\\boldsymbol{X})}{1 - p(\\boldsymbol{X})} \\right) = \\beta_0 + \\sum_{j=1}^p\\beta_jX_j. \\tag{4.2} \\end{equation}\\] The left-hand is referred to as the log odds or logit, and here we see that the logistic regression model has a logit that is linear in \\(\\boldsymbol{X}\\). Consider now the exponent of the log odds, i.e.Â the odds: \\[\\begin{equation} \\text{odds} = \\frac{p(\\boldsymbol{x})}{1 - p(\\boldsymbol{x})} = \\exp(\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p). \\tag{4.3} \\end{equation}\\] This quantity  which we note can take on any value from 0 to \\(\\infty\\)  can be interpreted as the odds that \\(Y = 1\\) vs \\(Y = 0\\), given \\(\\boldsymbol{x}\\). In other words, how many times more likely we are to observe \\(Y = 1\\) than \\(Y = 0\\) for that particular set of \\(\\boldsymbol{x}\\) values. Before considering how to interpret the individual regression coefficients, we will briefly discuss their estimation. 4.1.1 Estimation The most common approach for estimating the parameters \\(\\boldsymbol{\\beta}\\) is by way of maximising the likelihood. The likelihood, a function of the parameters, is simply the joint probability of observing the responses: \\[\\begin{align} L(\\boldsymbol{\\beta}) &amp;= \\Pr(Y = y_1|\\boldsymbol{X} = \\boldsymbol{X}_1)\\times \\Pr(Y = y_2|\\boldsymbol{X} = \\boldsymbol{x}_2)\\times \\cdots \\times \\Pr(Y = y_n|\\boldsymbol{X} = \\boldsymbol{x}_n) \\\\ &amp;= \\prod_{i=1}^np(\\boldsymbol{x}_i)^{y_i}(1 - p(\\boldsymbol{x}_i))^{1 - y_i} \\tag{4.4} \\end{align}\\] Maximising this quantity is akin to maximising the log-likelihood: \\[\\begin{equation} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i\\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)) \\right], \\tag{4.5} \\end{equation}\\] or, equivalently, minimising the negative log-likelihood (NLL), also referred to as the cross-entropy error function. Finding the maximum is done in the usual way of setting the derivative of the log-likelihood equal to zero, yielding the so-called score equations. Since these equations are nonlinear in \\(\\boldsymbol{\\beta}\\), solving for \\(\\hat{\\boldsymbol{\\beta}}\\) requires an optimisation procedure such as iteratively reweighted least squares. The details of this procedure are beyond the scope of this course, but can be found in pages 120-121 of Hastie et al. (2009). 4.1.2 Interpretation The model parameters (regression coefficients) can be interpreted in a similar way to the linear model, by considering how a specific change in a coefficients corresponding parameters relates to the outcome. The simplest way of considering this effect is via the odds, given in Equation (4.3). For example, consider the case where \\(X_1\\) increases by one unit, i.e.Â from \\(x_1\\) to \\(x_1 + 1\\). Then: \\[\\begin{align} \\text{odds}(X_1 = x_1 + 1) &amp;= \\exp(\\beta_0 + \\beta_1(x_1 + 1) + \\cdots + \\beta_px_p) \\\\ &amp;= e^{\\beta_1}\\exp(\\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p) \\\\ &amp;= e^{\\beta_1}\\text{odds}(X_1 = x_1) \\tag{4.6} \\end{align} \\] Therefore, increasing \\(X_1\\) by one unit (whilst holding all predictors constant), increases the odds of \\(Y = 1\\) by a factor of \\(e^{\\beta_1}\\). Note that the linear predictor may still take predictors with various measurement scales, including categorical variables represented by dummy variables, in which case we can interpret the change in odds for a specific category of the predictor against the reference category. 4.1.3 Prediction Prediction for logistic regression is carried out in exactly the same way as with linear regression. We use the parameter estimates to evaluate the probabilities associated with each outcome: \\[\\begin{equation} \\hat{\\Pr}(Y=1|\\boldsymbol{X}) = \\hat{p}(\\boldsymbol{X}) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X_1+\\cdots+\\hat{\\beta}_pX_p}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X_1+\\cdots+\\hat{\\beta}_pX_p}} \\end{equation} \\tag{4.7}\\] In order to classify a new observation as belonging to a specific class, we need to apply a decision rule to this probability. For multiclass classification with \\(K\\) different levels, we will classify an observation to the class with the highest estimated probability: \\[\\hat{Y} = \\underset{k}{\\text{argmax}} \\left[ \\hat{p}_k(\\boldsymbol{X}) \\right],\\] where \\(k = 1, \\ldots, K\\). In the binary case, this implies that the decision rule has a threshold of 0.5: \\[\\begin{equation} \\hat{Y} = \\begin{cases} 1 &amp; \\mbox{if } \\hat{p}(\\boldsymbol{X}) \\geq 0.5\\\\ 0 &amp; \\mbox{if } \\hat{p}(\\boldsymbol{X}) &lt; 0.5\\\\ \\end{cases} \\end{equation} \\tag{4.8}\\] We will return to this decision rule when considering model evaluation in section 4.2. The threshold determines the exact decision boundary, although for logistic regression this boundary will always be linear, since the logit function is linear in the predictors. The decision boundary is best illustrated by means of an example. 4.1.4 Example 3  Default data This example entails the simulated (fictitious) dataset Default from the ISLR package. In this binary classification problem, we aim to predict whether or not an individual will default on their credit card payment based on three features: student: Whether they are a student (yes/no) balance: The average balance that the customer has remaining on their credit card after making their monthly payment income: Annual income Exploration Ignoring the student variable for now, we see a very strong (somewhat exaggerated!) relationship between balance and default. Since the overall default rate is approximately 3%, we will only plot a random subsample of the non-defaulters, similar to what was done in Figure 4.1 of James et al. (2013). library(ISLR) data(Default) def_prop &lt;- mean(as.numeric(Default$default) - 1) #Proportion of defaulters...only 0.0333 #Let&#39;s plot non-defaulters to defaulters at a 4:1 ratio def_rows &lt;- which(Default$default == &#39;No&#39;) set.seed(4026) Default_plot &lt;- Default[-sample(def_rows, length(def_rows)*(1 - def_prop*4)),] # Using base R plotting plot(Default_plot$balance, Default_plot$income, col = ifelse(Default_plot$default == &#39;Yes&#39;, &#39;darkorange&#39;, &#39;lightblue&#39;), pch = ifelse(Default_plot$default == &#39;Yes&#39;, 3, 1), xlab = &#39;Balance&#39;, ylab = &#39;Income&#39;) legend(&#39;topright&#39;, c(&#39;Default&#39;, &#39;Non-default&#39;), col = c(&#39;darkorange&#39;, &#39;lightblue&#39;), pch = c(3, 1)) Figure 4.1: Annual incomes and monthly credit card balances of a subsample from the Default dataset library(ggplot2) library(gridExtra) #For grid.arrange() - to combine multiple ggplots in one plot # Using ggplot2: box_bal &lt;- ggplot(Default_plot, aes(x = default, y = balance, group = default)) + geom_boxplot(aes(fill = default)) + scale_fill_manual(values=c(&#39;lightblue&#39;, &#39;darkorange&#39;)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), legend.position = &#39;none&#39;) box_inc &lt;- ggplot(Default_plot, aes(x = default, y = income, group = default)) + geom_boxplot(aes(fill = default)) + scale_fill_manual(values=c(&#39;lightblue&#39;, &#39;darkorange&#39;)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), legend.position = &#39;none&#39;) grid.arrange(box_bal, box_inc, ncol = 2) Figure 4.2: Boxplots of balance and income as a function of default status Model fit Now lets fit a logistic regression model to the entire dataset, using all three features, and print the results. library(kableExtra) library(broom) #For nice tables log_mod &lt;- glm(default ~ ., data = Default, family = binomial) log_mod %&gt;% tidy() %&gt;% kable(digits = 2, caption = &#39;Summary of logistic regression model fitted to the Default dataset&#39;) %&gt;% kable_styling(full_width = F) Table 4.1: Summary of logistic regression model fitted to the Default dataset term estimate std.error statistic p.value (Intercept) -10.87 0.49 -22.08 0.00 studentYes -0.65 0.24 -2.74 0.01 balance 0.01 0.00 24.74 0.00 income 0.00 0.00 0.37 0.71 As expected based on the visual exploration, we observe that balance is a highly significant predictor, whilst income is decidedly non-significant. The student indicator variable is also highly significant. Interpretation In order to interpret the regression coefficients, we calculate the exponent thereof. exp(coef(log_mod)) %&gt;% tidy() %&gt;% kable(digits = 3,col.names = c(&#39;$X_j$&#39;, &#39;$e^{\\\\beta_j}$&#39;), escape = F, caption = &#39;Odds effects for the logistic regression model fitted to the Default dataset&#39;) %&gt;% kable_styling(full_width = F) Table 4.2: Odds effects for the logistic regression model fitted to the Default dataset \\(X_j\\) \\(e^{\\beta_j}\\) (Intercept) 0.000 studentYes 0.524 balance 1.006 income 1.000 The coefficient corresponding to studentYes is with regards to the reference category No. Therefore, a student is approximately half as likely to default on their payment than a non-student when holding balance and income constant. A one unit increase in the account balance increases the odds of defaulting by a factor of 1.006. Note, however, the scale of measurement  a one dollar increase amounts to very little. Instead consider that for every 100 unit increase in balance, the odds of defaulting increase by a factor of \\(e^{100\\beta_2} =\\) 1.775. In section 4.3 we show how to use the model for prediction on a different example. First, however, let us illustrate what is meant by the decision boundary being linear, together with a visual depiction of the logistic regression curve. 4.1.5 Decision boundaries Continuing with the previous example, first consider the case where \\(p = 1\\), where we only include the balance variable. Applying the decision rule shown in Equation (4.8), i.e.Â applying a threshold predicted probability of 0.5, we observe the following fit and classifications: # 1-dimensional fit mod1 &lt;- glm(default ~ balance, &#39;binomial&#39;, Default) # Calculate curve values from fit Y &lt;- as.numeric(Default$default) - 1 x_range &lt;- seq(0, max(Default$balance)*1.2, length.out = 1000) coefs_1 &lt;- mod1$coefficients logit_1 &lt;- coefs_1[1] + coefs_1[2]*x_range sigmoid &lt;- exp(logit_1)/(1 + exp(logit_1)) # Plot the fit plot(x_range, sigmoid, type = &#39;l&#39;, lwd = 2, col = &#39;gray&#39;, xlab = &#39;Balance&#39;, ylab = &#39;p(x)&#39;) tau &lt;- 0.5 #Threshold x_cutoff &lt;- (log(tau/(1-tau)) - coefs_1[1])/(coefs_1[2]) abline(v = x_cutoff, lwd = 2, col = &#39;navy&#39;) points(x_cutoff, 0, pch = 4, col = &#39;navy&#39;, cex = 2) segments(-500, tau, x_cutoff, tau, col = &#39;gray&#39;, lty = 4) points(Default$balance, Y, pch = 16, col = ifelse(Default$balance &lt; x_cutoff, &#39;green&#39;, &#39;black&#39;), cex = 0.5) legend(&#39;right&#39;, c(expression(paste(hat(Y), &#39; = 1&#39;)), &#39;&#39;, expression(paste(hat(Y), &#39; = 0&#39;))), pch = 16, col = c(&#39;black&#39;, &#39;white&#39;, &#39;green&#39;)) Figure 4.3: Logistic curve fitted to the default dataset using only balance as predictor. A decision rule based on a threshold of 0.5 is applied. For the 1-dimensional case, the decision boundary is just a single point. In this instance, observations with balance \\(\\geq\\) 1937 will yield \\(\\hat{p}(x) \\geq 0.5\\) and will be classified as defaulters. The linearity of the decision boundary is better illustrated for \\(p = 2\\), so lets add the income variable to the model. # 2-dimensional fit mod2 &lt;- glm(default ~ balance + income, &#39;binomial&#39;, Default) coefs_2 &lt;- coef(mod2) x1s &lt;- seq(0, max(Default$balance)*1.2, length.out = 100) x2s &lt;- seq(min(Default$income)*0.8, max(Default$income)*1.2, length.out = 100) sigmoid2 &lt;- function(x1s, x2s) exp(cbind(1, x1s, x2s)%*%coef(mod2))/(1 + exp(cbind(1, x1s, x2s)%*%coef(mod2))) # Plot the curve plot2 &lt;- persp3d(x1s, x2s, outer(x1s, x2s, sigmoid2), aspect = c(1, 1, 0.5), col = &#39;gray&#39;, alpha = 0.5, xlab = &#39;balance&#39;, ylab = &#39;income&#39;, zlab = &#39;p(x)&#39;) view3d(theta = 0, phi = -75, fov = 45, zoom = 0.75) #Initial view settings #Add vertical plane, just based on g(X) = 0 planes3d(coefs_2[2], coefs_2[3], 0, coefs_2[1], col = &#39;navy&#39;, lwd = 3, alpha = 0.5) # Lines are tricky... x2min &lt;- min(x2s) x1min &lt;- -(coefs_2[1] + coefs_2[3]*x2min)/coefs_2[2] abclines3d(x1min, x2min, 0, -1, coefs_2[2]/coefs_2[3], 0, col = &#39;navy&#39;, lwd = 3) abclines3d(x1min, x2min, 0.5, -1, coefs_2[2]/coefs_2[3], 0, col = &#39;navy&#39;, lwd = 3) #Why doesn&#39;t this add the lines??? # Use predict() for the classifications p2 &lt;- predict(mod2, type = &#39;response&#39;) points3d(Default$balance, Default$income, Y, col = ifelse(p2 &gt;= 0.5, &#39;green&#39;, &#39;black&#39;), size = 6) Note that this figure is interactive in html. Here we can see that the contour on the sigmoid surface corresponding to a constant threshold yields a straight line. Projecting this line orthogonally onto the xy-plane together with the observations yields our original plot of the data in Figure 4.1 (this time with all the data, not just a subset) with the resulting linear decision boundary. # Same plot as above, but all the data plot(Default$balance, Default$income, col = ifelse(Default$default == &#39;Yes&#39;, &#39;darkorange&#39;, &#39;lightblue&#39;), pch = ifelse(Default$default == &#39;Yes&#39;, 3, 1), xlab = &#39;Balance&#39;, ylab = &#39;Income&#39;) legend(&#39;topright&#39;, c(&#39;Default&#39;, &#39;Non-default&#39;), col = c(&#39;darkorange&#39;, &#39;lightblue&#39;), pch = c(3, 1)) # Add the decision boundary abline(-coefs_2[1]/coefs_2[3], -coefs_2[2]/coefs_2[3], col = &#39;navy&#39;, lwd = 3) #Do the math!! Figure 4.4: The logistic regression decision boundary for the default dataset using balance and income as predictors. Note that the decision boundary for the decision rule threshold \\(\\hat{p}(\\boldsymbol{X}) \\geq 0.5\\) corresponds to the set of points in the parameters space for which the odds are 1 (or, equivalently, the log-odds are zero). Showing this is left as a homework exercise. Using this fact allows us to easily determine the boundary in Figure 4.4. Finally, we will now add the student variable (coded as 0 and 1) to the model. The following plot shows a 3-dimensional image of the data, with the decision boundary added as a plane. Note that the sigmoidal curve will be 4-dimensional here. # 3-dimensional fit mod3 &lt;- glm(default ~ balance + income + student, &#39;binomial&#39;, Default) coefs_3 &lt;- coef(mod3) # Plot the data plot3d(Default$student, Default$balance, Default$income, xlab = &#39;student&#39;, ylab = &#39;balance&#39;, zlab = &#39;income&#39;, col = ifelse(Default$default == &#39;Yes&#39;, &#39;orange&#39;, &#39;lightblue&#39;), size = 5) view3d(theta = 45, phi = 45, fov = 45, zoom = 0.9) #Initial view settings # Add the decision boundary. Easy with planes3d, since it is set up similarly planes3d(coefs_3[4], coefs_3[2], coefs_3[3], coefs_3[1], col = &#39;navy&#39;, lwd = 3, alpha = 0.5) Note that this figure is interactive in html. Now that we have sufficiently explored the logistic regressions decision boundaries, we turn our attention to measuring the accuracy of the classifications resulting from these decision boundaries. 4.2 Model evaluation Any classification method yields a set of predicted outcomes that can be compared to the observed outcomes to quantify the models performance. The simplest loss function for measuring a classification models accuracy is the misclassification rate, which is simply the proportion of observations for which the classified label does not match the actual label: \\[\\begin{equation} \\text{Error} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{I}\\left(Y_i \\ne \\hat{Y}_i \\right) = \\frac{\\text{# of misclassifications}}{n} \\tag{4.9} \\end{equation}\\] A useful way of representing a fitted models predicted labels against the observed, is by means of a confusion matrix, which is just a table showing the number of observations in each combination of observed and predicted label. Returning to the Default example above, lets use the model with all three predictors and the decision rule \\(\\hat{p}(\\boldsymbol{X}) \\geq 0.5\\) to create this table. # First get predictions (probabilities) from the model phat3 &lt;- predict(mod3, Default, &#39;response&#39;) # Then apply the decision rule yhat3 &lt;- ifelse(phat3 &gt;= 0.5, &#39;Yes&#39;, &#39;No&#39;) y &lt;- Default$default #True labels # Using table(). Caret package has a function too confmat3 &lt;- table(yhat3, y, dnn = c(&#39;Predicted label&#39;, &#39;True label&#39;)) # Calculate training error error3 &lt;- mean(yhat3 != y) # confmat3 %&gt;% kable(caption = &#39;Confusion matrix for the saturated logistic regression model fitted to the Default dataset&#39;) %&gt;% kable_styling(full_width = F) #Failing to preserve the headers confmat3 ## True label ## Predicted label No Yes ## No 9627 228 ## Yes 40 105 The values on the diagonals represent the correct classifications, whilst the off-diagonals are the misclassifications. Here we observe an error rate of 2.7%, or equivalently a classification accuracy of 97.3%. Although this sounds excellent, we need to bear in mind that the data were quite severely imbalanced, since the vast majority of individuals were non-defaulters. In fact, if we had simply predicted every observations to be No, then our training error would only have been 3.3%. This does not seem like a massive difference compared to 2.7%, yet we would not correctly identify a single defaulter with this approach! Clearly we need to be more nuanced in our calculation of misclassifications. Therefore, define the following errors: False Positive (FP): Classify \\(\\hat{Y} =1\\) when \\(Y =0\\) False Negative (FN): Classify \\(\\hat{Y} =0\\) when \\(Y =1\\) These quantities, together with the True Positives (TP) and True Negatives (TN), make up the confusion matrix: True state Negative Positive Prediction Negative TN FN Positive FP TP Tot Neg Tot Pos We can now define several metrics to describe the performance of a classification algorithm across various aspects, some of which have several equivalent names: True Positive Rate = TPR = Sensitivity = Recall = \\(\\frac{TP}{Tot Pos}\\) True Negative Rate = TNR = Specificity = \\(\\frac{TN}{Tot Neg}\\) Positive Predictive Value = PPV = Precision = \\(\\frac{TP}{TP + FP}\\) Negative Predictive Value = NPV = \\(\\frac{TN}{TN + FN}\\) Note that the complements of these values yield the corresponding errors. Various other metrics exist for specific testing purposes. For example, the F1 score  which represents the harmonic mean of precision and sensitivity  is a popular performance metric, especially when evaluating performance on imbalanced data7: \\[F_1=2\\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\] This metric ranges from 0 to 1, with higher being better. It is often used to gauge the performance of classification models across a variety of different tasks, for example in computer vision (image classification, object detection, segmentation), Natural Language Processing (named entity recognition, sentiment analysis, document classification), recommender systems, etc. All of these metrics can be calculated manually from a confusion matrix. Alternatively, we could use packages like caret or MLmetrics to set up the confusion matrix and get the measurements directly, as shown here: library(MLmetrics) library(caret) # Example using MLMetrics function, caret has similar f1_mod3 &lt;- MLmetrics::F1_Score(y, yhat3, positive = &#39;Yes&#39;) # Confusion matrix and metrics using caret caretmat3 &lt;- confusionMatrix(as.factor(yhat3), y, positive = &#39;Yes&#39;) #NB to set the ref category! caretmat3 ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 9627 228 ## Yes 40 105 ## ## Accuracy : 0.9732 ## 95% CI : (0.9698, 0.9763) ## No Information Rate : 0.9667 ## P-Value [Acc &gt; NIR] : 0.0001044 ## ## Kappa : 0.4278 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.3153 ## Specificity : 0.9959 ## Pos Pred Value : 0.7241 ## Neg Pred Value : 0.9769 ## Prevalence : 0.0333 ## Detection Rate : 0.0105 ## Detection Prevalence : 0.0145 ## Balanced Accuracy : 0.6556 ## ## &#39;Positive&#39; Class : Yes ## Our logistic regression model on the Default dataset yielded a training F1 score of 0.439, which provides a more informative measure of the models performance than just considering the accuracy. This metric balances the facts that the model only correctly identified 31.53% of defaulters (recall), and of those predicted as defaulters, 72.41% had actually defaulted (precision). We have noted that all of the above metrics can be calculated from the confusion matrix, regardless of which underlying model yielded the matrix. But the exact confusion matrix is determined by the predictions, which in turn depend on the decision rule threshold. Now that we know all errors are not equal, can we perhaps change the threshold to adjust certain metrics? Note that, for now, all of this is applied to just the training set in order to illustrate the principle. 4.2.1 Changing the threshold By setting \\(\\hat{Y} = 1\\) if \\(\\hat{p}(X) &gt; 0.5\\), we are actually being naive about our choice of decision threshold when attempting to map probabilities to classifications. This decision rule attempts to approximate the Bayes classifier, which minimises the total error rate. However, this implicitly assumes that the two error types (FP &amp; FN) are equally bad, which will not necessarily be the case, depending on the problem at hand. For the Default example, we might be specifically interested in classifying default events (TP), perhaps at the (lesser) cost of false positives. By adjusting the threshold level (\\(\\tau\\)), we can accommodate this asymmetric cost of misclassification. To illustrate this, lets use the same saturated model on the Default data as previously, but lower the decision rule threshold to \\(\\tau = 0.2\\), such that \\(\\hat{Y} = 1\\) if \\(\\hat{p}(X) &gt; 0.2\\). Therefore, we are more liberal in our classification of defaulters. yhat3_2 &lt;- ifelse(phat3 &gt;= 0.2, &#39;Yes&#39;, &#39;No&#39;) confmat3_2 &lt;- table(yhat3_2, y, dnn = c(&#39;Predicted label&#39;, &#39;True label&#39;)) confmat3_2 ## True label ## Predicted label No Yes ## No 9390 130 ## Yes 277 203 By manual inspection, we can see that the number of correctly classified defaulters almost doubled from 105 to 203. Therefore, the true positive rate (recall or sensitivity) increased, although it would have come at the cost of a worse false positive rate (fallout or 1 - specificity), as we see in Table 4.3. Note that the overall accuracy slightly decreased. tprs &lt;- c(sensitivity(as.factor(yhat3), y, &#39;Yes&#39;), sensitivity(as.factor(yhat3_2), y, &#39;Yes&#39;)) # Note that the following function requires the negative label specified! Always check help files. fprs &lt;- c(1 - specificity(as.factor(yhat3), y, &#39;No&#39;), 1 - specificity(as.factor(yhat3_2), y, &#39;No&#39;)) accs &lt;- c(mean(yhat3 == y), mean(yhat3_2 == y)) compar &lt;- rbind(tprs, fprs, accs) colnames(compar) &lt;- c(&#39;$\\\\tau$ = 0.5&#39;, &#39;$\\\\tau$ = 0.2&#39;) rownames(compar) &lt;- c(&#39;TPR&#39;, &#39;FPR&#39;, &#39;Accuracy&#39;) compar %&gt;% kable(digits = 3, caption = &#39;Comparison of the TPR, FPR, and accuracy for two different decision rule thresholds -- 0.5 and 0.2 -- applied to the saturated logistic regression model fitted to the Default dataset.&#39;) Table 4.3: Comparison of the TPR, FPR, and accuracy for two different decision rule thresholds  0.5 and 0.2  applied to the saturated logistic regression model fitted to the Default dataset. \\(\\tau\\) = 0.5 \\(\\tau\\) = 0.2 TPR 0.315 0.610 FPR 0.004 0.029 Accuracy 0.973 0.959 Another way of measuring how well the classifier is performing, is by varying \\(\\tau\\) over a range of values and keeping track of the TPR and FPR. To illustrate this, consider again the 1-variable model where we include only Balance. For convenience and clearer illustration, Figure 4.5 only displays a subsample of 200 observations. # Create a range of thresholds and empty vectors for their corresponding metrics taus &lt;- seq(0, 1, 0.01) TPR &lt;- vector(length = length(taus)) FPR &lt;- vector(length = length(taus)) # Same curve as above set.seed(1) sub_Default &lt;- Default[sample.int(nrow(Default), 200),] #Only 200 obs mod_t &lt;- glm(default ~ balance, data = sub_Default, family = &#39;binomial&#39;) Y_sub &lt;- as.numeric(sub_Default$default) - 1 x_range &lt;- seq(0, max(sub_Default$balance)*1.2, length.out = 1000) coefs_t &lt;- mod_t$coefficients logit_t &lt;- coefs_t[1] + coefs_t[2]*x_range sigmoid &lt;- exp(logit_t)/(1 + exp(logit_t)) phat_t &lt;- predict(mod_t, type = &#39;response&#39;) par(mfrow=c(1,2)) iter &lt;- 0 for (tau in taus){ iter &lt;- iter + 1 yhat_t &lt;- ifelse(phat_t &gt;= tau, 1, 0) x_cutoff &lt;- (log(tau/(1-tau)) - coefs_t[1])/(coefs_t[2]) #solve from logit # Left plot (curve) plot(x_range, sigmoid, type = &#39;l&#39;, lwd = 2, col = &#39;orangered&#39;, xlab = &#39;Balance&#39;, ylab = &#39;p(x)&#39;) abline(v = x_cutoff, lwd = 2, col = &#39;navy&#39;) segments(-500, tau, x_cutoff, tau, col = &#39;gray&#39;, lty = 4) points(sub_Default$balance, Y_sub, pch = 16, col = ifelse(sub_Default$balance &lt; x_cutoff, &#39;green&#39;, &#39;black&#39;)) legend(&#39;right&#39;, c(expression(paste(hat(Y), &#39; = 1&#39;)), &#39;&#39;, expression(paste(hat(Y), &#39; = 0&#39;))), pch = 16, col = c(&#39;black&#39;, &#39;white&#39;, &#39;green&#39;)) TPR[iter] &lt;- sum(yhat_t*Y_sub == 1)/sum(Y_sub) FPR[iter] &lt;- sum(yhat_t == 1 &amp; Y_sub == 0)/sum(Y_sub == 0) E &lt;- mean(yhat_t != Y_sub) text(0, 0.9, pos = 4, substitute(paste(&#39;Tau = &#39;, tau), list(tau = tau)), cex = 1.2) text(0, 0.8, pos = 4, substitute(paste(&#39;TPR = &#39;, TPR), list(TPR = round(TPR[iter], 3))), cex = 1.2) text(0, 0.7, pos = 4, substitute(paste(&#39;FPR = &#39;, FPR), list(FPR = round(FPR[iter], 3))), cex = 1.2) text(0, 0.6, pos = 4, substitute(paste(&#39;Misclass = &#39;, E), list(E = round(E, 3))), cex = 1.2) # Right plot (errors) plot(0,0, &#39;n&#39;, xlim = c(0,1), ylim = c(0,1), ylab = &#39;&#39;, xlab = &#39;Threshold&#39;) lines(seq(0, tau, length.out = iter), TPR[1:iter], &#39;l&#39;, lwd = 2, col = &#39;orange&#39;) lines(seq(0, tau, length.out = iter), FPR[1:iter], &#39;l&#39;, lwd = 2, col = &#39;lightblue&#39;) legend(&#39;topright&#39;, c(&#39;TPR&#39;, &#39;FPR&#39;), lty = 1, lwd = 2, col = c(&#39;orange&#39;, &#39;lightblue&#39;)) } Figure 4.5: The effect of varying the decision rule threshold on the TPR and FPR This pattern can now be combined in the Receiver Operating Characteristic (ROC) curve. 4.2.2 ROC Curve The most common way of displaying how TPR and FPR change as the threshold changes, is by plotting each on an axis for different values of the threshold. This yields the ROC curve, which provides an indication of the performance of a classifier over the entire range of decision rules, allowing us to compare different classifiers in a more rigorous way. Figure 4.6 shows the ROC curve for the saturated model fitted to the entire Default dataset with the points corresponding to \\(\\tau = 0.2\\) and \\(\\tau = 0.5\\)  for which we calculated the confusion matrices earlier  highlighted on the curve. We will make use of the ROCR package. library(ROCR) # Using the full model pred &lt;- prediction(phat3, Y) perf &lt;- performance(pred, &#39;tpr&#39;, &#39;fpr&#39;) plot(perf, colorize = FALSE, col = &#39;black&#39;) lines(c(0,1), c(0,1), col = &#39;gray&#39;, lty = 4) # tau = 0.5 points(compar[1,1] ~ compar[2,1], col = &#39;red&#39;, pch = 16) text(compar[1,1] ~ compar[2,1], labels = 0.5, pos = 4) # tau = 0.2 points(compar[1,2] ~ compar[2,2], col = &#39;red&#39;, pch = 16) text(compar[1,2] ~ compar[2,2], labels = 0.2, pos = 4) Figure 4.6: ROC curve for the full model fitted to the Default dataset. Thresholds of 0.2 and 0.5 are highlighted. A completely random classifier would (on average) lie on the diagonal of the ROC curve, whilst a classifier exclusively predicting one category would lie exactly on the diagonal. A perfect classifier would perfectly predict all positive cases and never falsely identify an observation as positive, although note that this would only be possible in the case of perfectly linearly separable data. For real-world data, an ideal classifier would lie as close as possible to the top left corner of the plot and we compare models by measuring how close they are to this ideal. The way to identify this is by calculating the area under the curve (AUC), which is simply a value between 0.5 and 1 (technically 0 and 1) measuring a binary classification models ability to distinguish between positive and negative responses. auc &lt;- performance(pred, measure = &#39;auc&#39;)@y.values[[1]] Using ROCR, we can calculate the AUC for the ROC curve in Figure 4.6 as 0.95. Although this metric can be objectively interpreted, the real value lies in comparing metrics across different models, ideally in a way that estimates out-of-sample performance. One way of potentially improving the fit of a logistic regression model, is once again through regularisation. 4.3 Regularisation In the previous chapter we considered the lasso, aka \\(L_1\\) regularisation as a variable selection method for a linear (or any parameterised) regression model. This was predicated on penalising the loss function, the RSS, which is minimised in order to find the estimated regression coefficients. We can apply the same reasoning to logistic regression, noting again that for this problem we estimate the regression coefficients by maximising the likelihood. Therefore, the penalty term is subtracted, instead of added. Let \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_1 &amp; \\beta_2 &amp; \\cdots &amp; \\beta_p \\end{bmatrix}\\). Now, using (4.1), the log-likelihood in (4.5) can be written as follows: \\[\\begin{align} \\ell(\\boldsymbol{\\beta}) &amp;= \\sum_{i=1}^n \\left[ y_i\\log(p(\\boldsymbol{x}_i)) + (1 - y_i)\\log(1 - p(\\boldsymbol{x}_i)) \\right] \\\\ &amp;= \\sum_{i=1}^n \\left[ y_i\\log\\left(\\frac{e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i}}{1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i}}\\right) + \\log\\left(1 - \\frac{e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i}}{1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i}} \\right) - y_i\\log\\left(1 - \\frac{e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i}}{1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i}} \\right) \\right] \\\\ &amp;= \\sum_{i=1}^n \\left[y_i\\left(\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i \\right) - y_i\\log\\left(1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i} \\right) - \\log\\left(1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i} \\right) + y_i\\log\\left(1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i} \\right) \\right] \\\\ &amp;= \\sum_{i=1}^n \\left[y_i\\left(\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i \\right) - \\log\\left(1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i} \\right) \\right]. \\tag{4.10} \\end{align}\\] Therefore, \\(L_1\\) regularised logistic regression parameters are given by: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_L = \\underset{\\boldsymbol{\\beta}}{\\text{argmax}}\\left\\{ \\sum_{i=1}^n \\left[y_i\\left(\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i \\right) - \\log\\left(1 + e^{\\beta_0 + \\boldsymbol{\\beta}&#39;\\boldsymbol{x}_i} \\right)\\right] - \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}. \\tag{4.11} \\end{equation}\\] As in the linear regression model, the predictors are generally standardised and the intercept term is not penalised. Once again, the optimsation methods used to solve for the coefficients are beyond the scope of this course. We shall illustrate the application by means of an example. 4.3.1 Example 4  Heart failure Chicco &amp; Jurman (2020) analyses the medical records of 299 patients who experienced heart failure, with data collected during the patients follow-up period. Each patient profile contains 12 clinical features, the details of which are given in Table 1 of the paper. The goal is to predict the 13th variable, namely death event (binary). After setting aside 20% of the data for testing, start by fitting the saturated model to the training data. # Read in the data and turn the categorical features to factors heart &lt;- read.csv(&#39;data/heart.csv&#39;, header = TRUE, colClasses = c(anaemia=&#39;factor&#39;, diabetes = &#39;factor&#39;, high_blood_pressure = &#39;factor&#39;, sex = &#39;factor&#39;, smoking = &#39;factor&#39;, DEATH_EVENT = &#39;factor&#39;)) # Create indices for train/test split set.seed(4026) train &lt;- sample(nrow(heart), size=0.8*nrow(heart)) # Always check the prevalence rate y_train &lt;- heart$DEATH_EVENT[train] # Fit &quot;vanilla&quot; logistic regression heart_lr &lt;- glm(DEATH_EVENT ~ ., data = heart, subset = train, family = &#39;binomial&#39;) heart_lr %&gt;% tidy() %&gt;% kable(digits = 2, caption = &#39;Saturated logistic regression model fitted to the heart failure dataset&#39;) Table 4.4: Saturated logistic regression model fitted to the heart failure dataset term estimate std.error statistic p.value (Intercept) 15.52 7.09 2.19 0.03 age 0.06 0.02 3.05 0.00 anaemia1 0.19 0.39 0.49 0.63 creatinine_phosphokinase 0.00 0.00 1.63 0.10 diabetes1 0.34 0.39 0.89 0.37 ejection_fraction -0.07 0.02 -4.01 0.00 high_blood_pressure1 -0.05 0.40 -0.12 0.90 platelets 0.00 0.00 -0.84 0.40 serum_creatinine 0.66 0.20 3.28 0.00 serum_sodium -0.11 0.05 -2.22 0.03 sex1 -0.81 0.46 -1.76 0.08 smoking1 0.13 0.46 0.27 0.79 time -0.02 0.00 -6.00 0.00 First, we observe that approximately a third of the patients in the training set passed away (33.9% to be exact). In Table 4.4 we see that a few of the predictors are not statistically significant in this model at any reasonable significance level and should be removed, most notably the hypertension, smoking, and decrease of hemoglobin indicator variables (high_blood_pressure1, smoking1 and anaemia1). Using \\(L_1\\) regularisation to perform the variable selection: library(glmnet) library(dplyr) #Gentle introduction to some tidyverse functions library(plotmo) #Specifically for glmnet plotting (also has a gbm function) # Using dplyr: x_train &lt;- select(heart, -DEATH_EVENT) %&gt;% slice(train) # Fit the lasso and plot using plotmo heart_l1 &lt;- glmnet(x_train, y_train, alpha = 1, standardize = T, family = &#39;binomial&#39;) plot_glmnet(heart_l1, xvar = &#39;norm&#39;) Figure 4.7: Coefficient profiles for \\(L_1\\) logistic regression on the heart failure dataset as a function of the constraint Figure 4.7 shows how the regression coefficients are reduced to zero as the \\(L_1\\) norm constraint decreases. To decide on the penalty to apply  and by extension the number of variables to drop  we again use 10-fold cross-validation with classification error as loss function. # cv.glmnet cannot take factor variables directly, we must create explicit dummy variables x_train_dummies &lt;- makeX(x_train) #For which the package has this function set.seed(1) heart_l1_cv &lt;- cv.glmnet(x_train_dummies, y_train, family = &#39;binomial&#39;, type.measure = &#39;class&#39;) # Plot plot(heart_l1_cv) Figure 4.8: 10-fold CV classification errors as a function of \\(\\log(\\lambda)\\) for \\(L_1\\) logistic regression applied to the heart failure dataset In Figure 4.8 we see that the best model according to cross-validated classification accuracy contains five features. However, it is crucial to note that this model displays high variance, and that setting different seeds will yield different results! The selected features in this instance are the ones with non-zero coefficients below: round(coef(heart_l1_cv, s = &#39;lambda.min&#39;), 3) ## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 3.646 ## age 0.009 ## anaemia0 . ## anaemia1 . ## creatinine_phosphokinase . ## diabetes0 . ## diabetes1 . ## ejection_fraction -0.020 ## high_blood_pressure0 . ## high_blood_pressure1 . ## platelets . ## serum_creatinine 0.200 ## serum_sodium -0.023 ## sex0 . ## sex1 . ## smoking0 . ## smoking1 . ## time -0.011 This will be our selected model, noting that the included variables were all significant at \\(\\alpha = 0.07\\) in the saturated model. sex was also borderline significant in this model and had the largest absolute coefficient (although the largest standard error too). We might wish to include this variable for physiological reasons, in which case we can relax the regularisation penalty. Another option is to optimise according to a different metric, for example the ROC AUC. Through cv.glmnet() we can keep track of the cross-validated ROC curves across the varying penalty and select the one yielding the largest AUC, as shown here: set.seed(1) auc_cv &lt;- cv.glmnet(x_train_dummies, y_train, family = &#39;binomial&#39;, type.measure = &#39;auc&#39;, keep = T) all_rocs &lt;- roc.glmnet(auc_cv$fit.preval, newy = y_train) #67 different curves! best_roc &lt;- auc_cv$index[&#39;min&#39;, ] #Also labeled min, even though here it&#39;s a max :| plot(all_rocs[[best_roc]], type = &#39;l&#39;) invisible(sapply(all_rocs, lines, col = &#39;grey&#39;)) lines(all_rocs[[best_roc]], lwd = 2,col = &#39;red&#39;) Figure 4.9: Cross-validated ROC curves for varying \\(L_1\\) logistic regression regularisation penalties. The curve with the largest AUC is indicated in red. However, the resulting model corresponds to a relatively small penalty, only dropping four variables from the model (retaining sex), although the other two included variables have very small coefficients: round(coef(auc_cv, s = &#39;lambda.min&#39;), 3) ## 18 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 9.929 ## age 0.034 ## anaemia0 . ## anaemia1 . ## creatinine_phosphokinase 0.000 ## diabetes0 . ## diabetes1 . ## ejection_fraction -0.050 ## high_blood_pressure0 . ## high_blood_pressure1 . ## platelets 0.000 ## serum_creatinine 0.469 ## serum_sodium -0.072 ## sex0 0.294 ## sex1 0.000 ## smoking0 . ## smoking1 . ## time -0.016 We now have three logistic regression models of varying complexity: the full model8 (containing some unnecessary variables), the 5-variable model (regularised according to CV accuracy), and the 8-variable model (regularised according to CV AUC). At this point, one would need to select one of these models to use on the test set. If the goal is classification accuracy, then we would use the 5-variable model. However, for the sake of this example we will apply all three models to the test set and compare their performance across a few metrics. x_test &lt;- select(heart, -DEATH_EVENT) %&gt;% slice(-train) #predict.glm requires matrix x_test_dummies &lt;- makeX(x_test) #with dummy variables y_test &lt;- heart$DEATH_EVENT[-train] pred_full &lt;- predict(heart_lr, newdata = heart[-train, ], type = &#39;response&#39;) #response for p(x) pred_l1_acc &lt;- predict(heart_l1_cv, x_test_dummies, s = &#39;lambda.min&#39;, type = &#39;response&#39;)[,1] #default output is matrix pred_l1_auc &lt;- predict(auc_cv, x_test_dummies, s = &#39;lambda.min&#39;, type = &#39;response&#39;)[,1] # Write a function for calculating all the desired metrics from a model&#39;s predicted probs my_metrics &lt;- function(pred, labs){ p_labs &lt;- round(pred) #Threshold 0.5 t &lt;- table(p_labs, labs) acc &lt;- mean(p_labs == labs) #accuracy rec &lt;- recall(t, &#39;1&#39;) #from caret sp &lt;- specificity(t, &#39;0&#39;) #ditto pr &lt;- precision(t, &#39;1&#39;) #ditto f1 &lt;- F_meas(t, &#39;1&#39;) #ditto auc &lt;- performance(prediction(pred, labs), &#39;auc&#39;)@y.values[[1]] #from ROCR metrics &lt;- c(&#39;Accuracy&#39; = acc, &#39;Recall&#39; = rec, &#39;Specificity&#39; = sp, &#39;Precision&#39; = pr, &#39;F1&#39; = f1, &#39;ROC AUC&#39; = auc) return(metrics) } heart_results &lt;- rbind(&#39;Vanilla LR&#39; = my_metrics(pred_full, y_test), &#39;L1 LR (CV accuracy)&#39; = my_metrics(pred_l1_acc, y_test), &#39;L1 LR (CV AUC)&#39; = my_metrics(pred_l1_auc, y_test)) heart_results %&gt;% kable(digits = 3, caption = &#39;Comparison of model performance on the heart failure test set. The vanilla logistic regression model used all 12 features; the models regularised according to accuracy and AUC contained 5 and 8 features, respectively.&#39;) Table 4.5: Comparison of model performance on the heart failure test set. The vanilla logistic regression model used all 12 features; the models regularised according to accuracy and AUC contained 5 and 8 features, respectively. Accuracy Recall Specificity Precision F1 ROC AUC Vanilla LR 0.833 0.800 0.844 0.632 0.706 0.899 L1 LR (CV accuracy) 0.867 0.667 0.933 0.769 0.714 0.944 L1 LR (CV AUC) 0.850 0.800 0.867 0.667 0.727 0.939 Both approaches to the regularisation improved the results from the baseline vanilla logistic regression across all metrics with the exception of recall, which is arguably the most important metric for death events! None of the models could correctly predict more than 80% of the observed deaths. Whilst the model maximising CV accuracy did indeed yield the highest accuracy on the test set, it correctly identified only two-thirds of death events, again illustrating the danger of focusing solely on accuracy even with mildly imbalanced data. This model actually produced a marginally better test AUC than the model fitted to maximise AUC, although the latter model did yield the best F1 score. However, due in part to the relatively small size of this problem, the differences in results were relatively small. It should also be highlighted once more that there is some notable sampling variability, both in the train/test split and the cross-validation. In fact, Chicco &amp; Jurman (2020) presented their results, across a variety of models and metrics, as the averages of 100 results from randomised train/test splits. Replicating this as well as repeated CV sampling are left as homework exercises. In the following chapters we will move away from the linear class of models, considering some parametric and non-parametric approaches to modelling non-linearity. 4.4 Homework exercises Show that for binary classification, the logistic regression decision boundary for the decision threshold \\(\\hat{p}(\\boldsymbol{X}) \\geq 0.5\\) corresponds to the set of points in the parameter space for which the odds are 1 (or, equivalently, the log-odds are zero). For Example 3 (Default data), create an animation showing how the decision boundary in Figure 4.4 shifts as the decision threshold \\(\\tau\\) is varied from 0 to 1 (both excluded). Replicate the methodology of Chicco &amp; Jurman (2020) by repeating the modeling done in Example 4 100 times. Draw a boxplot for each metric across the 100 runs for each of the three models and report the averages. Continuing with Example 4: For a single train/test split, repeat the cross-validated regularisation (using both accuracy and AUC) 100 times and again visualise the resulting metrics via boxplots. Now repeat this procedure for 100 random train/test splits, whilst in each iteration keeping track of the mean and standard deviation of each metric across the 100 CV runs. How do the boxplots for the means compare to your results in Question 2? Imbalanced data refers to a dataset where the distribution of target classes is not uniform, i.e.Â there are significantly more observations with labels from the majority class(es) than the minority class(es). This model  using all the predictors without any constraints and applying the default settings  is often colloquially referred to as the vanilla model. "],["non-linear-models.html", "Chapter 5 Non-Linear Models 5.1 Polynomial regression 5.2 K-Nearest Neighbours 5.3 Homework exercises", " Chapter 5 Non-Linear Models The regression and classification models we considered in the previous two chapters were predicated on the assumption  even after applying regularisation  that the input variables map linearly to the response. In reality, this will usually be an approximation at best, and wildly over simplistic at worst. Therefore, although these models offer clear interpretation and inference, this often comes at the cost of predictive power. As we saw in chapter 2, we can increase a models flexibility in an attempt to decrease the bias component of the error, although we must again be cognizant of the fact the increased variance will eventually offset this gain. Some of the possible non-linear modelling approaches are polynomial regression, step functions, regression and smoothing splines, multivariate adaptive regression splines (MARS), local regression methods, and generalised additive models (GAMs). There are many more, and these are just the parametric techniques! In this chapter we will only cover one parametric approach in polynomial regression and a simple non-parametric approach in K-nearest neighbours (KNN). Tree-based methods (including ensembling), another powerful non-parametric approach for both regression and classification, will be left for the final chapter. The reader is encouraged to explore some of the other aforementioned techniques in chapter 7 of James et al. (2013). 5.1 Polynomial regression Polynomial regression is simply a type of multiple regression in which the relationship between the independent variable(s) and the dependent variable is modelled as an \\(d^{th}\\)-degree polynomial. In contrast to linear regression, where the relationship is modeled as a straight line/hyperplane, polynomial regression allows us to capture more complex, non-linear relationships between variables. Note that linear regression is the simplest case of polynomial regression, i.e.Â 1st-degree. This approach can be applied to both regression (multiple linear regression) and classification problems (logistic regression). 5.1.1 Regression The polynomial regression model for a single predictor variable can be represented as follows: \\[\\begin{equation} Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\cdots + \\beta_dX^d + \\epsilon. \\tag{5.1} \\end{equation}\\] By selecting an appropriate degree for the polynomial, we can capture different types of nonlinear relationships. As we have seen previously though, using a degree that is too high yields an overly complex model that will overfit on the training data. In practice it is unusual to use \\(d &gt; 4\\), since these highly flexible models can perform especially poorly near the boundaries of the observed predictors. Let us illustrate the application via an example. 5.1.2 Example 5  Auto We will consider the well-known Auto dataset, available in the ISLR package, which contains measurements on the mileage, engine specifications, and manufacturing information for 392 vehicles. A sensible relationship to model is how a vehicles mileage depends on its specifications. First, visually explore the relationships between the numeric variables. One could do so quickly using the pairs() function, although the ggplot version offers more options for customisation. library(ISLR) library(GGally) library(dplyr) data(Auto) #Origin is categorical (as is name, but we will exclude this) Auto$origin &lt;- as.factor(Auto$origin) levels(Auto$origin) &lt;- c(&#39;US&#39;, &#39;Euro&#39;, &#39;Japan&#39;) Auto %&gt;% select(-c(name, year)) %&gt;% ggpairs(mapping = aes(colour = origin, alpha = 0.5), legend = 1) + scale_alpha_identity() Figure 5.1: Exploratory plot for the Auto dataset Since we are interested in the relationship between miles per gallon (mpg) and the other covariates, the focus is on the left-most column in Figure 5.1. There are clear inverse relationships between mileage and displacement, horsepower, and weight, although neither of these seem to be linear, especially for American vehicles9. We do not require a highly flexible function to model these variables either; a quadratic polynomial will suffice. For example, consider first the horsepower variable. Although this feature is highly significant in a linear model, the quadratic fit captures a higher proportion of the variation in the data. Here we use the stargazer package to print both models results for comparison. library(stargazer) # Linear fit mod1 &lt;- lm(mpg ~ horsepower, Auto) mod2 &lt;- lm(mpg ~ poly(horsepower, 2, raw = T), data=Auto) # This is the same as lm(mpg ~ horsepower + I(horsepower^2), data=Auto) # Using raw=F (the default) yields orthogonal polynomials stargazer(mod1, mod2, type = &#39;html&#39;, digits = 3, star.cutoffs = NA, report=(&#39;vc*p&#39;), omit.table.layout = &#39;n&#39;, title = &#39;Linear vs quadratic model results for the Auto dataset, using only horsepower&#39;) Linear vs quadratic model results for the Auto dataset, using only horsepower Dependent variable: mpg (1) (2) horsepower -0.158 p = 0.000 poly(horsepower, 2, raw = T)1 -0.466 p = 0.000 poly(horsepower, 2, raw = T)2 0.001 p = 0.000 Constant 39.936 56.900 p = 0.000 p = 0.000 Observations 392 392 R2 0.606 0.688 Adjusted R2 0.605 0.686 Residual Std. Error 4.906 (df = 390) 4.374 (df = 389) F Statistic 599.718 (df = 1; 390) 428.018 (df = 2; 389) In Figure 5.2 we can clearly see that the quadratic fit (red line) captures the shape of the data better than the linear model (gray line), especially at the boundaries. However, note that we include the lower degree terms in the polynomial regression model as well  in this case the linear term alone  to capture these components in the data too. # Plot mpg vs horsepower plot(mpg ~ horsepower, data = Auto, pch = 16, col= &#39;darkblue&#39;) # Add fits x_axs &lt;- seq(min(Auto$horsepower), max(Auto$horsepower), length = 100) lines(x_axs, predict(mod1, data.frame(horsepower = x_axs)), col = &#39;darkgrey&#39;) lines(x_axs, predict(mod2, data.frame(horsepower = x_axs)), col = &#39;red&#39;) # Calculate 95% confidence intervals around fits CI_1 &lt;- predict(mod1, data.frame(horsepower = x_axs), interval = &#39;confidence&#39;, level = 0.95) CI_2 &lt;- predict(mod2, data.frame(horsepower = x_axs), interval = &#39;confidence&#39;, level = 0.95) # And add to plot matlines(x_axs, CI_1[, 2:3], col = &#39;darkgrey&#39;, lty = 2) matlines(x_axs, CI_2[, 2:3], col = &#39;red&#39;, lty = 2) Figure 5.2: Linear (gray) and quadratic (red) fits for the Auto dataset, using only horsepower. 95% confidence intervals are indicated with dashed lines. One could now add the rest of the variables in the model, although the extreme collinearity will result in only some of them being included in the final model after applying variable selection/regularisation. This is left as an exercise. Before moving on to a localised approach, we note that polynomial regression can also be applied to classification problems. 5.1.3 Classification In the previous chapter we saw that the logistic regression model yielded linear decision boundaries since the logit is linear in \\(\\boldsymbol{X}\\). To create non-linear decision boundaries, we simply add the polynomial terms to the logit in Equation 4.2: \\[\\begin{equation} \\log \\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\cdots + \\beta_dX^d, \\tag{5.2} \\end{equation}\\] for only a single predictor \\(X\\). More variables can be added to the model in a similar fashion. We will once again illustrate this by means of an example, by returning to the heart failure dataset first seen in section 4.3.1. 5.1.4 Example 4  Heart failure (continued) For illustration purposes we will focus on two of the numeric predictors, namely age and ejection_fraction. Fitting a linear logistic regression model on the entire dataset using only these two variables yields the decision boundary displayed in 5.3: # Read in the data and turn the categorical features to factors heart &lt;- read.csv(&#39;data/heart.csv&#39;, header = TRUE, colClasses = c(anaemia=&#39;factor&#39;, diabetes = &#39;factor&#39;, high_blood_pressure = &#39;factor&#39;, sex = &#39;factor&#39;, smoking = &#39;factor&#39;, DEATH_EVENT = &#39;factor&#39;)) # Fit logistic regression (linear) lin_log &lt;- glm(DEATH_EVENT ~ age + ejection_fraction, data = heart, family = &#39;binomial&#39;) cfs1 &lt;- coef(lin_log) #Extract coefficients # Plot Age vs Ejection fraction plot(heart$age, heart$ejection_fraction, col = ifelse(heart$DEATH_EVENT == &#39;1&#39;, &#39;darkorange&#39;, &#39;lightblue&#39;), pch = ifelse(heart$DEATH_EVENT == &#39;1&#39;, 3, 1), xlab = &#39;Age&#39;, ylab = &#39;Ejection fraction&#39;) legend(&#39;topright&#39;, c(&#39;Death&#39;, &#39;Survival&#39;), col = c(&#39;darkorange&#39;, &#39;lightblue&#39;), pch = c(3, 1)) # Add the decision boundary abline(-cfs1[1]/cfs1[3], -cfs1[2]/cfs1[3], col = &#39;navy&#39;, lwd = 2) Figure 5.3: The linear logistic regression decision boundary for the heart failure dataset using age and ejection fraction as predictors. Let us now propose a more flexible model: \\[\\begin{equation} \\log \\left( \\frac{p(\\boldsymbol{X})}{1 - p(\\boldsymbol{X})} \\right) = \\beta_0 + \\beta_1\\text{Age} + \\beta_2\\text{Age}^2 + \\beta_3\\text{Age}^3 + \\beta_4\\text{Age}^4 + \\beta_5\\text{EjectionFraction}, \\end{equation}\\] The resulting non-linear decision boundary can be seen in Figure 5.4. # Fit logistic regression with 5th degree on age + serum_creatinine poly_log &lt;- glm(DEATH_EVENT ~ age + I(age^2) + I(age^3) + I(age^4) + ejection_fraction, data = heart, family = &#39;binomial&#39;) cfs4 &lt;- coef(poly_log) #Extract coefficients # Plot age vs ejection fraction plot(heart$age, heart$ejection_fraction, col = ifelse(heart$DEATH_EVENT == &#39;1&#39;, &#39;darkorange&#39;, &#39;lightblue&#39;), pch = ifelse(heart$DEATH_EVENT == &#39;1&#39;, 3, 1), xlab = &#39;Age&#39;, ylab = &#39;Ejection fraction&#39;) legend(&#39;topright&#39;, c(&#39;Death&#39;, &#39;Survival&#39;), col = c(&#39;darkorange&#39;, &#39;lightblue&#39;), pch = c(3, 1)) # And add decision boundary xx &lt;- seq(min(heart$age), max(heart$age), length.out = 100) lines(xx, (cbind(1, xx, xx^2, xx^3, xx^4) %*% cfs4[-6])/-cfs4[6], #again, do the math! col = &#39;navy&#39;, lwd = 2) Figure 5.4: The polynomial logistic regression decision boundary for the heart failure dataset using age4 and ejection fraction as predictors. Of course we know that increased complexity does not necessarily imply improved fit or prediction. As with any other model, we would need to use out-of-sample data to determine whether this model better captures the underlying relationship in the data, whilst again considering what constitutes an ideal fit for the problem, i.e.Â weighing the asymmetric cost of misclassification. This is once again left as an exercise to the reader. To limit the scope of this section of the course, we will end our discussion on non-linear parametric models here, with a brief addendum: 5.1.5 Extension to basis functions and generalised additive models Although polynomial regression is a valuable tool for capturing nonlinear relationships, it has its limitations, especially when dealing with complex data patterns. To address these limitations and provide more flexible modelling options, one can apply a more general basis function approach, where any family of functions or transformations are applied to the features. These functions can also be fitted piecewise (locally), defining splines, which are generally smoothed to be piecewise continuous and linear at the boundaries. Finally, Generalised Additive Models (GAMs) are a powerful extension of linear regression that allow for the modeling of complex interactions and nonlinear relationships without relying on a single global polynomial. They are particularly useful when dealing with high-dimensional data and when you want to capture intricate relationships between predictors and the target. As with polynomial regression, these methods can be applied in both regression and classification contexts. 5.2 K-Nearest Neighbours K-Nearest Neighbours (KNN) is a simple non-parametric algorithm that can perform surprisingly well in various contexts. At its core, KNN makes predictions based on the similarity between data points. It operates on the premise that similar data points tend to belong to the same class (in classification) or have similar target values (in regression). During the training phase, KNN stores the entire dataset in memory. No explicit model is constructed and no parameters are learned  the training phase simply involves memorising the data. When making a prediction for a new, unseen data point, KNN looks at the \\(K\\) nearest data points from the training dataset, where \\(K\\) is a user-defined hyperparameter. Euclidean distance is generally employed as distance metric, although other measurements such as Manhattan distance and the Minkowski distance (of which the Euclidean distance is a special case) can also be used. We will again consider these output separately for regression and classification tasks. 5.2.1 Regression Consider a continuous target variable. Given a value for \\(K\\) and a prediction point \\(\\boldsymbol{x}_0\\), KNN regression identifies the \\(K\\) training observations that are closest to \\(\\boldsymbol{x}_0\\), for now we will assume according to Euclidean distance. Denote these observations by \\(\\mathcal{N}_0\\). We then simply estimate \\(f(\\boldsymbol{x}_0)\\) as the average of the training responses in \\(\\mathcal{N}_0\\). Therefore, \\[\\begin{equation} \\hat{f}(\\boldsymbol{x}_0) = \\frac{1}{K}\\sum_{\\boldsymbol{x}_i \\in \\mathcal{N}_0} y_i. \\tag{5.3} \\end{equation}\\] The choice of \\(K\\) once again amounts to deciding on the flexibility of the decision boundary, as illustrated in the following example: 5.2.2 Example 2  Prostate cancer (continued) Consider again the prostate cancer dataset from Chapter 3. We will focus on the variable that was least significant in the saturated model, namely age. Although the KNN algorithm is relatively simple to code from scratch, we will use the knnreg() function from the caret package. library(caret) dat_pros &lt;- read.csv(&#39;data/prostate.csv&#39;) # Extract train and test examples and drop the indicator column train_pros &lt;- dat_pros[dat_pros$train, -10] test_pros &lt;- dat_pros[!dat_pros$train, -10] # KNN reg with k = 3 and k = 10 knn3 &lt;- knnreg(lpsa ~ age, train_pros, k = 3) knn10 &lt;- knnreg(lpsa ~ age, train_pros, k = 10) # Range of xs and predictions (fitted &quot;curve&quot;) xx &lt;- min(train_pros$age):max(train_pros$age) #Integer variable knn3_f &lt;- predict(knn3, data.frame(age = xx)) knn10_f &lt;- predict(knn10, data.frame(age = xx)) # Plots par(mfrow=c(1,2)) for(i in 1:length(xx)){ # Need the distances just for illustration f &lt;- cbind(c(xx[i], train_pros$age), c(knn3_f[i], train_pros$lpsa)) dists &lt;- dist(f)[1:nrow(train_pros)] dist_ords &lt;- order(dists) # Left plot (k = 3) plot(lpsa ~ age, train_pros, pch = 16, col = &#39;navy&#39;, main = &#39;KNN regression with K = 3&#39;) lines(xx[1:i], knn3_f[1:i], type = &#39;s&#39;) segments(max(xx)*2, knn3_f[i], xx[i], knn3_f[i], lty = 3) mtext(substitute(hat(y) == a, list(a = round(knn3_f[i], 1))), 4, at = knn3_f[i], padj = 0.5) segments(train_pros$age[dist_ords[1:3]], train_pros$lpsa[dist_ords[1:3]], xx[i], knn3_f[i], col = &#39;darkgrey&#39;) # Right plot (k = 10) plot(lpsa ~ age, train_pros, pch = 16, col = &#39;navy&#39;, main = &#39;KNN regression with K = 10&#39;) lines(xx[1:i], knn10_f[1:i], type = &#39;s&#39;) segments(max(xx)*2, knn10_f[i], xx[i], knn10_f[i], lty = 3) mtext(substitute(hat(y) == a, list(a = round(knn10_f[i], 1))), 4, at = knn10_f[i], padj = 0.5) segments(train_pros$age[dist_ords[1:10]], train_pros$lpsa[dist_ords[1:10]], xx[i], knn10_f[i], col = &#39;darkgrey&#39;) } Figure 5.5: KNN regression with \\(K\\) = 3 and \\(K\\) = 10 on the prostate cancer dataset, using only age As we can see in Figure 5.5, there is more volatility in the fit for smaller values of \\(K\\). To further illustrate this, Figure 5.6 shows the different fits as \\(K\\) changes. for(k in 1:20){ knn_fit &lt;- knnreg(lpsa ~ age, train_pros, k = k) knn_f &lt;- predict(knn_fit, data.frame(age = xx)) plot(lpsa ~ age, train_pros, pch = 16, col = &#39;navy&#39;, main = paste0(&#39;KNN regression with k = &#39;, k)) lines(xx, knn_f, type = &#39;s&#39;) } Figure 5.6: KNN regression on the prostate cancer dataset, using only age, for varying values of \\(K\\) The resulting fit is a stepped function, which becomes a stepped surface when increasing the dimensionality, as can be seen in Figure 5.7 when adding lbph to the model and fitting a KNN regression with \\(K = 3\\). library(plotly) # Fit knn3_2d &lt;- knnreg(lpsa ~ age + lbph, train_pros, k = 3) # Surface xx1 &lt;- min(train_pros$age):max(train_pros$age) xx2 &lt;- seq(min(train_pros$lbph), max(train_pros$lbph), length.out = length(xx1)) fgrid &lt;- expand.grid(age = xx1, lbph = xx2) f &lt;- predict(knn3_2d, fgrid) z &lt;- matrix(f, nrow = length(xx1), byrow = T) # Plot fig &lt;- plot_ly(x = ~xx1, y = ~xx2, z = ~z, type = &#39;surface&#39;, showscale = F) %&gt;% add_markers(x = train_pros$age, y = train_pros$lbph, z = train_pros$lpsa, inherit = F, showlegend = F, marker = list(size = 5, color = &#39;magenta&#39;)) %&gt;% layout(scene = list( xaxis = list(title = &#39;Age&#39;), yaxis = list(title = &#39;LBPH&#39;), zaxis = list(title = &#39;LPSA&#39;) )) fig Figure 5.7: KNN regression with \\(K\\) = 3 applied to the prostate cancer dataset, using age and lbph Now, \\(K = 3\\) was chosen arbitrarily here, hence the question again arises of which value of \\(K\\) to use, i.e.Â what our model complexity should be. Once again we will make use of cross-validation to fit and validate models of varying complexity. We will now introduce the caret package as a tool for performing this hyperparameter tuning. One of the drawbacks of KNN is that there is no sensible way of determining how much a specific variable contributes towards explaining the variance in the target variable, which makes feature selection a difficult, often trial-and-error process. Since we have relatively few observations in this dataset, we will only use 3 predictors  the last 3 predictors that remain in the lasso model  namely lcavol, lweight, and svi. Also, since the dataset is so small, we will repeat the CV procedure 10 times and average over the results. # See names(getModelInfo()) for a list of algorithms in caret # This is where one would specify combinations of hyperparameters knn_grid &lt;- expand.grid(k = 3:15) # knn only only has one: k. See getModelInfo()$knn$parameters # Specify the CV procedure ctrl &lt;- trainControl(method = &#39;repeatedcv&#39;, number = 10, repeats = 10) # Use train() to fit all the models set.seed(4026) knn_cv &lt;- train(lpsa ~ lcavol + lweight + svi, data = train_pros, method = &#39;knn&#39;, trControl = ctrl, tuneGrid = knn_grid) # Plot results plot(knn_cv) Figure 5.8: Repeated CV results for KNN as applied to the prostate cancer dataset. As can be seen in 5.8, the best KNN model according to the CV procedure (lowest RMSE) is one with \\(K = 6\\). Remember that for this model, lower \\(K\\) increases flexibility, therefore the model starts overfitting for \\(K\\) smaller than 6. Now we use this model to predict on the test set. test_y &lt;- test_pros[, 9] knn_pred &lt;- predict(knn_cv, test_pros) (knn_mse &lt;- round(mean((test_y - knn_pred)^2), 3)) ## [1] 0.39 The testing MSE of 0.39 is actually noticeably better than our best regularised linear model, which yielded an MSE of 0.45. Testing other combinations of features, which could possibly improve the results further, is left as a homework exercise. Finally, we will apply KNN in a classification setting. 5.2.3 Classification The training setup in the classification setting is exactly the same as for regression. Let the target \\(Y \\in \\{1, 2, \\ldots, J\\}\\) and again denote the \\(K\\) training observations closest to \\(\\boldsymbol{x}_0\\) as \\(\\mathcal{N}_0\\). The KNN classifier then simply estimates the conditional probability for class \\(j\\) as the proportion of points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\): \\[\\begin{equation} \\Pr(Y=j|\\boldsymbol{X} = \\boldsymbol{x}_0) = \\frac{1}{K}\\sum_{\\boldsymbol{x}_i \\in \\mathcal{N}_0} I(y_i = j). \\tag{5.4} \\end{equation}\\] To illustrate this, we will again return to the heart failure dataset. 5.2.4 Example 4  Heart failure (continued) To illustrate the decision boundary resulting from the KNN classifier, consider again the predictors age and ejection_fraction. We will use the class packages knn() function, although note that many other packages provide the same functionality. library(class) xx1 &lt;- min(heart$age):max(heart$age) xx2 &lt;- min(heart$ejection_fraction):max(heart$ejection_fraction) fgrid &lt;- expand.grid(age = xx1, ejection_fraction = xx2) tr &lt;- select(heart, age, ejection_fraction) # Fit KNN with k = 3 knn3_class &lt;- knn(tr, fgrid, heart$DEATH_EVENT, k = 3) knn10_class &lt;- knn(tr, fgrid, heart$DEATH_EVENT, k = 10) fgrid$f3 &lt;- knn3_class fgrid$f10 &lt;- knn10_class #Animation taking too long...Something for future # fitmat_3 &lt;- matrix(knn3_class, nrow = length(xx1), byrow = T) # fitmat_10&lt;- matrix(knn10_class, nrow = length(xx1), byrow = T) # for(i in 1:length(xx1)){ # for(j in 1:length(xx2)){ # # Plot age vs ejection fraction # plot(heart$age, heart$ejection_fraction, # col = ifelse(heart$DEATH_EVENT == &#39;1&#39;, &#39;darkorange&#39;, &#39;lightblue&#39;), # pch = ifelse(heart$DEATH_EVENT == &#39;1&#39;, 3, 1), # xlab = &#39;Age&#39;, ylab = &#39;Ejection fraction&#39;, main = &#39;KNN classification with K = 3&#39;) # # points(xx1[i], xx2[j], pch = 15, # col = ifelse(fitmat_3[i,j] == 1, # rgb(255, 140, 0, maxColorValue = 255, alpha = 255*0.4), # rgb(173, 216, 230, maxColorValue = 255, alpha = 255*0.4))) # segments(xx1[i], min(xx2)/2, xx1[i], xx2[j], lty = 3) # segments(min(xx1)/2, xx2[j], xx1[i], xx2[j], lty = 3) # } # } par(mfrow = c(1, 2)) # K = 3 plot(heart$age, heart$ejection_fraction, col = &#39;black&#39;, pch = ifelse(heart$DEATH_EVENT == &#39;1&#39;, 3, 1), xlab = &#39;Age&#39;, ylab = &#39;Ejection fraction&#39;, main = &#39;KNN classification with K = 3&#39;) points(fgrid$age, fgrid$ejection_fraction, pch = 15, col = ifelse(fgrid$f3 == 1, rgb(255, 140, 0, maxColorValue = 255, alpha = 255*0.25), rgb(173, 216, 230, maxColorValue = 255, alpha = 255*0.25))) # K = 10 plot(heart$age, heart$ejection_fraction, col = &#39;black&#39;, pch = ifelse(heart$DEATH_EVENT == &#39;1&#39;, 3, 1), xlab = &#39;Age&#39;, ylab = &#39;Ejection fraction&#39;, main = &#39;KNN classification with K = 10&#39;) points(fgrid$age, fgrid$ejection_fraction, pch = 15, col = ifelse(fgrid$f10 == 1, rgb(255, 140, 0, maxColorValue = 255, alpha = 255*0.25), rgb(173, 216, 230, maxColorValue = 255, alpha = 255*0.25))) Figure 5.9: KNN regression with \\(K\\) = 3 and \\(K\\) = 10 on the heart failure dataset, using age and ejection fraction. Crosses are observed deaths, circles are survivals. The orange regions pertain to predicted death, the blue to predicted survival. Figure 5.9 shows highly flexible decision boundaries, which clearly fit local noise especially when \\(K\\) is small. As before, we can use CV to determine the ideal complexity according to an appropriate model evaluation metric. This is left as a homework exercise. As we have seen, there are several advantages and disadvantages to using KNN. Advantages: It is a very simple algorithm to understand and implement, for both regression and multiclass classification. It does not make assumptions about the decision boundaries, allowing it to capture non-linear relationships between features. It does not make assumptions about the distribution of the data, making it suitable for a wide range of problems. Disadvantages: It requires a lot of memory and is computationally expensive for large and complex datasets. It is not suitable for imbalanced data (classification), as it is biased towards the majority class. In regression contexts it is sensitive to outliers, especially for smaller \\(K\\). There are no neat ways of measuring variable importance or performing feature selection. It performs particularly poorly on very noisy data. It requires a lot of data for high-dimensional problems, suffering severely from the curse of dimensionality. In the final chapter, we will encounter another potentially powerful family of heuristics by exploring tree-based methods. 5.3 Homework exercises Split the heart failure dataset into the same training and testing sets as in chapter 4. Fit the polynomial regression from section 5.1.4 to the training set and compare the results with those from the linear models in chapter 4. For the prostate cancer dataset, use different combinations of features in the KNN model, compare them according to CV RMSE, and evaluate the best combination on the test set. How does this compare with the model applied above? Continuing with question 1, fit a KNN model (applying hyperparameter tuning) to the heart failure training set and compare the test set performance with the linear and polynomial regression models. Adding interaction terms with origin might be advisable. "],["tree-based-methods.html", "Chapter 6 Tree-based Methods 6.1 Regression trees 6.2 Classification trees 6.3 Bagging and random forests 6.4 Gradient boosting 6.5 Homework exercises", " Chapter 6 Tree-based Methods The final topic in this section of the course is tree-based methods, i.e.Â decision trees10 and their extensions. We will cover classification and regression trees, random forests, and gradient boosted trees. Although chapter 8 of James et al. (2013) contains a concise summary of these methods, the interested reader is rather referred to section 9.2 of Hastie et al. (2009) for trees and chapter 15 for random forests, and section 16.4 of Murphy (2013) for boosting. Classification and regression trees  often abbreviated as CART or just referred to as decision trees collectively  are simple, non-parametric classification/regression methods that utilise a tree structure to model the relationships among the features and the potential outcomes. Although decision trees are conceptually very simple, they can be quite powerful, especially when ensembled. They are built using a heuristic called recursive binary partitioning (divide and conquer) that partitions the feature space into a set of (hyper) rectangles. The prediction following the partitioning depends on whether the response is quantitative on qualitative, hence we will again consider regression and classification problems separately, starting with the former. 6.1 Regression trees Let \\(Y\\) denote a continuous outcome variable and let \\(X_1, X_2, \\ldots, X_p\\) be a set of predictors/features, of which we have \\(n\\) observations. The regression tree algorithm entails repeatedly splitting the p-dimensional feature space into distinct, non-overlapping regions, with the splits orthogonal to the axes. Suppose that we partition the feature space into \\(M\\) regions, \\(R_1 , R_2, \\ldots, R_M\\). Then for each region we model the response as a constant, \\(c_m\\): \\[\\begin{equation} f(\\boldsymbol{x}) = \\sum_{m=1}^Mc_mI(\\boldsymbol{x} \\in R_m). \\tag{6.1} \\end{equation}\\] As with other regression models, our goal is still to minimise \\[\\begin{equation} RSS = \\sum_{m=1}^{M}\\sum_{i:x_i\\in R_m}\\left[y_i - \\hat{f}(\\boldsymbol{x} _i)\\right]^2. \\tag{6.2} \\end{equation}\\] Therefore, the best \\(\\hat{c}_m\\) to minimise this criterion is simply the average of the \\(y_i\\) in each region \\(R_j\\): \\[\\begin{equation} \\hat{c}_m = \\text{ave}(y_i|\\boldsymbol{x}_i \\in R_m) \\tag{6.3} \\end{equation}\\] Exhaustively searching over every possible combination of regions is computationally infeasible. Therefore, we use a top-down, greedy algorithm called recursive binary splitting: First, select the predictor \\(X_j\\) and split point \\(s\\) such that the regions \\(R_1(j, s) = \\{X|X_j &lt; s\\}\\) and \\(R_2(j, s) = \\{X|X_j \\geq s\\}\\) lead to the greatest possible reduction in RSS. That is, we consider all predictors \\(X_1, \\ldots, X_p\\), and all possible values of the split point \\(s\\) for each of the predictors, and then choose the predictor and split point such that Equation (6.2) is minimised. Next, identify the predictor and split point that best splits one of the previously identified regions, such that there are now three regions. Continue splitting the regions until a stopping criterion is reached. For instance, we may continue until no region contains more than 5 observations, or when the proportional reduction in \\(RSS\\) is less than some threshold. Once the regions \\(R_1 , R_2, \\ldots, R_M\\) have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs. As a brief side note, the model from Equation (6.1) can also be written in the form \\[f(\\boldsymbol{x}) = \\sum_{m=1}^Mc_mI(\\boldsymbol{x} \\in R_m) = \\sum_{m=1}^Mc_m \\phi(\\boldsymbol{x}; \\boldsymbol{v}_m),\\] where \\(\\boldsymbol{v}_m\\) encodes the choice of variable to split on \\((X_j)\\) and the split point value \\((s)\\) on the path from the start (root) to the \\(m\\)th region (leaf). When viewed as such, a CART model is just an adaptive basis function model (alluded to in the previous chapter), where the basis functions define the regions, and the weights specify the response value in each region (Murphy, 2013, p. 544). One of the advantages of a decision tree is its visual representation which makes for easy interpretation. This is best illustrating via an example. 6.1.1 Example 6  California housing In this example we consider a well-known dataset from the 1990 California census, which contains aggregated data for 20 640 neighbourhoods. The variables include information like the neighbourhoods median house age, the median income, total number of bedrooms and bathrooms, number of households, population, and the location, as can be seen here: library(DT) calif &lt;- read.csv(&#39;data/calif.csv&#39;) datatable(calif, options = list(scrollX = T, pageLength = 6)) For now, we will only use the spatial information (latitude and longitude) to model the target variable: (log of) median house price. We begin by plotting a map of the data  a 2-dimensional feature space  using a colour scale to indicate the target variable. library(maps) library(RColorBrewer) # For colourblind friendly palettes # Replace with log calif$HousePrice &lt;- log(calif$HousePrice) # Make custom colour scale n_breaks &lt;- 9 my_cols &lt;- brewer.pal(n_breaks, &#39;YlOrBr&#39;)[cut(calif$HousePrice, breaks = n_breaks)] # Plot the data plot(calif$Longitude, calif$Latitude, pch = 16, cex = 0.5, bty = &#39;L&#39;, xlab = &#39;Longitude&#39;, ylab = &#39;Latitude&#39;, col = my_cols) legend(x = &#39;topright&#39;, bty = &#39;n&#39;, title = &#39;Log Median House Price&#39;, cex = 0.9, legend = round(seq(min(calif$HousePrice), max(calif$HousePrice), length.out = n_breaks), 2), fill = brewer.pal(n_breaks, &#39;YlOrBr&#39;)) # Add the state border map(&#39;state&#39;, &#39;california&#39;, add = T, lwd = 2) text(-123, 35, &#39;Pacific Ocean&#39;) Figure 6.1: Californian median neighbourhood house prices (logscale) in 1990. We can now proceed to partition the feature space using recursive binary splitting, resulting in a regression tree. There are many packages in R that implement decision trees. We will be using one of the more basic ones: tree. For now we are not going to split into training and testing; for illustrative purposes we shall use the entire dataset. Using the default stopping criteria, we fit the following decision tree shown below in Figure 6.2 library(tree) #Easy as one, two, tree calif_tree &lt;- tree(HousePrice ~ Latitude + Longitude, data = calif) # Plot the tree plot(calif_tree) text(calif_tree) Figure 6.2: Vanilla regression tree fitted to the Cailfornia dataset The corresponding feature space is shown below in Figure 6.3. # Plot the data again plot(calif$Longitude, calif$Latitude, pch = 16, cex = 0.5, col = my_cols, xlab = &#39;Longitude&#39;, ylab = &#39;Latitude&#39;, bty = &#39;o&#39;) # And add the partitioned feature space partition.tree(calif_tree, ordvars = c(&#39;Longitude&#39;, &#39;Latitude&#39;), add = T, lwd = 3) Figure 6.3: Partitioned feature space for vanilla regression tree fitted to the Cailfornia dataset The labels in Figure 6.2 indicate both the splitting variable and the split point. All data points below the split point for that variable go to the left of that branch or, equivalently, are left/below the resulting line in the partitioned feature space. The final regions correspond to the ends of the trees branches, which are indeed referred to as leaves, or terminal nodes. The leaf values are the fitted values for all the data points in that region, which is just the average (Equation (6.3)). It is easy to see visually that the algorithm attempts to group together pockets of most-similar observations. Again, though, the question arises of when to stop? This returns us to the same consideration we encountered in all previous models, namely that of model complexity. 6.1.2 Cost complexity pruning A decision trees complexity is measured by the number of leaves (which is equal to the number of splits + 1). We could continue splitting until every single observation is in its own region, such that the training \\(RSS = 0\\) (each \\(y_i\\) will be equal to its regions average). However, this would be extreme overfitting, and the model would perform very poorly on unseen data. As with all statistical learning algorithms, we need to determine the ideal level of complexity that captures as much information as possible without overfitting. One way of controlling complexity is to continue growing the tree only until the decrease in RSS fails to exceed some high threshold. However, this is short-sighted since a split later on might produce an even greater reduction in RSS. A better strategy is therefore to grow a very large tree and then prune it back to obtain a smaller subtree. Since our goal is to find the subtree with the lowest test error, we can estimate any given subtrees test error using cross-validation. However, estimating the CV error for every possible subtree is not computationally feasible. Instead we only consider a sequence of trees obtained by pruning the full tree back in a nested manner. This tree pruning technique is known as cost complexity pruning, or weakest link pruning, and is essentially a form of regularisation. Let \\(T_0\\) be the full tree and let \\(T\\subseteq T_0\\) be a subtree with \\(|T|\\) terminal nodes or, equivalently, regions in the feature space. We now penalise the RSS according to \\(|T|\\): \\[\\begin{equation} RSS_{\\alpha} = \\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m}\\left[y_i - \\hat{f}(\\boldsymbol{x} _i)\\right]^2 + \\alpha|T|, \\tag{6.4} \\end{equation}\\] where \\(\\alpha \\ge 0\\) is a tuning parameter. When \\(\\alpha = 0\\), \\(RSS_{\\alpha} = RSS\\) and therefore \\(T=T_0\\). For values of \\(\\alpha &gt; 0\\), we seek the subtree \\(T\\) that minimises \\(RSS_{\\alpha}\\). As \\(\\alpha\\) increases, subtrees with a large number of leaves are penalised more heavily and a smaller \\(T\\) will be favoured such that branches are pruned off. This happens in a nested fashion, such that we can easily obtain subtrees as a function of \\(\\alpha\\). By choosing \\(\\alpha\\), we are effectively selecting a subtree; that is, a more parsimonious model that does not overfit the sample data. We want the subtree (and hence \\(\\alpha\\) value) that has the lowest test error and therefore has the best predictive performance on unseen data. Therefore, we will once again use CV to determine this \\(\\alpha\\)/subtree. To illustrate this, we return to the California dataset, this time using all the predictors. # First fit larger tree by relaxing stopping criteria # ?tree.control # minsize: will only consider split if at least so many observations in node # mincut: will only split if both child nodes then have at least so many # mindev: will only consider split if within-node deviance is at least this times root RSS stopcrit &lt;- tree.control(nobs = nrow(calif), mindev = 0.003) calif_bigtree &lt;- tree(HousePrice ~ . , data = calif, control = stopcrit) (s &lt;- summary(calif_bigtree)) ## ## Regression tree: ## tree(formula = HousePrice ~ ., data = calif, control = stopcrit) ## Variables actually used in tree construction: ## [1] &quot;Income&quot; &quot;Latitude&quot; &quot;Longitude&quot; &quot;HouseAge&quot; ## Number of terminal nodes: 25 ## Residual mean deviance: 0.1162 = 2396 / 20620 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.86000 -0.21050 -0.01136 0.00000 0.19030 2.03900 By slightly relaxing the stopping criterion of RSS reduction, we increased the size of the tree to 25. We also see that the only features used to split were income, house age, and latitude and longitude, so the location information was indeed very useful! Plotting the tree at this size is already quite messy (Figure 6.4), with a lot of unnecessary, indistinguishable information at the bottom. plot(calif_bigtree) text(calif_bigtree) Figure 6.4: Large regression tree fitted to the Cailfornia dataset We now apply cost complexity pruning, measuring the CV RSS for each subtree. set.seed(4026) calif_cv &lt;- cv.tree(calif_bigtree) # See str(calif_cv) # size = number of terminal nodes # dev = RSS # k = alpha (tuning parameter that determines tree size) calif_cv$k[1] &lt;- 0 #First entry was -Inf # Plot everything plot(calif_cv$size, calif_cv$dev, type = &#39;o&#39;, pch = 16, col = &#39;navy&#39;, lwd = 2, xlab = &#39;Number of terminal nodes&#39;, ylab = &#39;CV RSS&#39;) axis(side = 1, at = 1:max(calif_cv$size), labels = 1:max(calif_cv$size)) # Add alpha values to the plot: alpha &lt;- round(calif_cv$k) axis(3, at = calif_cv$size, lab = alpha) mtext(expression(alpha), 3, line = 2.5, cex = 1.5) # Highlight the minimum CV Error T &lt;- calif_cv$size[which.min(calif_cv$dev)] abline(v = T, lty = 2, lwd = 2, col = &#39;red&#39;) Figure 6.5: Cross-validated pruning results on the large tree fitted to the Cailfornia dataset We can see that the CV error did indeed keep decreasing. To check this, we show that the minimum RSS is indeed observed at 25 terminal nodes: calif_cv$size[which.min(calif_cv$dev)] ## [1] 25 However, the improvement in results is negligible from about 15 leaves. One could certainly also argue that a tree of size 5 might be preferable, given that it performs comparably to a tree of size 10, and the improvement from that point on is not drastic. In fact, decision trees will rarely be used for prediction purposes, since they will most often be outperformed by other algorithms. Their main value arguably lies in their interpretability, hence we will consider opting for the simpler model. # Prune the tree calif_pruned5 &lt;- prune.tree(calif_bigtree, best = 5) plot(calif_pruned5) text(calif_pruned5) Figure 6.6: Cailfornia housing regression tree pruned to 5 terminal nodes We see that this tree only uses income and latitude as splitting variables, perhaps suggesting that it is too simplistic, especially when compared to the 15-node tree. In Figure 6.7 we see that although the gain in RSS reduction is comparatively limited, the later splits make extensive use of the location information. However, for neighbourhoods with a median income above 3.547, only income and house age were used in subsequent splits, indicating less interaction between income and location in the wealthy areas. # Prune the tree calif_pruned15 &lt;- prune.tree(calif_bigtree, best = 15) plot(calif_pruned15) text(calif_pruned15) Figure 6.7: Cailfornia housing regression tree pruned to 15 terminal nodes This example primarily serves to illustrate the interpretive properties of decision trees. We will explore their sensitivity to sampling variation and their performance on unseen data later once ensembling methods are introduced. First, however, we will see how this method can be used to classify a qualitative response. 6.2 Classification trees Consider now a categorical response variable \\(Y \\in \\{1, 2, \\ldots, K\\}\\). Just like with regression trees, classification trees split the feature space into \\(M\\) regions which correspond to the terminal (leaf) nodes of the tree. In lieu of an average to calculate for the observations falling within each region, the predicted value of an observation with \\(X \\in R_m\\) will now be the most commonly occurring class in \\(R_m\\). The class proportions in each terminal node provide an indication of the reliability of the prediction: \\[\\begin{equation} \\hat{p}_{mk} = \\frac{1}{N_m} \\sum_{i:x_i\\in R_m} I(y_i = k), \\tag{6.5} \\end{equation}\\] where \\(N_m\\) denotes the number of observations in region \\(R_m\\). Although classification trees are also grown by recursive binary splitting, RSS cannot be used as splitting criterion. Now, we could grow the tree by choosing the split that minimises the classification error at each step, i.e.Â minimise the proportion of incorrectly predicted observations. In practice, however, splitting on the reduction in classification error does not produce good trees. We will instead consider the following three splitting criteria, all of which are designed to measure node impurity. 6.2.1 Splitting criteria We would like the leaf nodes to be as homogeneous or pure as possible, with each leaf node ideally containing observations of only one response category. When deciding where to split on a particular dimension, we can use the following measurements to measure the resulting homogeneity. 1. Gini Index Arguably the simplest way of measuring node impurity is via the Gini index (also referred to as Gini impurity or Ginis diversity index), which is defined as follows: \\[\\begin{equation} G = \\sum_{m=1}^{M}\\sum_{k=1}^{K} \\hat{p}_{mk} (1-\\hat{p}_{mk}), \\tag{6.6} \\end{equation}\\] where \\(\\hat{p}_{mk}\\) is the proportion of observations in response category \\(k = 1, \\ldots, K\\) within leaf node \\(m = 1, \\ldots, M\\). At each step during tree growth, we would therefore choose the split that produces the greatest reduction in the Gini index. For a binary response, for example, \\(G_m = 2\\hat{p}_m(1 - \\hat{p}_m)\\). Note that the Gini index is minimised when leaf node \\(m\\) is homogeneous, i.e.Â when \\(\\hat{p}_m = 0\\) or \\(\\hat{p}_m = 1\\). 2. Shannon entropy An alternative way to determine node impurity is to calculate the Shannon entropy of each terminal node, and again sum across all \\(M\\) leaves: \\[\\begin{equation} H = -\\sum_{m=1}^{M}\\sum_{k=1}^{K}\\hat{p}_{mk}\\log(\\hat{p}_{mk}). \\tag{6.7} \\end{equation}\\] This measure, usually just referred to as entropy, stems from information theory and is equivalent to measuring information gain. Note that \\(\\log_2\\) is also often used in the definition, depending on the context. This yields units of bits, whereas the natural logarithm in Equation (6.7) yields nats. Considering again the binary case, Figure 6.8 shows the similarity between the Gini index and entropy, with the misclassification rate included for reference. Note that the entropy has been scaled to go through the point \\((0.5, 0.5)\\). p &lt;- seq(0, 1, 0.001) Gini &lt;- 2*p*(1-p) Entropy &lt;- -p*log(p) - (1-p)*log(1-p) Entropy &lt;- Entropy/max(na.omit(Entropy))*0.5 Misclass &lt;- 0.5 - abs(p - 0.5) plot(p, Entropy, &#39;l&#39;, lwd = 2, col = &#39;orange&#39;, ylab = &#39;&#39;) lines(p, Gini, &#39;l&#39;, lwd = 2, col = &#39;lightblue&#39;) lines(p, Misclass, &#39;l&#39;, lwd = 2, col = &#39;darkgreen&#39;) legend(&#39;bottom&#39;, c(&#39;Entropy&#39;, &#39;Gini Index&#39;, &#39;Misclassification rate&#39;), col = c(&#39;orange&#39;, &#39;lightblue&#39;, &#39;darkgreen&#39;), lty = 1, lwd = 2) Figure 6.8: Gini index, (scaled) entropy, and classification error as a function of node impurity for a binary response. As seen in Figure 6.8, the Gini index and entropy are computationally similar. In fact, the tree package in R uses Gini as a possible splitting criterion, but not entropy. There are several sources on the topic that use ambiguous and, at times, contradictory terminology to refer to Shannon entropy. For example, page 309 of Hastie et al. (2009) refers to the measurement in Equation (6.7) as cross-entropy or deviance, whilst Murphy (2013) uses the term entropy, but also equates it to deviance on page 547. However, on page 221 of Hastie et al. (2009) we see that the term deviance is also used to refer to the quantity \\(2 \\times \\text{NLL}\\) (negative log-likelihood). In fact, it is the NLL that is known as cross-entropy (Bishop, 2006, p. 209). Cross-entropy11 is usually used as a measure of dissimilarity between two probability distributions, typically the predicted class probabilities and the true class probabilities, in which case this loss function is also referred to as log loss. Deviance, which is proportional to NLL, therefore offers a likelihood-based approach to measuring node impurity. 3. Deviance The deviance is constructed by viewing a classification tree as a probability model. Let \\(\\boldsymbol{Y}_m\\), denoting the set of categorical responses in leaf node \\(m\\), be a random sample of size \\(n_m\\) from the multinomial distribution: \\[\\begin{equation} p(\\boldsymbol{y}_m) = \\binom{n_m}{n_{m1} \\cdots n_{mK} } \\prod_{k=1}^{K} p_{mk}^{n_{mk}}. \\tag{6.8} \\end{equation}\\] The likelihood over all \\(M\\) leaf nodes is then given by \\[\\begin{equation} L = \\prod_{m=1}^{M} p(\\boldsymbol{y}_m) \\propto \\prod_{m=1}^{M} \\prod_{k=1}^{K} p_{mk}^{n_{mk}}. \\tag{6.9} \\end{equation}\\] Now, the deviance is defined as \\[\\begin{equation} D = -2\\log(L) = -2 \\sum_{m=1}^{M} \\sum_{k=1}^{K} n_{mk} \\log(p_{mk}). \\tag{6.10} \\end{equation}\\] We want the model that makes the data most probable, i.e.Â has the highest likelihood. This is equivalent to minimising the deviance. When we grow a classification tree, we therefore choose the split that reduces the deviance by the most at each step. Note that the deviance takes the number of observations in each terminal node into account, unlike the Gini index and entropy. 6.2.2 Link between deviance and RSS Returning to regression trees for a moment, if we inspect the object created by the tree() function for the California housing data, we note that the RSS is actually referred to as deviance in the model output. # Model summary (calif_s &lt;- summary(calif_tree)) ## ## Regression tree: ## tree(formula = HousePrice ~ Latitude + Longitude, data = calif) ## Number of terminal nodes: 12 ## Residual mean deviance: 0.1662 = 3429 / 20630 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.75900 -0.26080 -0.01359 0.00000 0.26310 1.84100 # RSS (dev) calif_s$dev ## [1] 3428.558 To make the link between the deviance of a classification tree and the RSS of a regression tree, suppose that for terminal node \\(m\\) the continuous response \\(\\boldsymbol{Y}_m\\) is a random sample of size \\(n_m\\) from \\(N(\\mu_m, \\sigma^2)\\), such that the joint distribution of \\(\\boldsymbol{Y}_m\\) is \\[\\begin{equation} f(\\boldsymbol{y}_m) = \\left(\\frac{1}{\\sigma \\sqrt{2\\pi}} \\right)^{n_m} \\exp \\left(\\frac{-\\sum_{i:x_i\\in R_m} (y_i - \\mu_m)^2}{2\\sigma^2} \\right). \\tag{6.11} \\end{equation}\\] Since we are effectively modelling the \\(\\mu_m\\), the likelihood of the means over all \\(M\\) leaf nodes is given by \\[\\begin{equation} L(\\boldsymbol{\\mu}|\\boldsymbol{x}) = \\prod_{m=1}^{M} f(\\boldsymbol{y}_m) \\propto \\exp \\left(-\\frac{1}{2}\\sum_{m=1}^{M}\\sum_{i:x_i\\in R_m} (y_i - \\mu_m)^2 \\right), \\tag{6.12} \\end{equation}\\] such that the deviance can be expressed as \\[\\begin{equation} D = -2\\log(L) \\propto \\sum_{m=1}^{M}\\sum_{i:x_i\\in R_m} (y_i - \\mu_m)^2 = RSS. \\tag{6.13} \\end{equation}\\] Since the deviance is directly proportional to the RSS in this setting, minimising the one is equivalent to minimising the other. Let us turn our attention back to classification trees and consider managing their complexity. 6.2.3 Cost complexity pruning Just like with regression trees, we must prune a fully grown classification tree to avoid overfitting. However, as with the splitting rules, we cannot use the \\(RSS_{\\alpha}\\) criterion for pruning when the response is categorical. Similar to regression trees though, we will again minimise a specified cost complexity criterion, penalised according to the size of the tree, \\(|T|\\): \\[\\begin{equation} C_{\\alpha}(T) = C(T) + \\alpha|T|, \\tag{6.14} \\end{equation}\\] where \\(C(T)\\) represents a chosen cost function. Although we could use any of the aforementioned criteria, if prediction accuracy of the final pruned tree is the goal, then \\(C(T)\\) should be taken as the misclassification rate of tree \\(T\\). The following section illustrates the application to a binary classification problem. 6.2.4 Example 7  Titanic The famous Titanic dataset contains information on 891 passengers that were aboard the Titanic on its one and only trans-Atlantic journey. The goal is to predict survival (yes/no) based on the predictors shown in the following table: titanic &lt;- read.csv(&#39;data/titanic.csv&#39;, stringsAsFactors = T) titanic$Pclass &lt;- as.factor(titanic$Pclass) #Class coded as integer, should be factor datatable(titanic, options = list(scrollX = T, pageLength = 6)) Most of the features are self-explanatory. SibSp refers to the passengers number of siblings plus spouses on board; Parch to their number of parents and children; and Embarked to their port of embarkation12. To start with, we will first fit a classification tree using Gini index as splitting criterion. titanic_tree_gini &lt;- tree(Survived ~ ., data = titanic, split = &#39;gini&#39;) plot(titanic_tree_gini) text(titanic_tree_gini, pretty = 0) Figure 6.9: Classification tree fitted to the Titanic dataset using Gini index as splitting criterion This yields a rather large tree, where the most useful feature (sex) is only used after a few prior splits. Compare this to the tree resulting from using deviance as splitting criterion (the default in the tree package), shown in Figure 6.10. titanic_tree &lt;- tree(Survived ~ ., data = titanic) plot(titanic_tree) text(titanic_tree, pretty = 0) Figure 6.10: Classification tree fitted to the Titanic dataset using deviance as splitting criterion This presents a similar picture to what one would expect given the emergency protocol of women and children first, although age was only an important factor for 2nd and 3rd class male passengers. We can extract any information for any of the nodes from the fitted object, or add different labels to the terminal nodes (see ?text.tree). As a brief detour, let us show that the deviance measurement provided by the models summary indeed matches the definition set out in Equation (6.10). We will use the frame object contained in the model object to calculate the total deviance of the tree shown in Figure 6.10. f &lt;- titanic_tree$frame #We need the n&#39;s and the probs n_m &lt;- f$n[f$var == &#39;&lt;leaf&gt;&#39;] #This is total n per leaf, not per class! p_n &lt;- f$yprob[,1][f$var == &#39;&lt;leaf&gt;&#39;] #Pr(no) for all leaf nodes p_y &lt;- f$yprob[,2][f$var == &#39;&lt;leaf&gt;&#39;] #Pr(yes) for all leaf nodes nlp &lt;- (n_m*p_n)*log(p_n)+(n_m*p_y)*log(p_y) #\\sum_k (n_k log(p_k)) for all leaves nlp[is.nan(nlp)] &lt;- 0 #Need to deal with log(0) (D &lt;- -2*sum(nlp)) #Deviance ## [1] 563.8652 Comparing this to the total deviance reported, we see that it indeed matches: titanic_s &lt;- summary(titanic_tree) titanic_s$dev ## [1] 563.8652 Note that although the deviance splitting criterion is used in software implementation as was shown here, none of the aforementioned sources explicitly define this criterion in the relevant sections on classification trees. Another way of exploring the fitted tree  more as an exploration tool as opposed to one for reporting  is to simply print the object: titanic_tree ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 714 964.500 No ( 0.59384 0.40616 ) ## 2) Sex: Female 261 290.800 Yes ( 0.24521 0.75479 ) ## 4) Pclass: 3 102 140.800 No ( 0.53922 0.46078 ) ## 8) Fare &lt; 20.8 79 108.500 Yes ( 0.44304 0.55696 ) * ## 9) Fare &gt; 20.8 23 17.810 No ( 0.86957 0.13043 ) * ## 5) Pclass: 1,2 159 69.170 Yes ( 0.05660 0.94340 ) * ## 3) Sex: Male 453 459.900 No ( 0.79470 0.20530 ) ## 6) Pclass: 2,3 352 298.300 No ( 0.84943 0.15057 ) ## 12) Age &lt; 9.5 30 41.050 Yes ( 0.43333 0.56667 ) ## 24) SibSp &lt; 2.5 16 0.000 Yes ( 0.00000 1.00000 ) * ## 25) SibSp &gt; 2.5 14 7.205 No ( 0.92857 0.07143 ) * ## 13) Age &gt; 9.5 322 225.600 No ( 0.88820 0.11180 ) * ## 7) Pclass: 1 101 135.600 No ( 0.60396 0.39604 ) * Here we can see the path containing male passengers of class 2 or 3, age \\(\\leq\\) 9, and siblings \\(\\leq\\) 2 had a total of 16 passengersall of whom survived! The best way to gauge whether this is indeed a pattern in the data (as opposed to a seemingly significant pocket of random variation), is to use cross-validation to estimate out-of-sample performance for pruned trees of different sizes. In the code below we see that one must specify the pruning cost complexity function, which we will set to the misclassification rate. # First, grow a slightly larger tree titanic_bigtree &lt;- tree(Survived ~ ., data = titanic, control = tree.control(nobs = nrow(na.omit(titanic)), mindev = 0.005)) # Then prune it down set.seed(28) titanic_cv &lt;- cv.tree(titanic_bigtree, FUN = prune.misclass) #Use classification error rate for pruning # Make the CV plot plot(titanic_cv$size, titanic_cv$dev, type = &#39;o&#39;, pch = 16, col = &#39;navy&#39;, lwd = 2, xlab = &#39;Number of terminal nodes&#39;, ylab=&#39;CV error&#39;) titanic_cv$k[1] &lt;- 0 #Don&#39;t want no -Inf alpha &lt;- round(titanic_cv$k,1) axis(3, at = titanic_cv$size, lab = alpha, cex.axis = 0.8) mtext(expression(alpha), 3, line = 2.5, cex = 1.2) axis(side = 1, at = 1:max(titanic_cv$size)) T &lt;- titanic_cv$size[which.min(titanic_cv$dev)] #The minimum CV Error abline(v = T, lty = 2, lwd = 2, col = &#39;red&#39;) Figure 6.11: Cross-validated pruning results on a classification tree fitted to the Titanic dataset For the specific seed set above, we see that a more parsimonious model with 4 leaves yields a lower CV error, which in Figure 6.11 is given as the number of misclassifications on the y-axis. Note that different seeds will yield different result. Next we prune the tree down to 4 leaves and plot the result. titanic_pruned &lt;- prune.misclass(titanic_bigtree, best = T) plot(titanic_pruned) text(titanic_pruned, pretty = 0) Figure 6.12: Pruned classification tree fitted to the Titanic dataset From the resulting tree, all males would be classified as not surviving, together with female passengers in class 3 whos fare was more than 20.8. We will once again not focus on prediction for single trees now, although we will include this as a benchmark when considering random forests and boosted trees. In this example we saw that classification trees handle categorical predictors with ease, without the need to code dummy variables. Another benefit of this approach is that we can just as easily model target variables with multiple classes, as illustrated in the following example. 6.2.5 Example 8  Iris Another famous dataset, Iris contains data on 50 flowers from each of 3 species of iris. The measurements are on 4 features: sepal width, sepal length, petal width, and petal length. For the sake of illustrating the decision boundaries in two dimensions, we will only consider the sepal width and length. First, lets plot the data: # Load data data(&#39;iris&#39;) # Create index of species ind &lt;- as.numeric(iris$Species) # Plot plot(Sepal.Width ~ Sepal.Length, iris, xlab = &#39;Sepal Length&#39;, ylab = &#39;Sepal Width&#39;, col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)[ind], pch = c(1, 0, 5)[ind], lwd = 2, cex = 1.2) legend(x = &#39;topright&#39;, legend = levels(iris$Species), pch = c(1, 0, 5), col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;), cex = 1.2) Figure 6.13: Iris data plotted across sepal width and length. Now lets fit a vanilla classification tree to the data and view the resulting partitioned feature space. # Vanilla tree iris_tree &lt;- tree(Species ~ Sepal.Width + Sepal.Length, iris) # Grid for predicting x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), length.out = 65) y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), length.out = 65) fgrid &lt;- expand.grid(Sepal.Length = x, Sepal.Width = y) # Predict over entire space iris_pred &lt;- predict(iris_tree, fgrid, type = &#39;class&#39;) ind_pred &lt;- as.numeric(iris_pred) # Custom colours r &lt;- rgb(255, 0, 0, maxColorValue = 255, alpha = 255*0.25) g &lt;- rgb(0, 255, 0, maxColorValue = 255, alpha = 255*0.25) b &lt;- rgb(0, 0, 255, maxColorValue = 255, alpha = 255*0.25) # Plot the predicted classes plot(fgrid$Sepal.Width ~ fgrid$Sepal.Length, col = c(r, g, b)[ind_pred], pch = 15, xlab = &#39;Sepal Length&#39;, ylab = &#39;Sepal Width&#39;) # And add the data points(Sepal.Width ~ Sepal.Length, iris, col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)[ind], pch = c(1, 0, 5)[ind], lwd = 2, cex = 1.2) legend(x = &#39;topright&#39;, legend = levels(iris$Species), pch = c(1, 0, 5), col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;), cex = 1.2) Figure 6.14: Partitioned feature space resulting from a vanilla classification tree fitted to the iris data. Although these boundaries capture the underlying pattern seemingly well, we might well ask whether non-orthogonal boundaries would not have been more appropriate  either linear or non-linear. This raises the question of which approach/model is best. The answer is that it depends on the problem. For some cases, a particular method might work well, whilst it is entirely inappropriate for others. Figure 6.15, taken from James et al. (2013), shows two hypothetical binary classification scenarios  one where a logistic regression will yield perfect separation whilst the classification tree struggles (top), and one where a classification tree requires only two splits to perfectly capture the decision boundaries, whilst logistic regression performs poorly. Figure 6.15: Top row: A hypothetical binary classification scenario in which logistic regression would outperform a decision tree. Bottom row: A different scenario in which the decision tree would outperform logistic regression. Source: James et al. (2013), p.Â 339. Decision trees have some definite advantages: They are very easy to explain and interpret  easier than linear regression! Residual assumptions and multicollinearity are moot. It mirrors human decision making. Easily handle qualitative predictors without the need to create dummy variables. However, one major drawback is that their predictive accuracy is generally (for most problems) not as good as most other regression and classification approaches. We will now consider two ways of improving on their predictive accuracy, namely random forests and boosting. 6.3 Bagging and random forests The primary reason for the weak performance of decision trees on out-of-sample data is that they suffer from high sampling variability, especially on higher dimensional data. In other words, if we were to take different samples from the same population and fit trees to each sample, the results could look quite different, as seen in the following example. For illustration purposes, consider the Sonar dataset contained in the mlbench package. The task is to determine whether sets of 60 sonar signals pertain to a metal cylinder/mine (M) or a roughly cylindrical rock (R). For every random 70/30 data split we fit a vanilla regression tree and track both the fitted tree and the resulting test classification accuracy. library(mlbench) library(maptree) #Better tree plotting data(Sonar) accs &lt;- c() par(mfrow=c(1,2)) set.seed(4026) for (i in 1:100){ # Split and fit train &lt;- sample(1:nrow(Sonar), 0.7*nrow(Sonar)) my_tree &lt;- tree(formula = Class ~ ., data = Sonar, subset = train) # Predict and evaluate yhat &lt;- predict(my_tree, Sonar[-train,], type = &#39;class&#39;) y &lt;- Sonar[-train, &#39;Class&#39;] accs &lt;- c(accs, sum(diag(table(y, yhat)))/length(y)*100) # Plots draw.tree(my_tree, cex = 0.5) plot(1:i, accs, type = &#39;o&#39;, pch = 16, xlab = &#39;&#39;, ylab = &#39;Classification accuracy (%)&#39;, xlim = c(1, 100), ylim = c(50, 100)) } Figure 6.16: Different trees and test accuracies resulting from different train/test splits on the Sonar dataset Figure 6.16 shows how the fitted model and its testing accuracy can vary extensively across different random samples, with a classification accuracy range of 30.2% in these 100 samples. Although we can manage the complexity  and, therefore, the sampling variance  to some extent through pruning, bagging and random forests offer a much more effective approach. 6.3.1 Bagging Before defining a random forest, we first need to explain the bagging procedure (we will shortly see that a bagged tree model is a special case of a random forest). Bootstrap aggregation, or bagging, is a general purpose procedure for reducing the variance of a statistical learning method by averaging over multiple predictions. To illustrate the concept of variance reduction, consider a set of \\(n\\) independent observations \\(X_1, \\ldots, X_n\\) each with variance \\(\\sigma^2\\). It is well known that \\(\\text{var}\\left({\\bar{X}}\\right) = \\frac{\\sigma^2}{n}\\), i.e.Â averaging over multiple variables reduces the variance. If we had \\(B\\) training sets, we could build a statistical model on each one and average the predictions on the test set: \\[\\hat{y}_{\\text{ave}} = \\frac{1}{B} \\sum_{b=1}^B \\hat{y}_b\\] The average prediction \\(\\hat{y}_{\\text{ave}}\\) will therefore have a lower sampling variance than the prediction \\(\\hat{y}_b\\) for any single training set. However, we of course only have one sample in practice. This is where bootstrap sampling is useful: we can construct multiple bootstrap samples by repeated sampling with replacement from the training set, then grow \\(B\\) unpruned trees on each of these samples. For a continuous response variable we simply average the \\(B\\) predictions from the regression trees for each observation, whilst for categorical responses we take a majority vote, i.e.Â the overall prediction for an observation is the most commonly occurring category among the \\(B\\) predictions. Figure 6.17 illustrates the process for a simple binary classification example with 5 observations and 3 trees. The default number of trees in most R packages is 500, which generally tends to be enough for the error to stabilise, depending on the size of the data. One can add more trees at extra computational cost, although note that since we are only reducing the prediction variability, a large value of \\(B\\) will not lead to overfitting. Figure 6.17: A dummy example illustrating the bagging procedure. The benefit of the bagging procedure above a single tree is easily measured by out-of-sample performance, which in essence means that this procedure does better at capturing the true underlying relationship, \\(f\\). This is seen by investigating the decision boundaries, as shown for simulated linearly separable data in Figure 6.18. library(randomForest) par(mfrow = c(1, 2)) # Simulate random points set.seed(123) x1 &lt;- runif(500, 0, 10) x2 &lt;- runif(500, 0, 10) y &lt;- as.factor(ifelse(x2 &lt;= 10 - x1, 0, 1)) # With a linear decision boundary # Tree first mytree &lt;- tree(y ~ x1 + x2, split = &#39;gini&#39;) # Checked separately that 10 nodes is a good size (verify with cv.tree) pruned_tree &lt;- prune.tree(mytree, best = 10, method = &#39;misclass&#39;) plot(x1, x2, col = ifelse(y == 0, &#39;red&#39;, &#39;navy&#39;), pch = 16, xlim = c(0, 10), ylim = c(0, 10), xlab = expression(X[1]), ylab = &#39;&#39;, cex.axis = 1.2, cex.lab = 1.5, asp = 1) title(ylab = expression(X[2]), line = 1.8, cex.lab = 1.5) #ylab was being cut off # Boundaries abline(10, -1, lwd=3) partition.tree(pruned_tree, add = T, lwd = 6, col = &#39;darkgreen&#39;, cex = 2) x &lt;- seq(0, 10, length = 100) fgrid &lt;- expand.grid(x1 = x, x2 = x) yhat &lt;- predict(pruned_tree, fgrid)[, 2] contour(x, x, matrix(yhat, 100, 100), add = T, levels = 0.5, drawlabels = F, lwd = 4, col = &#39;darkgreen&#39;) # Now bagged model (special case of random forest with m = p) mybag &lt;- randomForest(y ~ x1 + x2, mtry = 2) # Plot plot(x1, x2, col = ifelse(y == 0, &#39;red&#39;, &#39;navy&#39;), pch = 16, xlim = c(0, 10), ylim = c(0, 10), xlab = expression(X[1]), ylab = &#39;&#39;, cex.axis = 1.2, cex.lab = 1.5, asp = 1) title(ylab = expression(X[2]), line = 1.8, cex.lab = 1.5) #ylab was being cut off # Boundaries abline(10, -1, lwd=3) yhat &lt;- predict(mybag, fgrid, type = &#39;prob&#39;)[, 2] contour(x, x, matrix(yhat, 100, 100), add = T, levels = 0.5, drawlabels = F, lwd = 3, col = &#39;darkgreen&#39;) Figure 6.18: Estimated (green) vs actual (black) decision boundaries for a single tree (left) compared to a vanilla bagged model (right). Here we see that the bagged model better approximates the decision boundary. Of course, one would rather fit a linear logistic regression to such a problem, but what about scenarios where the decision boundary is especially non-linear? library(randomForest) library(plotrix) #For drawing the circle par(mfrow = c(1, 2)) # Simulate random points set.seed(123) x1 &lt;- runif(1000, 0, 10) x2 &lt;- runif(1000, 0, 10) circ &lt;- (x1 - 5)^2 + (x2 - 5)^2 y &lt;- as.factor(ifelse(circ &lt;= 3^2, 0, 1)) # Tree first mytree &lt;- tree(y ~ x1 + x2) # Checked separately that 5 nodes is a good size (verify with cv.tree) pruned_tree &lt;- prune.tree(mytree, best = 5, method = &#39;misclass&#39;) plot(x1, x2, col = ifelse(y == 0, &#39;red&#39;, &#39;navy&#39;), pch = 16, xlim = c(0, 10), ylim = c(0, 10), xlab = expression(X[1]), ylab = &#39;&#39;, cex.axis = 1.2, cex.lab = 1.5, asp = 1) title(ylab = expression(X[2]), line = 1.8, cex.lab = 1.5) #ylab was being cut off # Boundaries draw.circle(5, 5, 3, nv = 100, border = &#39;black&#39;, col = NA, lwd = 4) partition.tree(pruned_tree, add = T, lwd = 6, col = &#39;darkgreen&#39;, cex = 2) x &lt;- seq(0, 10, length = 150) fgrid &lt;- expand.grid(x1 = x, x2 = x) yhat &lt;- predict(pruned_tree, fgrid)[, 2] contour(x, x, matrix(yhat, 150, 150), add = T, levels = 0.5, drawlabels = F, lwd = 4, col = &#39;darkgreen&#39;) # Now bagged model (special case of random forest with m = p) mybag &lt;- randomForest(y ~ x1 + x2, mtry = 2) # Plot plot(x1, x2, col = ifelse(y == 0, &#39;red&#39;, &#39;navy&#39;), pch = 16, xlim = c(0, 10), ylim = c(0, 10), xlab = expression(X[1]), ylab = &#39;&#39;, cex.axis = 1.2, cex.lab = 1.5, asp = 1) title(ylab = expression(X[2]), line = 1.8, cex.lab = 1.5) #ylab was being cut off # Boundaries draw.circle(5, 5, 3, nv = 100, border = &#39;black&#39;, col = NA, lwd = 4) yhat &lt;- predict(mybag, fgrid, type = &#39;prob&#39;)[, 2] contour(x, x, matrix(yhat, 150, 150), add = T, levels = 0.5, drawlabels = F, lwd = 3, col = &#39;darkgreen&#39;) Figure 6.19: Estimated (green) vs actual (black) decision boundaries for a single tree (left) compared to a vanilla bagged model (right). Figure 6.19 again shows that although the tree does roughly capture the overall pattern, its orthogonal splits struggle to capture the circular boundary, whereas the bagged model adjusts appropriately to some extent. 6.3.2 Out-of-bag error estimation One convenient upshot of this approach is the built-in capability of estimating the test error without the need for cross-validation. This is due to the fact that each bagged tree only uses approximately 2/3 of the observations on average \\(\\left(\\frac{e-1}{e}\\text{, to be exact}\\right)\\). The remaining ~1/3 observations \\(\\left(\\frac{1}{e}\\right)\\) not used to grow a bagged tree are called out-of-bag (OOB) observations. To obtain an estimate of the test error, we simply predict the response for observation \\(i\\) using each of the bagged trees for which that observation was OOB. This will yield approximately \\(\\frac{B}{3}\\) predictions per observation, which are then combined into a single prediction per observation by either computing the OOB RSS (regression trees) or OOB classification error (classification trees). Since this error rate is based on out-of-sample observations, it serves as an estimate of the test error, just like cross-validation errors. Figure 6.20 again illustrates the process for a simple regression example (note that some rounding was applied for ease of presentation). Figure 6.20: A dummy example illustrating out-of-bag (OOB) error estimation. 6.3.3 Variable importance When we bag a large number of trees, we lose some of the interpretation qualities of decision trees since it is no longer possible to represent the model with a single tree. We can, however, provide a summary of the overall importance of each predictor. To do so, for each predictor we record the amount by which the splitting criterion is improved every time that predictor is selected to split on, then average that value across all splits across all \\(B\\) trees. Now that we have motivated for bagged trees and explored some of their uses, we will make one tweak to the algorithm, thereby defining random forests. 6.3.4 Random forests We have established that bagging reduces the sampling variability of predictions by averaging over many trees. However, if the trees are highly correlated, this improvement will be limited. Consider again a set of random variables \\(X_1, \\ldots, X_n\\) with common variance \\(\\sigma^2\\), but this time suppose they are not independent, such that there is some non-zero correlation. We have that \\[\\text{var}\\left({\\bar{X}}\\right) = \\frac{\\sigma^2}{n} + \\frac{2\\sigma^2}{n^2} \\sum_{i\\ne j} \\rho_{ij}.\\] Therefore, the stronger the correlation, the greater the variance of the average. Random forests provide an improvement over bagged trees by making a small change that decorrelates the trees. Suppose that there is one very strong predictor in a dataset  most of the bagged trees will use this predictor. Consequently, the bagged trees will look quite similar to each other and their predictions will therefore be highly correlated. As with bagging, a random forest is constructed by growing \\(B\\) decision trees on \\(B\\) bootstrapped samples. However, each time a split is considered, a random sample of \\(m &lt; p\\) predictors are chosen as split candidates such that, on average, \\(\\frac{p-m}{p}\\) of the splits will not even consider the strong predictor. This has the effect of decorrelating the trees, causing the average predictions to be less variable between samples, thereby further reducing the variance component of the total error. In the R package randomForest (amongst others) the default is \\(m = \\lfloor \\sqrt{p} \\rfloor\\) for classification trees and \\(m = \\lfloor p/3 \\rfloor\\) for regression trees. Note that bagging is a special case of a random forest with \\(m = p\\). As with bagging, we can estimate the testing error by means of the OOB errors and we can also report the importance of each predictor across all splits across all trees in the forest. We will explore the application by returning to the earlier examples for both regression and classification. 6.3.5 Example 6  California housing (continued) After applying an 80/20 data split into training and testing sets, we will start by using the randomForest package to fit both a vanilla random forest and a bagged model to the California housing dataset, both using 250 trees. We will also keep track of the training times13. # Train/test split set.seed(4026) train &lt;- sample(1:nrow(calif), 0.8*nrow(calif)) # Bagging bag_250_time &lt;- system.time( calif_bag &lt;- randomForest(HousePrice ~ ., data = calif, subset = train, mtry = ncol(calif) - 1, #Use all features (minus response) ntree = 250, importance = T, #Keep track of importance (faster without) #do.trace = 25, #Can keep track of progress if we want na.action = na.exclude) ) # Random Forest rf_250_time &lt;- system.time( calif_rf &lt;- randomForest(HousePrice ~ ., data = calif, subset = train, ntree = 250, importance = T, na.action = na.exclude) ) From the model object we can extract much information on the fit (see calif_rf$...). For example, we can see that the random forests trees contained on average 10985 terminal nodes! We can also look at the percentage of variation in \\(Y\\) captured by the model, i.e.Â the coefficient of determination (\\(R^2\\)), by just printing the model object: calif_bag ## ## Call: ## randomForest(formula = HousePrice ~ ., data = calif, mtry = ncol(calif) - 1, ntree = 250, importance = T, subset = train, na.action = na.exclude) ## Type of random forest: regression ## Number of trees: 250 ## No. of variables tried at each split: 8 ## ## Mean of squared residuals: 0.05236698 ## % Var explained: 83.83 calif_rf ## ## Call: ## randomForest(formula = HousePrice ~ ., data = calif, ntree = 250, importance = T, subset = train, na.action = na.exclude) ## Type of random forest: regression ## Number of trees: 250 ## No. of variables tried at each split: 2 ## ## Mean of squared residuals: 0.05985603 ## % Var explained: 81.52 Verifying this calculation manually (just for the random forest model): y_calif &lt;- calif[train, ]$HousePrice yhat_calif_rf &lt;- calif_rf$predicted # Calculate SSR = 1 - SSE/SST round((1 - sum((y_calif - yhat_calif_rf)^2)/sum((y_calif - mean(y_calif))^2))*100, 2) ## [1] 81.52 Interestingly, we see that the bagged model has the higher \\(R^2\\). Comparing their OOB error plots, we see that the bagged model also yields better (lower) errors: plot(calif_bag$mse, type = &#39;l&#39;, xlab = &#39;Number of trees&#39;, ylab = &#39;OOB MSE&#39;, col = &#39;blue&#39;, lwd = 2, ylim = c(0, max(calif_rf$mse))) lines(calif_rf$mse, col = &#39;darkgreen&#39;, lwd = 2, type = &#39;s&#39;) legend(&#39;topright&#39;, legend = c(&#39;Bagging&#39;, &#39;Random Forest&#39;), col = c(&#39;blue&#39;, &#39;darkgreen&#39;), lwd = 2) Figure 6.21: OOB errors for the bagged tree and random forest (default m) fitted to the California housing dataset. Based on this, if we had to choose a model between these two, we would pick the bagged model. We will, however, apply both to the test set to compare the OOB estimates to the observed test errors. For comparison, let us add the test error for a single pruned tree as well. # Quick single tree stopcrit &lt;- tree.control(nobs = length(train), mindev = 0.002) calif_tree_overfit &lt;- tree(HousePrice ~ ., data = calif[-train, ], control = stopcrit) set.seed(4026) calif_cv &lt;- cv.tree(calif_tree_overfit) T &lt;- calif_cv$size[which.min(calif_cv$dev)] calif_pruned &lt;- prune.tree(calif_tree_overfit, best = T) # Predictions calif_bag_pred &lt;- predict(calif_bag, newdata = calif[-train, ]) calif_rf_pred &lt;- predict(calif_rf, newdata = calif[-train, ]) calif_tree_pred &lt;- predict(calif_pruned, newdata = calif[-train, ]) # Prediction accuracy y_calif_test &lt;- calif[-train, 1] calif_bag_mse &lt;- mean((y_calif_test - calif_bag_pred)^2) calif_rf_mse &lt;- mean((y_calif_test - calif_rf_pred)^2) calif_tree_mse &lt;- mean((y_calif_test - calif_tree_pred)^2) # Plot MSEs plot(calif_bag$mse, type = &#39;l&#39;, xlab = &#39;Number of trees&#39;, ylab = &#39;MSE&#39;, col = &#39;blue&#39;, lwd = 2, ylim = c(0, calif_tree_mse*2)) lines(calif_rf$mse, col = &#39;darkgreen&#39;, lwd = 2) abline(h = calif_bag_mse, col = &#39;blue&#39;, lty = 2, lwd = 2) abline(h = calif_rf_mse, col = &#39;darkgreen&#39;, lty = 2, lwd = 2) abline(h = calif_tree_mse, col = &#39;grey&#39;, lty = 2, lwd = 2) legend(&#39;topright&#39;, legend = c(&#39;Bagging OOB&#39;, &#39;Bagging test&#39;, &#39;Random forest OOB&#39;, &#39;Random forest test&#39;, &#39;Single tree test&#39;), col = c(&#39;blue&#39;, &#39;blue&#39;, &#39;darkgreen&#39;, &#39;darkgreen&#39;, &#39;grey&#39;), lwd = 2, lty = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;, &#39;dashed&#39;)) Figure 6.22: Testing errors for the bagged tree, random forest (default m), and single tree fitted to the California housing dataset, added to the OOB plots. As expected, the model with \\(m=8\\) performed better on the test set. It is also interesting to note that for both models, the OOB error slightly overestimated the testing error. The bagged models test MSE of 0.05 is approximately half of the pruned regression tree test MSE (0.09). Next, we create the variable importance plots. par(mfrow = c(1,2)) varImpPlot(calif_bag, type = 1) varImpPlot(calif_rf, type = 1) Figure 6.23: Variable importance plots for the bagged tree and random forest (default m) fitted to the California housing dataset. This contains the information, although it is not very presentable! One can extract the raw information using the importance function and spruce it up as follows. Since the bagged model yielded superior predictive performance, we will only consider its variable importance. par(mar=c(5,6,4,1) + 0.1) calif_bag_imp &lt;- randomForest::importance(calif_bag, type = 1) calif_bag_imp &lt;- calif_bag_imp[order(calif_bag_imp, decreasing = F), ] barplot(calif_bag_imp, horiz = T, col = &#39;navy&#39;, las = 1, xlab = &#39;Mean decrease in MSE&#39;) Figure 6.24: Variable importance plots for the bagged tree model fitted to the California housing dataset. As expected, we see that median neighbourhood income yielded the greatest average reduction in RSS, followed by the location information. Finally, looking at the training times (in seconds), we see that it executed fairly quickly, even for this moderately-sized dataset: bag_250_time ## user system elapsed ## 231.91 0.08 231.98 rf_250_time ## user system elapsed ## 185.30 0.08 185.38 The ranger package offers inter alia built-in parallelisation across cores to improve runtimes significantly. Fitting the bagged model using this package: library(ranger) ranger_time &lt;- system.time( ranger_bag &lt;- ranger(formula = HousePrice ~ ., data = calif[train, ], num.trees = 250, mtry = ncol(calif) - 1) ) We observe a significantly faster runtime: ranger_time ## user system elapsed ## 20.17 0.05 1.58 The model object also has some neat output to explore. For example, lets investigate the 100th tree in the forest: head(treeInfo(ranger_bag, 100)) ## nodeID leftChild rightChild splitvarID splitvarName splitval terminal prediction ## 1 0 1 2 0 Income 3.84240 FALSE NA ## 2 1 3 4 0 Income 2.45985 FALSE NA ## 3 2 5 6 0 Income 5.77605 FALSE NA ## 4 3 7 8 6 Latitude 34.44500 FALSE NA ## 5 4 9 10 6 Latitude 38.48500 FALSE NA ## 6 5 11 12 1 HouseAge 38.50000 FALSE NA Note, however, that since the trees are fitted in parallel we cannot keep track of OOB MSE as the number of trees increase; we can only extract the final OOB MSE. Another option for faster random forest implementation is Rborist. Wright &amp; Ziegler (2017) compares the two packages across a range of combinations of observations and features. The result is neatly summarised in Figure 6.25, taken from that paper. Figure 6.25: Runtime comparison of ranger with Rborist (classification, 1000 trees per run). Source: Wright &amp; Ziegler (2017). We see that for the California dataset, ranger will still be faster. We can now leverage the speed of this package to answer an important question, namely which value of \\(m\\) is best for this example. In the analyses above we only used \\(m = p = 8\\) and \\(m = \\lfloor p/3 \\rfloor = 2\\), but perhaps another value would yield better results. To determine this, we will use caret to perform a hyperparameter gridsearch (using ranger) across all possible values of \\(m\\). We can also control the tree depth, therefore we will try different size trees as well. We will again use 250 trees. library(caret) set.seed(1) # Create combinations of hyperparameters # see getModelInfo()$ranger$parameters calif_rf_grid &lt;- expand.grid(mtry = 2:(ncol(calif) - 1), splitrule = &#39;variance&#39;, #Must specify. This is RSS for regression min.node.size = c(1, 5, 20)) #Default for regression is 5. Controls tree size ctrl &lt;- trainControl(method = &#39;oob&#39;, verboseIter = F) #Can set to T to track progress # Use ranger to run all these models calif_rf_gridsearch &lt;- train(HousePrice ~ ., data = calif, subset = train, method = &#39;ranger&#39;, num.trees = 250, #verbose = T, trControl = ctrl, tuneGrid = calif_rf_grid) #Here is the grid The hyperparameter combination that yielded the best OOB result is as follows: calif_rf_gridsearch$finalModel ## Ranger result ## ## Call: ## ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x, mtry = min(param$mtry, ncol(x)), min.node.size = param$min.node.size, splitrule = as.character(param$splitrule), write.forest = TRUE, probability = classProbs, ...) ## ## Type: Regression ## Number of trees: 250 ## Sample size: 16512 ## Number of independent variables: 8 ## Mtry: 7 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 0.05215617 ## R squared (OOB): 0.8389851 According to this process, the best random forest model was one with \\(m =\\) 7 and terminal nodes of size 5. A particularly useful feature of carets train() function is the trivially easy plotting of the hyperparameter tuning results, allowing one to see not only which combination was best, but also how it compared to others, as well as the effect of changing certain hyperparameters. This is shown in Figure 6.26. plot(calif_rf_gridsearch) Figure 6.26: Hyperparameter combinations results for a random forest fitted to the California housing dataset. There does not seem to be much difference between using entirely overfitted trees (minimal node size = 1) and ones with a minimal node size of 5, but increasing the node size to 20 (reducing the trees size) yielded worse performance for all values of \\(m\\). Although there is not much in it, we will now use the model that yielded the lowest OOB error to predict on the test set. calif_rfgrid_pred &lt;- predict(calif_rf_gridsearch, newdata = calif[-train,]) mean((y_calif_test - calif_rfgrid_pred)^2) ## [1] 0.05024634 Comparing this to the \\(m=8\\) model from earlier, we see a miniscule improvement on its test MSE of 0.0502557, although the difference is negligible. To reiterate: in practice, one would only use the test set to get one final prediction after selecting a model based on CV/OOB errors. This test set comparison was merely for illustration. We can now apply the same procedure to the Titanic dataset. 6.3.6 Example 8  Titanic (continued) In practice one might want to jump straight to the hyperparameter tuning (after doing the necessary exploration and cleaning, of course). However, since we are still exploring and comparing these methods, we will again first see how the bagged model compares to the default random forest, with a single pruned tree again added for reference. This step also helps to determine whether the number of trees is sufficient. Since the Titanic dataset is much smaller than the California data, we will use a 70/30 split to be more certain of our out-of-sample performance. # Train/test split titanic &lt;- na.omit(titanic) set.seed(1) train &lt;- sample(1:nrow(titanic), 0.7*nrow(titanic)) # Bagging titanic_bag &lt;- randomForest(Survived ~ ., data = titanic, subset = train, mtry = ncol(titanic) - 1, ntree = 1000) # Random Forest titanic_rf &lt;- randomForest(Survived ~ ., data = titanic, subset = train, ntree = 1000) # Quick single tree stopcrit &lt;- tree.control(nobs = length(train), mindev = 0.005) titanic_tree_overfit &lt;- tree(Survived ~ ., data = titanic, subset = train, control = stopcrit) titanic_cv &lt;- cv.tree(titanic_tree_overfit) T &lt;- titanic_cv$size[which.min(titanic_cv$dev)] titanic_pruned &lt;- prune.tree(titanic_tree_overfit, best = T) # Predictions titanic_bag_pred &lt;- predict(titanic_bag, newdata = titanic[-train, ]) titanic_rf_pred &lt;- predict(titanic_rf, newdata = titanic[-train, ]) titanic_tree_pred &lt;- predict(titanic_pruned, newdata = titanic[-train, ], type = &#39;class&#39;) # Prediction accuracy y_titanic_test &lt;- titanic[-train, 1] titanic_bag_mis &lt;- mean(y_titanic_test != titanic_bag_pred) titanic_rf_mis &lt;- mean(y_titanic_test != titanic_rf_pred) titanic_tree_mis &lt;- mean(y_titanic_test != titanic_tree_pred) # Plot errors plot(titanic_rf$err.rate[, &#39;OOB&#39;], xlab = &#39;Number of trees&#39;, ylab = &#39;Classification error&#39;, type = &#39;s&#39;, col = &#39;darkgreen&#39;, ylim = c(0.1, titanic_tree_mis)) lines(titanic_bag$err.rate[, &#39;OOB&#39;], col = &#39;blue&#39;, type = &#39;s&#39;) abline(h = titanic_bag_mis, col = &#39;blue&#39;, lty = 2, lwd = 2) abline(h = titanic_rf_mis, col = &#39;darkgreen&#39;, lty = 2, lwd = 2) abline(h = titanic_tree_mis, col = &#39;grey&#39;, lty = 2, lwd = 2) legend(&#39;bottomright&#39;, legend = c(&#39;Bagging OOB&#39;, &#39;Bagging test&#39;, &#39;Random forest OOB&#39;, &#39;Random forest test&#39;, &#39;Single tree test&#39;), col = c(&#39;blue&#39;, &#39;blue&#39;, &#39;darkgreen&#39;, &#39;darkgreen&#39;, &#39;grey&#39;), lwd = 2, lty = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;, &#39;dashed&#39;)) Figure 6.27: Testing errors for the bagged tree, random forest (default m), and single tree fitted to the Titanic dataset, added to the OOB error plots. For this dataset  which is again considerably smaller than the California problem  we see that the OOB errors underestimated the testing errors. Although the default random forest was superior, we would again like to check all possible values of \\(m\\). We will also try two different splitting criteria: Gini index and Hellinger distance. See Aler et al. (2020) for information on the latter. set.seed(2) # Create combinations of hyperparameters titanic_rf_grid &lt;- expand.grid(mtry = 2:(ncol(titanic) - 1), splitrule = c(&#39;gini&#39;, &#39;hellinger&#39;), min.node.size = c(1, 5, 10)) ctrl &lt;- trainControl(method = &#39;oob&#39;, verboseIter = F) # Use ranger to run all these models titanic_rf_gridsearch &lt;- train(Survived ~ ., data = titanic, subset = train, method = &#39;ranger&#39;, num.trees = 1000, trControl = ctrl, tuneGrid = titanic_rf_grid, importance = &#39;impurity&#39;) plot(titanic_rf_gridsearch) Figure 6.28: Hyperparameter combinations results for a random forest fitted to the Titanic dataset. In Figure 6.28 we see that both splitting rules yield the highest accuracy when \\(m=4\\) and minimum node size is 5, with the Gini index split being superior at this combination. Note that these models fitted to a relatively small dataset like this one can be susceptible to sampling variation. As a homework exercise, rerun the above procedure multiple times and observe the change in results. Next, we use the model that did best in OOB performance for prediction. titanic_rfgrid_pred &lt;- predict(titanic_rf_gridsearch, newdata = titanic[-train,]) titanic_rf_tuned_mis &lt;- mean(y_titanic_test != titanic_rfgrid_pred) Tuning the random forests hyperparameters brought the misclassification rate down from 22.33% to 21.4%. Finally, lets consider the variable importance plot. plot(varImp(titanic_rf_gridsearch)) Figure 6.29: Variable importance plot for the random forest fitted to the Titanic dataset. Hyperparameter tuning has been applied to the model. The variable importance for this model  which has been optimised for classification accuracy  is measured by decrease in Gini index. Passenger sex is by far the most important factor, followed by age and fare, which were similar. Whether or not they were 3rd class passengers is somewhat important (also refer back to Figure 6.10, where we used the entire dataset). Although there are many modern extensions to random forests, those will be left for future exploration. Our final topic in this section is another, different form of tree ensembling, namely boosting. 6.4 Gradient boosting Boosting describes a general purpose family of algorithms that ensemble many weak learners into strong learners by letting the base models sequentially learn from the mistakes made by the previous ones, in contrast to bagging which grow all trees independently. The weak learner can be any classification or regression algorithm, but it is most common to use a decision tree, specifically one with few splits. There are several boosting variations, the more popular ones being AdaBoost and so-called stochastic gradient boosting (it has been shown that this boosting can be interpreted as a form of gradient descent in function space). Our application will entail the latter, which is a more generic and flexible algorithm. Boosting can be applied to both regression and classification trees  which we will do  although our exploration of the algorithm will focus only on the regression case. The main idea of boosting is to let the algorithm gradually learn from its mistakes by starting with a base prediction, for example the mean of the target variable. A regression tree with a small number of splits \\((d)\\), is fitted to the residuals of this base prediction. The (scaled) residuals of this tree are then treated as the response variable used to grow the next tree with \\(d\\) splits. The (scaled) residuals of the new tree are then used to grow another tree with \\(d\\) splits, and so on This yields a sequence of \\(B\\) trees, each one accounting for some of the variation in \\(y\\) that was not explained by the previous trees, gradually bringing the residuals down to zero. Therefore, the model eventually overfits as the number of trees increases, and we make use of cross-validation (CV) to determine at which point this occurs. The initial trees will capture the strongest patterns in the data and will contribute the most to RSS reduction, whilst subsequent trees will lead to smaller improvements. We limit the rate of learning firstly by using trees with few splits, and secondly via a shrinkage parameter \\(\\lambda\\) that down-weights the contribution of each tree. In general, methods that learn slowly tend to perform well, although this slow learning approach requires many more trees than a random forest. The steps are laid out in the following algorithm: Algorithm 1 Gradient Tree Boosting (as implemented in R) Let \\(\\hat{y}^{(b)}\\) be the predictions after fitting \\(b\\) trees Let \\(\\hat{r}^{(b)}\\) be the predicted values (of the residuals) of tree \\(b\\) Let \\(r^{(b)}\\) be the residuals after fitting \\(b\\) trees Set \\(\\hat{y}^{(0)} = \\bar{y}\\) such that \\(r^{(0)} = y - \\bar{y}\\) For \\(b = 1, \\ldots, B\\): Fit a regression tree with \\(d\\) splits to \\((X, r^{(b-1)})\\) Using the predictions \\(\\hat{r}^{(b)}\\) of the current tree, update the predictions \\[\\hat{y}^{(b)} = \\hat{y}^{(b-1)} + \\lambda\\hat{r}^{(b)}\\] Updated the residuals using the current predictions \\[r^{(b)} = r^{(b-1)} - \\lambda \\hat{r}^{(b)}\\] Calculate the models final predictions \\[ \\hat{y} = y - r^{(B)} = \\bar{y} + \\lambda \\sum_{b=1}^{B} \\hat{r}^{(b)} \\] This algorithm works remarkably well on a variety of problems despite (perhaps because of) its simplicity. In the code below we initiate the algorithm from scratch and apply it to a dummy data set. library(rpart) set.seed(4026) n &lt;- 100 x1 &lt;- round(runif(n, 0, 10), 0) x2 &lt;- round(runif(n, 0, 10), 0) y &lt;- round(10 + 3*x1 + x2 + rnorm(n, 0, 10)) dat &lt;- data.frame(cbind(y, x1, x2)) # Initiate algorithm &quot;by hand&quot; gb.manual &lt;- function(lambda, d, ntrees){ train_mse &lt;&lt;- c() #To keep track of training error resids &lt;&lt;- c() #To keep track of residuals rhats &lt;&lt;- c() #To keep track of tree predictions yhat_b &lt;- mean(y) #First initialisation of prediction -&gt; step 1 r_b &lt;- y - yhat_b #First initialisation of residuals -&gt; step 1 for(b in 1:ntrees){ ctrl &lt;- rpart.control(maxdepth = 1, cp = 0.001) t &lt;- rpart(r_b ~ x1 + x2, control = ctrl) #Fit tree to residuals -&gt; step 2a rhat_b &lt;- predict(t, newdata = dat) #Predicted values of tree b -&gt; step 2b yhat_b &lt;- yhat_b + lambda*rhat_b #Updated prediction -&gt; step 2b r_b &lt;- y - yhat_b #Updated residuals -&gt; step 2c # r_b &lt;- r_b - lambda*rhat_b #Could also write it like this, same thing # Save all the training mses, residuals, and tree predictions train_mse &lt;&lt;- c(train_mse, mean((y - yhat_b)^2)) #Double arrow saves object outside function resids &lt;&lt;- cbind(resids, r_b) rhats &lt;&lt;- cbind(rhats, rhat_b) } return(mean(y) + lambda*apply(rhats, 1, sum)) #Output final predictions -&gt; step 3 # Could also return y - resids[, ntrees] # Or just yhat_b } #Run algorithm yhat_manual &lt;- gb.manual(lambda = 0.01, d = 1, ntrees = 1000) #Final model training error (gbm_manual_train_mse &lt;- mean((y - yhat_manual)^2)) # Or just train_mse[ntrees] ## [1] 75.20785 In order to validate this procedure, let us compare the results with those obtained from using the gbm package. library(gbm) gbm_lib &lt;- gbm(y ~ ., data = dat, distribution = &#39;gaussian&#39;, #Applies squared error loss n.trees = 1000, interaction.depth = 1, shrinkage = 0.01, bag.fraction = 1) #Subsamples observations by default (0.5) yhat_gbm &lt;- gbm_lib$fit (mse_gbm &lt;- mean((y - yhat_gbm)^2)) ## [1] 75.20785 We see that the training MSEs match exactly. As a final sanity check, lets compare each trees training error between the manual implementation and the packages. all(round(gbm_lib$train.error, 10) == round(train_mse, 10)) ## [1] TRUE #Some rounding issue creeps in after 10 decimal places (exacerbated by larger trees) In the above algorithm description and example we see that there are three main hyperparameters that must be specified beforehand: The number of trees  \\(B\\) Unlike bagging and random forests, boosting can lead to overfitting if \\(B\\) is too large. We use cross-validation to select \\(B\\). The shrinkage parameter or learning rate  \\(\\lambda\\) Typical values are 0.01 or 0.001 (the default in gbm). Smaller values require more trees. The maximum number of splits in each tree  \\(d\\) Also called the interaction depth of the boosted model, since \\(d\\) splits can involve at most \\(d\\) variables. Depending on the size of the problem, \\(d=1\\) can work well when fitting many trees. Although these are the main hyperparameters, one can also further control the tree depth (useful when larger trees are preferred), as well as introduce randomisation by randomly subsampling the training set, somewhat reminiscent of the bagging process. To illustrate the application, let us return to the California housing example. 6.4.1 Example 6  California housing (continued) Since this is our final analysis of this dataset, we will apply some feature engineering, a process of adapting/transforming/combining the existing information into new features to improve model performance. Instead of using the total number of bedrooms, bathrooms, and people per neighbourhood, lets use the number of households to convert these into per-house rates. We will also apply a 70/30 train/test split, here showing how a caret function can be used to do so. library(dplyr) # Create calif feature engineered calif_fe &lt;- calif %&gt;% mutate(AveRooms = Rooms/Households, AveBedrms = Bedrooms/Households, AveOccup = Population/Households) %&gt;% select(-Rooms, -Bedrooms, -Households) ## Using caret for train/test split: set.seed(4026) train_index &lt;- createDataPartition(y = calif_fe$HousePrice, p = 0.8, list = FALSE) calif_fe_train &lt;- calif_fe[train_index,] calif_fe_test &lt;- calif_fe[-train_index,] As some benchmark for comparison  and to see whether the engineered features are useful  we will first fit a single regression tree. # Slightly larger tree calif_fe_tree &lt;- tree(HousePrice ~ ., data = calif_fe_train, control = tree.control(nobs=nrow(calif), mindev = 0.003)) # Tune set.seed(1) calif_fe_cv &lt;- cv.tree(calif_fe_tree) calif_fe_cv$k[1] &lt;- 0 plot(calif_fe_cv$size, calif_fe_cv$dev, type = &#39;o&#39;, pch = 16, col = &#39;navy&#39;, lwd = 2, xlab = &#39;Number of terminal nodes&#39;, ylab = &#39;CV RSS&#39;) axis(side = 1, at = 1:max(calif_fe_cv$size), labels = 1:max(calif_fe_cv$size)) alpha &lt;- round(calif_fe_cv$k) axis(3, at = calif_fe_cv$size, lab = alpha) mtext(expression(alpha), 3, line = 2.5, cex = 1.5) Figure 6.30: Cross-validated pruning results from a tree fitted to the feature-engineered Cailfornia housing training set. The CV RSS first reaches a minimum at 12 terminal nodes, hence we will prune it down to that size. calif_fe_pruned &lt;- prune.tree(calif_fe_tree, best = 12) plot(calif_fe_pruned) text(calif_fe_pruned) Figure 6.31: Pruning tree fitted to the feature-engineered Cailfornia housing training set. Here we see that the average number of rooms and average occupancy were indeed useful in this model (compare to Figure 6.7, which split almost exclusively on income and location). The CV RMSE is 0.387. Next, we fit a gradient boosted trees model to the training set via the gbm package, using caret to apply hyperparameter tuning according to 10-fold CV (which can be parallelised). Note that for a problem of this size, the gbm model does take a while to run. Therefore, the code run below has been commented out after running, with the results saved and loaded back in for quick reproducibility. # set.seed(4026) # ctrl &lt;- trainControl(method = &#39;cv&#39;, number = 10, verboseIter = T) # # calif_gbm_grid &lt;- expand.grid(n.trees = c(1000, 10000, 50000), # interaction.depth = c(1, 2, 5), # shrinkage = c(0.01, 0.001), # n.minobsinnode = 10) # # calif_gbm_gridsearch &lt;- train(HousePrice ~ ., data = calif_fe_train, # method = &#39;gbm&#39;, # distribution = &#39;gaussian&#39;, # trControl = ctrl, # verbose = T, # tuneGrid = calif_gbm_grid) # save(calif_gbm_gridsearch, file = &#39;data/gbm_calif_50k.Rdata&#39;) load(&#39;data/gbm_calif_50k.Rdata&#39;) Although we only touch on the subject here, the hyperparameter tuning procedure can be an entire topic on its own. One alternative to the grid search applied above  whereby all combinations of fixed sets of hyperparameter values are used  is to apply a random search, which introduces some variation in the hyperparameters. Bergstra &amp; Bengio (2012) initially provided a strong argument for this approach within a neural network context. Further exploration of these topics is left for the reader. Looking at the CV results in Figure 6.32, we see that higher values of \\(d\\) in fact yielded better results. As expected, the higher learning rate reduced the CV error quicker for the same number of trees. plot(calif_gbm_gridsearch) Figure 6.32: Hyperparameter combinations results for a gradient boosted trees model fitted to the feature-engineered California housing dataset. However, it is important to note that for this particular combination of number of trees it is difficult to determine whether the error is still decreasing at 50,000 trees. To ascertain this, we will use a large number of trees and see where the CV error reaches a minimum, as shown in Figure 6.33. # calif_fe_gbm &lt;- gbm(HousePrice ~ ., data = calif_fe_train, # distribution = &#39;gaussian&#39;, # n.trees = 100000, # interaction.depth = 5, # shrinkage = 0.01, # bag.fraction = 1, # cv.folds = 10, #built-in CV # n.cores = 10, #which can be parallelised # verbose = T) # save(calif_fe_gbm, file = &#39;data/gbm_calif_100k.Rdata&#39;) load(&#39;data/gbm_calif_100k.Rdata&#39;) d &lt;- gbm.perf(calif_fe_gbm) legend(&#39;topright&#39;, c(&#39;CV error&#39;, &#39;Training error&#39;), col = c(&#39;green&#39;, &#39;black&#39;), lty = 1) Figure 6.33: Cross-validation results for the gradient boosted trees model with \\(\\lambda=0.01\\) and \\(d=5\\) fitted to the feature-engineered California housing dataset. It turns out the CV error actually started increasing after 39731 trees. However, it is possible that one of the other hyperparameter combinations (different \\(\\lambda\\) and \\(d\\)) would have yielded a lower CV if allowed to run for more than 50,000 trees. This example serves to illustrate that this style of hyperparameter tuning can be tedious, time-consuming, and a trial-and-error process. Whilst more sophisticated tuning techniques exist, for example Bayesian optimisation, they are beyond the scope of this course. The gbm() package contains some convenient functions that one can apply to the model object. For example, extracting and plotting variable importance can be achieved as follows: par(mar=c(5,6,4,1) + 0.1) calif_gbm_varimp &lt;- summary(calif_fe_gbm, n.trees = d, las = 1, xlim = c(0, 50)) Figure 6.34: Variable importance for the gbm model fitted to the feature-engineered California housing dataset. Variable importance is measured in the same way as in the random forest model, i.e.Â by tracking the average reduction in the loss function resulting from splitting on each feature. Once again we see that median income and the location variables are the most important features, although two of our engineered features, average occupancy and average rooms were also somewhat infuential. Although variable importance plots tell us which features are most important for predicting the response, they do not tell us how the predictors influence the predictions. For this, we make use of partial dependence plots. 6.4.2 Partial dependence plots Partial dependence functions can be used to interpret the results of any black box learning method by measuring the relationship between certain features and the approximation of \\(f(\\boldsymbol{X})\\). For \\(p\\) features, \\(X_1, \\ldots, X_p\\), consider some feature \\(X_j\\) and let \\(X_{-j}\\) denote all other features. One way to define the average or partial dependence of \\(f(\\boldsymbol{X})\\) on \\(X_j\\) is \\[f_j(X_j) = E_{X_{-j}}f(X_j, X_{-j}),\\] which can be estimated by \\[\\bar{f}_j(X_j) =\\frac{1}{N} \\sum_{i=1}^N f(X_j, X_{i,-j}).\\] This must be done for each value of \\(X_j\\) and can be computationally intensive, even for moderately-sized datasets. Note that these partial dependence functions represent the effect of \\(X_j\\) on \\(f(\\boldsymbol{X})\\) after accounting for the average effects of the other features on \\(f(\\boldsymbol{X})\\), as opposed to the effect of \\(X_j\\) on \\(f(\\boldsymbol{X})\\) when ignoring the effects of the \\(X_{-j}\\). Unfortunately, visualisation of these relationships is limited to low-dimensional views. Although these slices of information can seldom provide a comprehensive depiction of the approximation, it can still provide a useful indication of how certain features influenced the predictions on average, especially when their interaction with other variables is limited. The gbm package again allows us to easily produce the partial dependence plots. Figure 6.35 shows 1-dimensional partial dependence plots for the previously fitted model on the (feature-engineered) California dataset for four of the (non-location) features. library(gridExtra) library(grid) ylims &lt;- c(11.5,12.8) p1 &lt;- plot.gbm(calif_fe_gbm, &#39;Income&#39;, d, ylim = ylims, ylab = &#39;&#39;) p2 &lt;- plot.gbm(calif_fe_gbm, &#39;AveRooms&#39;, d, ylim = ylims, ylab = &#39;&#39;, xlab = &#39;Average Rooms&#39;) p3 &lt;- plot.gbm(calif_fe_gbm, &#39;AveOccup&#39;, d, ylim = ylims, ylab = &#39;&#39;, xlab = &#39;Average Occupancy&#39;) p4 &lt;- plot.gbm(calif_fe_gbm, &#39;HouseAge&#39;, d, ylim = ylims, ylab = &#39;&#39;, xlab = &#39;House Age&#39;) grid.arrange(p1, p2, p3, p4, ncol = 2) grid.text(&#39;Partial Predicted Log House Price&#39;, x = 0, vjust = 1, rot = 90) Figure 6.35: Partial dependence plots for the gbm model fitted to the feature-engineered California housing dataset. Looking at the average occupancy and average rooms, something is not right  an average occupancy of more than 1000 does not make sense, nor does more than 100 rooms on average! It is most likely an errant observation(s) with too few households. calif[which.max(calif_fe$AveOccup),] ## HousePrice Income HouseAge Rooms Bedrooms Population Households Latitude Longitude ## 19007 11.83138 10.2264 45 19 5 7460 6 38.32 -121.98 calif[which.max(calif_fe$AveRooms),] ## HousePrice Income HouseAge Rooms Bedrooms Population Households Latitude Longitude ## 1915 13.12237 1.875 33 1561 282 30 11 38.91 -120.1 Indeed, six households for a population of 7460 is nonsensical. 1561 rooms for 11 households and a population of 30 is questionable, given some of the areas filled with mansions one finds in California. This goes to show how important an exploratory data analysis and cleaning are at the start of an analysis! These are just the observations yielding the maximum average occupancy and average rooms values, one could explore their distributions further. It would be best to rerun the analysis with the cleaned data, which is left as a homework exercise. For now, we will just cut off these outliers, for which we can use a package offering more options for partial dependence plots. pdp is a popular package for this. library(pdp) p1 &lt;- partial(calif_fe_gbm, &#39;Income&#39;, n.trees = d) p2 &lt;- partial(calif_fe_gbm, &#39;AveRooms&#39;, n.trees = d, trim.outliers = T) p3 &lt;- partial(calif_fe_gbm, &#39;AveOccup&#39;, n.trees = d, trim.outliers = T) p4 &lt;- partial(calif_fe_gbm, &#39;HouseAge&#39;, n.trees = d) grid.arrange(plotPartial(p1, ylab = &#39;&#39;, ylim = ylims), plotPartial(p2, ylab = &#39;&#39;, ylim = ylims, xlab = &#39;Average Rooms&#39;), plotPartial(p3, ylab = &#39;&#39;, ylim = ylims, xlab = &#39;Average Occupancy&#39;), plotPartial(p4, ylab = &#39;&#39;, ylim = ylims, xlab = &#39;House Age&#39;), ncol = 2) grid.text(&#39;Partial Predicted Log House Price&#39;, x = 0, vjust = 1, rot = 90) Figure 6.36: Partial dependence plots for some of the features in the gbm model fitted to the feature-engineered California housing dataset. Some of the x-axes have been rescaled. The lack of discernible relationship between house age and predicted house price reflects its low variable importance, noting that the y-axes have been adjusted such that the features are directly comparable. For income and average rooms we observe a general positive relationship, whilst increasing average occupancy initially increases the predicted house price (although increasing the resolution could add a lot of noise between one and two occupants), before it gradually decreases. Finally, we can also look at the partial dependence of two features simultaneously. To come full-circle with this example, we will do so for the latitude and longitude. In order to add the map border, we will extract the partial dependence information. latlong_pd &lt;- plot.gbm(calif_fe_gbm, c(&#39;Longitude&#39;, &#39;Latitude&#39;), return.grid = T) # Same colour scale as before n_breaks &lt;- 9 my_cols &lt;- brewer.pal(n_breaks, &#39;YlOrBr&#39;)[cut(latlong_pd$y, breaks = n_breaks)] # Plot the partial dependence values plot(latlong_pd$Longitude, latlong_pd$Latitude, pch = 15, xlab = &#39;Longitude&#39;, ylab = &#39;Latitude&#39;, col = my_cols) legend(x = &#39;topright&#39;, bty = &#39;n&#39;, title = &#39;Log Median House Price&#39;, cex = 0.9, legend = round(seq(min(latlong_pd$y), max(latlong_pd$y), length.out = n_breaks), 2), fill = brewer.pal(n_breaks, &#39;YlOrBr&#39;)) # Add the state border map(&#39;state&#39;, &#39;california&#39;, add = T, lwd = 2) text(-123, 35, &#39;Pacific Ocean&#39;) Figure 6.37: Partial dependence plot of the location information for the gbm model fitted to the feature-engineered California housing dataset. In Figure 6.37 we see that the model extrapolates beyond the state boundaries, extending the general trend observed earlier of the more south-west, the more expensive. This completely messes up the colour scale, so to get a more realistic and useful picture let us rather only plot the partial dependence information pertaining to populated locations. library(latticeExtra) locs &lt;- distinct(calif, Longitude, Latitude) p &lt;- partial(calif_fe_gbm, c(&#39;Longitude&#39;, &#39;Latitude&#39;), locs, n.trees = d) # Same colour scale as before n_breaks &lt;- 9 my_cols &lt;- colorRampPalette(brewer.pal(n_breaks, &#39;YlOrBr&#39;)) pp &lt;- plotPartial(p, col.regions = my_cols(100)) # Add the state border. plotPartial creates a lattice be default my_map &lt;- map_data(&#39;state&#39;, &#39;california&#39;) border_layer &lt;- latticeExtra::layer(panel.polygon(my_map, col = &#39;transparent&#39;)) pp + border_layer Figure 6.38: Partial dependence plot for the observed location information for the gbm model fitted to the feature-engineered California housing dataset. Comparing this with Figure 6.1, we can see how the gbm model learned how location relates to house price. So far we have only discussed the base version of stochastic gradient boosting, but there are many variants that use the same basic principle whilst making changes to some aspects of the algorithm. One particularly powerful, relatively recent development is Extreme Gradient Boosting (XGBoost). 6.4.3 XGBoost Since Chen &amp; Guestrin (2016) first proposed XGBoost, it has become a highly popular choice for solving a wide range of machine learning problems, especially structured/tabular data. For the sake of scope we will not delve into the details of its implementation; we only summarise some of the key innovations of the algorithm here: Regularisation: XGBoost incorporates \\(L_1\\) and \\(L_2\\) regularisation terms into its objective function to prevent overfitting. This helps in controlling the complexity of the learned trees. Newton boosting: Computes the second derivative (Hessian) of the loss function with respect to the predicted scores, allowing for more precise updates to the tree structure and yielding faster convergence. Tree pruning: Trees are grown to a maximum depth and then pruned iteratively, which helps in reducing overfitting and computational complexity. Parallel processing: It is designed for parallel processing, making it faster and more scalable. Subsampling: Row subsampling helps prevent overfitting; column subsampling adds extra randomisation, similar to random forests. Missing values: It automatically learns how to partition data points with missing values and assign them to the appropriate child nodes during tree construction. To illustrate its application, we will analyse the California data one final time. The required package in R is xgboost, although we will again implement it via caret for hyperparameter tuning. The only hyperparameters we will search over are \\(B\\) from 5000 to 40000 in increments of 5000; \\(d \\in (6, 8)\\); \\(\\lambda \\in (0.005, 0.01)\\); and proportion of features used per tree (similar to \\(m\\) in the random forest, only at tree level) set to 0.5 and 1. Note that separate models are not run for different numbers of trees; the chain is simply extended. Therefore, we can actually use more granular trees than we did above. Below we see all the hyperparameters than must be specified when creating a search grid for XGBoost. getModelInfo()$xgbTree$parameters ## parameter class label ## 1 nrounds numeric # Boosting Iterations ## 2 max_depth numeric Max Tree Depth ## 3 eta numeric Shrinkage ## 4 gamma numeric Minimum Loss Reduction ## 5 colsample_bytree numeric Subsample Ratio of Columns ## 6 min_child_weight numeric Minimum Sum of Instance Weight ## 7 subsample numeric Subsample Percentage # calif_xgb_grid &lt;- expand.grid(nrounds = seq(5000, 40000, 5000), #number of trees # max_depth = c(6, 8), #interaction depth # eta = c(0.01, 0.005), #learning rate # gamma = 0.001, #mindev # colsample_bytree = c(1, 0.5), #proportion random features per tree # min_child_weight = 1, #also controls tree depth # subsample = 1) #bootstrap proportion # # ctrl &lt;- trainControl(method = &#39;cv&#39;, number = 5, verboseIter = T) # # calif_xgb_gridsearch &lt;- train(HousePrice ~ ., data = calif_fe_train, # method = &#39;xgbTree&#39;, # trControl = ctrl, # verbose = F, # tuneGrid = calif_xgb_grid) # # save(calif_xgb_gridsearch, file = &#39;data/xgb_calif_40k.Rdata&#39;) load(&#39;data/xgb_calif_40k.Rdata&#39;) Looking at the hyperparameter tuning CV results in Figure 6.39, we once again see that the deeper tree (8 leaves) is preferable across all combinations. We would ideally want to find the best tree size, since this seems to make a notable difference, especially when applying random feature selection. For this number of trees, the learning rate change does not make a substantial difference. plot(calif_xgb_gridsearch) Figure 6.39: Hyperparameter combinations results for an XGBoost model fitted to the feature-engineered California housing dataset. The best model combination (of the ones tested) is as follows: kable(calif_xgb_gridsearch$bestTune) nrounds max_depth eta gamma colsample_bytree min_child_weight subsample 21 25000 8 0.005 0.001 0.5 1 1 Finally, we can use the selected model to predict on the test set. y_calif_fe &lt;- calif_fe_test$HousePrice calif_xgb_pred &lt;- predict(calif_xgb_gridsearch, calif_fe_test) calif_xgb_mse &lt;- mean((y_calif_fe - calif_xgb_pred)^2) The CV RMSE of 0.211 was relatively close to the testing RMSE of 0.207 (MSE = 0.043), in fact slightly overestimating it. We can also determine the proportion of variation explained in the unseen data as \\(R^2\\) = 86.78%. One way of visualising a models predictions is to simply plot them against the observed values. The closer the points lie to a straight line, the better the fit. plot_data &lt;- as.data.frame(cbind(predicted = calif_xgb_pred, observed = calif_fe_test$HousePrice)) ggplot(plot_data, aes(predicted, observed)) + geom_point(color = &#39;darkred&#39;, alpha = 0.5) + geom_smooth(method = lm) + xlab(&#39;Predicted log(price)&#39;) + ylab(&#39;Observed log(price)&#39;) + theme(plot.title = element_text(color = &#39;darkgreen&#39;, size = 16, hjust = 0.5), axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12, hjust = 0.5), axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14)) + theme_bw() The first thing to notice is the clear ceiling on the observed house prices, which one should once again ideally have already picked up in the data exploration. This maximum value of 500001 is suspiciously round. It turns out that there are 992 neighbourhoods where the median house price was either $500,000 or $500,001. Whether this is an artifact of the data due to some measurement limit or estimation, or whether it was because of some real-world bound is something the analyst should determine before providing definitive conclusions. For our very last example, we will return to the Titanic data. 6.4.4 Example 7  Titanic (continued) Slow learning methods like gradient boosting generally tend to work better on larger datasets. Having said that, nothing prevents us from applying boosting to the Titanic data for comparison, even though it is relatively limited in size. The benefit of a smaller problem is of course that the training goes quicker and it generally requires fewer trees, such that one can test more hyperparameter combinations for the same time cost. Subsampling is also unlikely to yield much, if any benefit, hence we will just use the gbm method. We will search across trees from 500 to 6000 in increments of 500; \\(d = 1, \\ldots, 5\\); and \\(\\lambda \\in (0.001, 0.005, 0.01)\\). set.seed(1) train &lt;- sample(1:nrow(titanic), 0.7*nrow(titanic)) titanic_train &lt;- titanic[train,] titanic_test &lt;- titanic[-train,] # set.seed(4026) # titanic_gbm_grid &lt;- expand.grid(n.trees = seq(500, 6000, 500), # interaction.depth = 1:5, # shrinkage = c(0.01, 0.005, 0.001), # n.minobsinnode = 1) # # ctrl &lt;- trainControl(method = &#39;cv&#39;, number = 10, verboseIter = T) # # titanic_gbm_gridsearch &lt;- train(Survived ~ ., data = titanic_train, # method = &#39;gbm&#39;, # distribution = &#39;bernoulli&#39;, #For classification # trControl = ctrl, # verbose = F, # tuneGrid = titanic_gbm_grid) # save(titanic_gbm_gridsearch, file = &#39;data/gbm_titanic.Rdata&#39;) load(&#39;data/gbm_titanic.Rdata&#39;) The best model used the following hyperparameters: kable(titanic_gbm_gridsearch$bestTune) n.trees interaction.depth shrinkage n.minobsinnode 103 3500 4 0.005 1 This is a desirable result, since none of the hyperparameters are on the edge of the explored space. Using \\(d = 4\\) and \\(\\lambda = 0.005\\), we can now find the exact number of trees where the CV deviance is minimised. # gbm requires 0-1 for binary classification (without caret) titanic_train$Survived &lt;- as.numeric(titanic_train$Survived) - 1 set.seed(4026) titanic_gbm &lt;- gbm(Survived ~ ., data = titanic_train, distribution = &#39;bernoulli&#39;, #For binary classification (see ?gbm) n.trees = 4000, interaction.depth = 4, shrinkage = 0.005, bag.fraction = 1, cv.folds = 10, n.cores = 10, verbose = F) d &lt;- gbm.perf(titanic_gbm) legend(&#39;topright&#39;, c(&#39;CV error&#39;, &#39;Training error&#39;), col = c(&#39;green&#39;, &#39;black&#39;), lty = 1) Figure 6.40: Cross-validation results for the gradient boosted trees model with \\(\\lambda=0.005\\) and \\(d=4\\) fitted to the Titanic dataset. We will use this model, with only 946 trees, for prediction. y_titanic_test &lt;- as.numeric(titanic_test$Survived) - 1 titanic_gbm_pred &lt;- predict(titanic_gbm, newdata = titanic_test, type = &#39;response&#39;) titanic_gbm_mis &lt;- mean(y_titanic_test != round(titanic_gbm_pred)) Through gbm we have managed to reduce the testing misclassification rate even further, from 21.4% for the tuned random forest to 19.53%. Exploring the confusion matrix and other classification metrics is left as an exercise. Interestingly, the variable importance for the gbm model showed passenger class to be more influential than age and fare, contrary to the random forest model variable importance shown in Figure 6.29. Considering that the gbm model performed better on the same out-of-sample data, it can be interpreted as this model extracting more information from the passenger class variable than the random forest managed. Number of siblings/spouses also featured more prominently in this model. par(mar=c(5,6,4,1) + 0.1) titanic_gbm_varimp &lt;- summary(titanic_gbm, n.trees = d, las = 1) Figure 6.41: Variable importance for the gbm model fitted to the Titanic dataset. Moving on to the partial dependence plots, we observe the expected patterns based on the classification tree, where the log-odds of survival were significantly increased by being female; decreased as class increases; dipped sharply after late-teens, but picked up again for the elderly; and increased sharply beyond a specific fare. There are once again outliers in fare which can be removed, although this is left for the reader. p1 &lt;- plot.gbm(titanic_gbm, &#39;Sex&#39;, d, ylab = &#39;&#39;) p2 &lt;- plot.gbm(titanic_gbm, &#39;Pclass&#39;, d, ylab = &#39;&#39;) p3 &lt;- plot.gbm(titanic_gbm, &#39;Age&#39;, d, ylab = &#39;&#39;) p4 &lt;- plot.gbm(titanic_gbm, &#39;Fare&#39;, d, ylab = &#39;&#39;) grid.arrange(p1, p2, p3, p4, ncol = 2) grid.text(&#39;Partial Predicted Log-odds&#39;, x = 0, vjust = 1, rot = 90) Figure 6.42: Partial dependence plots for the gbm model fitted to the Titanic dataset. We might also be interested in the interaction between age and sex, shown in Figure 6.43. #Bivariate PD plot plot.gbm(titanic_gbm, c(&#39;Sex&#39;, &#39;Age&#39;), d) Figure 6.43: Bivariate partial dependence plot for the gbm model fitted to the Titanic dataset. Although the overall pattern across ages is roughly the same for males and females, we do a see a noticeably more pronounced dip specifically for males beyond age of approximately 16. These various approaches have allowed us to extract useful insights into the data (statistical learning paradigm) as well create some relatively accurate predictions on unseen data (machine learning paradigm). 6.4.5 Final word We end this section with one final reminder that the implementation of these supervised learning algorithms is constantly being developed. For example, we only used caret for hyperparameter tuning, although one could achieve the same with a number of other packages, e.g.Â the tried-and-trusted tidymodels, or the newly-developed EZtune. You are indeed encouraged to keep on exploring new methods in R, although one should always ensure that you have a deeply grounded understanding of the theory underpinning these methods. So continue balancing depth of theory, breadth of topic, and practical application on your future journey through supervised learning. Happy continued learning! 6.5 Homework exercises We observed a fairly high \\(R^2\\) value for the random forest fitted to the California housing dataset. How does this compare to the other regression models we encountered? Compare it with the lasso regression, some polynomial regression model, and KNN. Run the random forest grid search on the Titanic dataset multiple times. What is the distribution of hyperparameter selections? How does the splitting rule choice affect the predictive performance and variable importance measurements? Rerun the feature-engineered California housing analyses after removing all definite outliers. Does this affect any of the results? Fit and tune a random forest on the feature-engineered California training set. How do the test set results compare with that of the boosted trees? Not be confused with decision trees as used in the decision theory context. The cross-entropy between two distributions, \\(P\\) and \\(Q\\), is equal to the entropy of \\(P\\) plus the Kullback-Leibler (KL) divergence from \\(P\\) to \\(Q\\). The Titanic initially departed from Southampton (S), then picked up passengers at Cherbourg, France (C), before making a final stop at Queenstown, Ireland (Q), today known as Cobh. Run on the following cpu specifications: AuthenticAMD, AMD Ryzen 7 3700X 8-Core Processor, 16 cores. RAM = 17.2 GB. "],["references.html", "References", " References Abu-Mostafa, Y. S., Magdon-Ismail, M., &amp; Lin, H.-T. (2012). Learning from data. AMLBook. Aler, R., Valls, J. M., &amp; BostrÃ¶m, H. (2020). Study of hellinger distance as a splitting metric for random forests in balanced and imbalanced classification datasets. Expert Systems with Applications, 149, 113264. Bergstra, J., &amp; Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(2). Bishop, C. M. (2006). Pattern recognition and machine learning (information science and statistics). Springer-Verlag. Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785794. https://doi.org/10.1145/2939672.2939785 Chicco, D., &amp; Jurman, G. (2020). Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making, 20 (16). https://doi.org/10.1186/s12911-020-1023-5 Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The elements of statistical learning: Data mining, inference, and prediction (Vol. 2). Springer. James, G., Witten, D., Hastie, T., Tibshirani, R., et al. (2013). An introduction to statistical learning (Vol. 112). Springer. Murphy, K. P. (2013). Machine learning : A probabilistic perspective. MIT Press. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr_1_2?ie=UTF8&amp;qid=1336857747&amp;sr=8-2 Wright, M. N., &amp; Ziegler, A. (2017). Ranger: A fast implementation of random forests for high dimensional data in c++ and r. Journal of Statistical Software, 77(1), 117. https://doi.org/10.18637/jss.v077.i01 Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2), 301320. http://www.jstor.org/stable/3647580 "]]
