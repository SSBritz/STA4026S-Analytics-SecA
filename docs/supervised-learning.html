<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Supervised Learning | STA4026S – Honours Analytics Section A: Theory and Application of Supervised Learning</title>
  <meta name="description" content="STA4026S – Honours Analytics<br />
Section A: Theory and Application of Supervised Learning</p>" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Supervised Learning | STA4026S – Honours Analytics Section A: Theory and Application of Supervised Learning" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Supervised Learning | STA4026S – Honours Analytics Section A: Theory and Application of Supervised Learning" />
  
  
  

<meta name="author" content="Stefan S. Britz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="linear-model-selection-regularisation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="figs/UCTLogo.jpg"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>2.1</b> Bias-variance trade-off</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#example-1-simulation"><i class="fa fa-check"></i><b>2.1.1</b> Example 1 – Simulation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#model-validation"><i class="fa fa-check"></i><b>2.2</b> Model validation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#validation-set"><i class="fa fa-check"></i><b>2.2.1</b> Validation set</a></li>
<li class="chapter" data-level="2.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#k-fold-cv"><i class="fa fa-check"></i><b>2.2.2</b> <span class="math inline">\(k\)</span>-fold CV</a></li>
<li class="chapter" data-level="2.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#example-1-simulation-continued"><i class="fa fa-check"></i><b>2.2.3</b> Example 1 – Simulation (continued)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#side-note-statistical-learning-vs-machine-learning"><i class="fa fa-check"></i><b>2.3</b> Side note: Statistical learning vs machine learning</a></li>
<li class="chapter" data-level="2.4" data-path="supervised-learning.html"><a href="supervised-learning.html#homework-exercises"><i class="fa fa-check"></i><b>2.4</b> Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html"><i class="fa fa-check"></i><b>3</b> Linear Model Selection &amp; Regularisation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#linear-regression-models"><i class="fa fa-check"></i><b>3.1</b> Linear regression models</a></li>
<li class="chapter" data-level="3.2" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#l_1-and-l_2-regularisation"><i class="fa fa-check"></i><b>3.2</b> <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> regularisation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#ridge-regression-l_2"><i class="fa fa-check"></i><b>3.2.1</b> Ridge regression – <span class="math inline">\(L_2\)</span></a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#example-2-prostate-cancer"><i class="fa fa-check"></i><b>3.2.2</b> Example 2 – Prostate cancer</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#the-lasso-l_1"><i class="fa fa-check"></i><b>3.2.3</b> The Lasso – <span class="math inline">\(L_1\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#example-2-prostate-cancer-continued"><i class="fa fa-check"></i><b>3.2.4</b> Example 2 – Prostate cancer (continued)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#elastic-net"><i class="fa fa-check"></i><b>3.3</b> Elastic-net</a></li>
<li class="chapter" data-level="3.4" data-path="linear-model-selection-regularisation.html"><a href="linear-model-selection-regularisation.html#homework-exercises-1"><i class="fa fa-check"></i><b>3.4</b> Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-models.html"><a href="classification-models.html"><i class="fa fa-check"></i><b>4</b> Classification Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-models.html"><a href="classification-models.html#logistic-regression"><i class="fa fa-check"></i><b>4.1</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification-models.html"><a href="classification-models.html#estimation"><i class="fa fa-check"></i><b>4.1.1</b> Estimation</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-models.html"><a href="classification-models.html#interpretation"><i class="fa fa-check"></i><b>4.1.2</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.3" data-path="classification-models.html"><a href="classification-models.html#prediction"><i class="fa fa-check"></i><b>4.1.3</b> Prediction</a></li>
<li class="chapter" data-level="4.1.4" data-path="classification-models.html"><a href="classification-models.html#example-3-default-data"><i class="fa fa-check"></i><b>4.1.4</b> Example 3 – Default data</a></li>
<li class="chapter" data-level="4.1.5" data-path="classification-models.html"><a href="classification-models.html#decision-boundaries"><i class="fa fa-check"></i><b>4.1.5</b> Decision boundaries</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-models.html"><a href="classification-models.html#model-evaluation"><i class="fa fa-check"></i><b>4.2</b> Model evaluation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-models.html"><a href="classification-models.html#changing-the-threshold"><i class="fa fa-check"></i><b>4.2.1</b> Changing the threshold</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-models.html"><a href="classification-models.html#roc-curve"><i class="fa fa-check"></i><b>4.2.2</b> ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-models.html"><a href="classification-models.html#regularisation"><i class="fa fa-check"></i><b>4.3</b> Regularisation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classification-models.html"><a href="classification-models.html#example-4-heart-failure"><i class="fa fa-check"></i><b>4.3.1</b> Example 4 – Heart failure</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-models.html"><a href="classification-models.html#homework-exercises-2"><i class="fa fa-check"></i><b>4.4</b> Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>5</b> Non-Linear Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="non-linear-models.html"><a href="non-linear-models.html#polynomial-regression"><i class="fa fa-check"></i><b>5.1</b> Polynomial regression</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="non-linear-models.html"><a href="non-linear-models.html#regression"><i class="fa fa-check"></i><b>5.1.1</b> Regression</a></li>
<li class="chapter" data-level="5.1.2" data-path="non-linear-models.html"><a href="non-linear-models.html#example-5-auto"><i class="fa fa-check"></i><b>5.1.2</b> Example 5 – Auto</a></li>
<li class="chapter" data-level="5.1.3" data-path="non-linear-models.html"><a href="non-linear-models.html#classification"><i class="fa fa-check"></i><b>5.1.3</b> Classification</a></li>
<li class="chapter" data-level="5.1.4" data-path="non-linear-models.html"><a href="non-linear-models.html#example-4-heart-failure-continued"><i class="fa fa-check"></i><b>5.1.4</b> Example 4 – Heart failure (continued)</a></li>
<li class="chapter" data-level="5.1.5" data-path="non-linear-models.html"><a href="non-linear-models.html#extension-to-basis-functions-and-generalised-additive-models"><i class="fa fa-check"></i><b>5.1.5</b> Extension to basis functions and generalised additive models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="non-linear-models.html"><a href="non-linear-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.2</b> K-Nearest Neighbours</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="non-linear-models.html"><a href="non-linear-models.html#regression-1"><i class="fa fa-check"></i><b>5.2.1</b> Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="non-linear-models.html"><a href="non-linear-models.html#example-2-prostate-cancer-continued-1"><i class="fa fa-check"></i><b>5.2.2</b> Example 2 – Prostate cancer (continued)</a></li>
<li class="chapter" data-level="5.2.3" data-path="non-linear-models.html"><a href="non-linear-models.html#classification-1"><i class="fa fa-check"></i><b>5.2.3</b> Classification</a></li>
<li class="chapter" data-level="5.2.4" data-path="non-linear-models.html"><a href="non-linear-models.html#example-4-heart-failure-continued-1"><i class="fa fa-check"></i><b>5.2.4</b> Example 4 – Heart failure (continued)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="non-linear-models.html"><a href="non-linear-models.html#homework-exercises-3"><i class="fa fa-check"></i><b>5.3</b> Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>6</b> Tree-based Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-trees"><i class="fa fa-check"></i><b>6.1</b> Regression trees</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-6-california-housing"><i class="fa fa-check"></i><b>6.1.1</b> Example 6 – California housing</a></li>
<li class="chapter" data-level="6.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#cost-complexity-pruning"><i class="fa fa-check"></i><b>6.1.2</b> Cost complexity pruning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#classification-trees"><i class="fa fa-check"></i><b>6.2</b> Classification trees</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#splitting-criteria"><i class="fa fa-check"></i><b>6.2.1</b> Splitting criteria</a></li>
<li class="chapter" data-level="6.2.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#link-between-deviance-and-rss"><i class="fa fa-check"></i><b>6.2.2</b> Link between deviance and RSS</a></li>
<li class="chapter" data-level="6.2.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#cost-complexity-pruning-1"><i class="fa fa-check"></i><b>6.2.3</b> Cost complexity pruning</a></li>
<li class="chapter" data-level="6.2.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-7-titanic"><i class="fa fa-check"></i><b>6.2.4</b> Example 7 – Titanic</a></li>
<li class="chapter" data-level="6.2.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-8-iris"><i class="fa fa-check"></i><b>6.2.5</b> Example 8 – Iris</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-and-random-forests"><i class="fa fa-check"></i><b>6.3</b> Bagging and random forests</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>6.3.1</b> Bagging</a></li>
<li class="chapter" data-level="6.3.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#out-of-bag-error-estimation"><i class="fa fa-check"></i><b>6.3.2</b> Out-of-bag error estimation</a></li>
<li class="chapter" data-level="6.3.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#variable-importance"><i class="fa fa-check"></i><b>6.3.3</b> Variable importance</a></li>
<li class="chapter" data-level="6.3.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>6.3.4</b> Random forests</a></li>
<li class="chapter" data-level="6.3.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-6-california-housing-continued"><i class="fa fa-check"></i><b>6.3.5</b> Example 6 – California housing (continued)</a></li>
<li class="chapter" data-level="6.3.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-8-titanic-continued"><i class="fa fa-check"></i><b>6.3.6</b> Example 8 – Titanic (continued)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#gradient-boosting"><i class="fa fa-check"></i><b>6.4</b> Gradient boosting</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-6-california-housing-continued-1"><i class="fa fa-check"></i><b>6.4.1</b> Example 6 – California housing (continued)</a></li>
<li class="chapter" data-level="6.4.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#partial-dependence-plots"><i class="fa fa-check"></i><b>6.4.2</b> Partial dependence plots</a></li>
<li class="chapter" data-level="6.4.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#xgboost"><i class="fa fa-check"></i><b>6.4.3</b> XGBoost</a></li>
<li class="chapter" data-level="6.4.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#example-7-titanic-continued"><i class="fa fa-check"></i><b>6.4.4</b> Example 7 – Titanic (continued)</a></li>
<li class="chapter" data-level="6.4.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#final-word"><i class="fa fa-check"></i><b>6.4.5</b> Final word</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#homework-exercises-4"><i class="fa fa-check"></i><b>6.5</b> Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p>STA4026S – Honours Analytics<br />
Section A: Theory and Application of Supervised Learning</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Supervised Learning<a href="supervised-learning.html#supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter aims to briefly summarise the key aspects of supervised learning that will be relevant for the following sections, some of which you should be familiar with already. Whereas there are many good sources that provide a more comprehensive discussion, Chapter 2 of <span class="citation">James et al. (2013)</span> is sufficient for the level and scope of this course.</p>
<p>Using a set of observations to uncover some underlying process in the real world is the basic premise of “learning from data” <span class="citation">(Abu-Mostafa et al., 2012, p. 11)</span>. By discerning patterns, relationships, and trends within the data, machines become capable of making informed decisions and predictions in various domains. Different learning paradigms have developed to address different problems and data structures, and are generally divided into three broad classes: supervised learning, unsupervised learning, and reinforcement learning. We will not discuss the latter in this course.</p>
<p>The basic distinction is that <strong>supervised learning</strong> refers to cases where there is some <strong>target variable</strong>, usually indicated as <span class="math inline">\(Y\)</span>, whereas if we are interested in the structures and patterns in explanatory variables<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> only, we refer to unsupervised learning.</p>
<p>Given a quantitative response <span class="math inline">\(Y\)</span> and a set of <span class="math inline">\(p\)</span> predictors <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>, we are interested in the assumed, unobserved function that maps the inputs to the outputs:</p>
<p><span class="math display">\[Y = \underbrace{f(X)}_{systematic} + \underbrace{\epsilon}_{random},\]</span></p>
<p>where <span class="math inline">\(f(.)\)</span> represents the fixed, but unknown function and <span class="math inline">\(\epsilon\)</span> is a random error term, independent of <span class="math inline">\(X\)</span>, with <span class="math inline">\(E(\epsilon) = 0\)</span>. By estimating <span class="math inline">\(f\)</span> such that</p>
<p><span class="math display">\[\hat{Y} = \hat{f}(X),\]</span></p>
<p>we allow for both prediction of <span class="math inline">\(Y\)</span> – which is the primary goal in forecasting – and inference, i.e. describing how <span class="math inline">\(Y\)</span> is affected by changes in <span class="math inline">\(X\)</span>.</p>
<p>Hypothesising <span class="math inline">\(\hat{f}\)</span> can be done in two ways, namely via a parametric or a non-parametric approach.</p>
<p><strong>Parametric approach</strong></p>
<p>Here an assumption is made about the functional form of <span class="math inline">\(f\)</span>, for example the linear model</p>
<p><span class="math display">\[f(\boldsymbol{X}) = \beta_0 + \sum_{j=1}^p\beta_jX_j.\]</span></p>
<p>The best estimate of <span class="math inline">\(f\)</span> is now defined as the set of parameters <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimise some specified loss function. Given a set of observations <span class="math inline">\(\mathcal{D}=\{\boldsymbol{x}_i, y_i\}_{i=1}^n\)</span> – henceforth referred to as the <strong>training set</strong> – we could use ordinary least squares to minimise the mean squared error (MSE):</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^n\left[y_i - \hat{f}(x_i)\right]^2 \]</span> Therefore, the problem of estimating an arbitrary p-dimensional function is simplified to fitting a set of parameters.</p>
<p><strong>Non-parametric approach</strong></p>
<p>Another option is to make no explicit assumptions regarding the functional form of <span class="math inline">\(f\)</span>. This allows one to fit a wide range of possible forms for <span class="math inline">\(f\)</span> – in these notes we consider K-nearest neighbours and tree-based methods – but since estimation is not reduced to estimating a set of parameters, this approach generally requires more data than parametric estimation.</p>
<p>The objective remains to find <span class="math inline">\(f\)</span> that fits the available data as closely as possible, whilst avoiding overfitting to ensure that the model generalises well to unseen data.</p>
<p><strong>Generalisation</strong></p>
<p>The primary goal of prediction is of course to accurately predict the outcomes of observations not yet observed by the model, referred to as out-of-sample observations.</p>
<p>Consider the case where the estimated function <span class="math inline">\(\hat{f}\)</span> is fixed and out-of-sample observations of the variables are introduced, which we will denote as <span class="math inline">\(\{\boldsymbol{x}_0, y_0\}\)</span>. The expected MSE for these <strong>test set</strong> observations (see Section <a href="supervised-learning.html#model-validation">2.2</a>) can be deconstructed as follows:</p>
<span class="math display" id="eq:test-mse-decomp">\[\begin{align}
E\left[y_0 - \hat{f}(\boldsymbol{x}_0) \right]^2 &amp;= E\left[f(\boldsymbol{x}_0) + \epsilon - \hat{f}(\boldsymbol{x}_0)\right]^2 \\
&amp;= E\left[\left(f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right)^2 + 2\epsilon \left(f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0) \right) + \epsilon^2\right] \\
&amp;= E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right]^2 + 2E[\epsilon]E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right] + E\left[\epsilon^2\right] \\
&amp;= \underbrace{E\left[f(\boldsymbol{x}_0) - \hat{f}(\boldsymbol{x}_0)\right]^2}_{reducible} + \underbrace{Var(\epsilon)}_{irreducible} \tag{2.1}
\end{align}\]</span>
<p>The primary goal of machine learning is to find an <span class="math inline">\(\hat{f}\)</span> that best approximates the underlying, unknown relationship between the input and output variables by minimising the reducible error. Note that because of the irreducible component (the “noise” in the data), there will always be some lower bound for the theoretical MSE, and that <strong>this bound is almost always unknown in practice</strong> <span class="citation">(James et al., 2013, p. 19)</span>.</p>
<p>In order to achieve accurate out-of-sample prediction, we need to specify a <strong>model</strong> (another term for the estimated function <span class="math inline">\(\hat{f}\)</span>) that is sufficiently – but not overly – complex. Finding this balance of complexity is referred to as the <strong>bias-variance trade-off</strong>. The reducible error component can be decomposed further to help illustrate this trade-off.</p>
<div id="bias-variance-trade-off" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Bias-variance trade-off<a href="supervised-learning.html#bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider again a fixed <span class="math inline">\(\hat{f}\)</span> and out-of-sample observations <span class="math inline">\(\{\boldsymbol{x}_0, y_0\}\)</span>. For ease of notation, let <span class="math inline">\(f = f(\boldsymbol{x}_0)\)</span> and <span class="math inline">\(\hat{f} = \hat{f}(\boldsymbol{x}_0)\)</span>. Also note that <span class="math inline">\(f\)</span> is deterministic such that <span class="math inline">\(E\left[f\right] = f\)</span>.</p>
<p>Starting with the reducible error in Equation <a href="supervised-learning.html#eq:test-mse-decomp">(2.1)</a>, we have</p>
<span class="math display" id="eq:bias-var">\[\begin{align}
E\left[f - \hat{f} \right]^2 &amp;= E\left[\hat{f} - f \right]^2 \\
&amp;= E\left[\hat{f} - E(\hat{f}) + E(\hat{f}) - f \right]^2 \\
&amp;= E\left\{ \left[\hat{f} - E(\hat{f})\right]^2 + 2\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right] + \left[E(\hat{f}) - f\right]^2 \right\} \\
&amp;= E\left[\hat{f} - E(\hat{f})\right]^2 + 2E\left\{\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right]\right\} + E\left[E(\hat{f}) - f\right]^2 \\
&amp;= Var\left[\hat{f}\right] + 0 + \left[E(\hat{f}) - f\right]^2 \\
&amp;= Var\left[\hat{f}\right] + Bias^2\left[\hat{f}\right] \tag{2.2}
\end{align}\]</span>
<p>Showing that the crossproduct term equals zero:</p>
<span class="math display">\[\begin{align}
E\left\{\left[\hat{f} - E(\hat{f})\right] \left[E(\hat{f}) - f\right]\right\} &amp;= E\left[\hat{f}E(\hat{f}) - E(\hat{f})E(\hat{f}) - \hat{f}f + E(\hat{f})f\right] \\
&amp;= E(\hat{f})E(\hat{f}) - E(\hat{f})E(\hat{f}) - E(\hat{f})f + E(\hat{f})f \\
&amp;= 0
\end{align}\]</span>
<p>Therefore, in order to minimise the expected test MSE we need to find a model that has the lowest combined variance and (squared) bias.</p>
<p>The <strong>variance</strong> represents the extent to which <span class="math inline">\(\hat{f}\)</span> changes between different randomly selected training samples taken from the same population. The <strong>bias</strong> of <span class="math inline">\(\hat{f}\)</span> is simply the error that is introduced by approximating the real-world relationship with a simpler representation. Note that since <span class="math inline">\(f\)</span> is generally unknown, the bias component cannot be directly observed or measured outside of simulations. However, these simulations may help us illustrate how the bias and variance change as model complexity increases.</p>
<p>Although the concepts of model complexity and flexibility are not necessarily perfectly defined – depending on the class of model being hypothesised – the following example should provide an intuitive understanding.</p>
<div id="example-1-simulation" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Example 1 – Simulation<a href="supervised-learning.html#example-1-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To allow for easy visualisation, let us consider a simple function with only one feature:</p>
<p><span class="math display">\[Y = X + 2\cos(5X) + \epsilon,\]</span> where <span class="math inline">\(\epsilon \sim N(0, 2)\)</span>.</p>
<p>Below we simulate <span class="math inline">\(n = 100\)</span> observations from <span class="math inline">\(X \sim U(-2,2)\)</span>, to which we fit cubic smoothing splines of increasing complexity. The details of splines are beyond the scope of this course, but they provide an easy-to-see illustration of “flexibility”.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="supervised-learning.html#cb1-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb1-2"><a href="supervised-learning.html#cb1-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4026</span>)</span>
<span id="cb1-3"><a href="supervised-learning.html#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="supervised-learning.html#cb1-4" tabindex="-1"></a><span class="co">#Simulated data</span></span>
<span id="cb1-5"><a href="supervised-learning.html#cb1-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-6"><a href="supervised-learning.html#cb1-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-7"><a href="supervised-learning.html#cb1-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">5</span><span class="sc">*</span>x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">2</span>))</span>
<span id="cb1-8"><a href="supervised-learning.html#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="supervised-learning.html#cb1-9" tabindex="-1"></a><span class="co">#The true function</span></span>
<span id="cb1-10"><a href="supervised-learning.html#cb1-10" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb1-11"><a href="supervised-learning.html#cb1-11" tabindex="-1"></a>f <span class="ot">&lt;-</span> xx <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">5</span><span class="sc">*</span>xx)</span>
<span id="cb1-12"><a href="supervised-learning.html#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="supervised-learning.html#cb1-13" tabindex="-1"></a><span class="co">#Fit cubic splines with increasing degrees of freedom</span></span>
<span id="cb1-14"><a href="supervised-learning.html#cb1-14" tabindex="-1"></a><span class="cf">for</span>(dof <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">50</span>){</span>
<span id="cb1-15"><a href="supervised-learning.html#cb1-15" tabindex="-1"></a>  fhat <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x, y, <span class="at">df =</span> dof)</span>
<span id="cb1-16"><a href="supervised-learning.html#cb1-16" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb1-17"><a href="supervised-learning.html#cb1-17" tabindex="-1"></a>  <span class="fu">lines</span>(xx, f, <span class="st">&#39;l&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-18"><a href="supervised-learning.html#cb1-18" tabindex="-1"></a>  <span class="fu">lines</span>(fhat, <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-19"><a href="supervised-learning.html#cb1-19" tabindex="-1"></a>  <span class="fu">title</span>(<span class="at">main =</span> <span class="fu">paste</span>(<span class="st">&#39;Degrees of freedom:&#39;</span>, dof))</span>
<span id="cb1-20"><a href="supervised-learning.html#cb1-20" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&#39;bottomright&#39;</span>, <span class="fu">c</span>(<span class="st">&#39;f(x) - True&#39;</span>, <span class="fu">expression</span>(<span class="fu">hat</span>(f)(x) <span class="sc">~</span> <span class="st">&#39;- Cubic spline&#39;</span>)), </span>
<span id="cb1-21"><a href="supervised-learning.html#cb1-21" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;black&#39;</span>, <span class="st">&#39;blue&#39;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-22"><a href="supervised-learning.html#cb1-22" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:splines"></span>
<img src="figs/splines-.gif" alt="Cubic splines with varying degrees of freedom fitted to a sample of 100 datapoints drawn from $Y = X + 2\cos(5X) + \epsilon$, with $\epsilon \sim N(0, 2)$." width="672" />
<p class="caption">
Figure 2.1: Cubic splines with varying degrees of freedom fitted to a sample of 100 datapoints drawn from <span class="math inline">\(Y = X + 2\cos(5X) + \epsilon\)</span>, with <span class="math inline">\(\epsilon \sim N(0, 2)\)</span>.
</p>
</div>
<p>This serves to illustrate how the model’s degrees of freedom are directly proportional to the model’s complexity. However, to extricate the bias and variance components, we need to observe these models’ fit on out-of-sample data across many random realisations of training samples.</p>
<p>In the following simulation we again observe <span class="math inline">\(n=100\)</span> training observations at a time, to which the same models of varying complexity as above are fitted. Each model’s fit is then assessed on a set of 100 testing observations, where the <span class="math inline">\(\boldsymbol{x}_0\)</span> (and, therefore, true <span class="math inline">\(f(\boldsymbol{x}_0)\)</span>) are fixed, but random noise is added. This process is repeated 1000 times, such that we can keep track of how each test observation’s predictions vary across the iterations, as well as the errors.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="supervised-learning.html#cb2-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb2-2"><a href="supervised-learning.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="supervised-learning.html#cb2-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>          <span class="co">#Sample size</span></span>
<span id="cb2-4"><a href="supervised-learning.html#cb2-4" tabindex="-1"></a>num_sims <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co">#Number of iterations (could be parallelised)</span></span>
<span id="cb2-5"><a href="supervised-learning.html#cb2-5" tabindex="-1"></a>dofs <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">25</span>      <span class="co">#Model complexities</span></span>
<span id="cb2-6"><a href="supervised-learning.html#cb2-6" tabindex="-1"></a>var_eps <span class="ot">&lt;-</span> <span class="dv">2</span>      <span class="co">#Var(epsilon): The irreducible error</span></span>
<span id="cb2-7"><a href="supervised-learning.html#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="supervised-learning.html#cb2-8" tabindex="-1"></a>pred_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> num_sims, <span class="at">ncol =</span> n) <span class="co">#To store each set of predictions</span></span>
<span id="cb2-9"><a href="supervised-learning.html#cb2-9" tabindex="-1"></a>mses <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length =</span> num_sims)             <span class="co">#Also want to track the testing MSEs</span></span>
<span id="cb2-10"><a href="supervised-learning.html#cb2-10" tabindex="-1"></a>red_err <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length =</span> num_sims)          <span class="co">#As well as the reducible error</span></span>
<span id="cb2-11"><a href="supervised-learning.html#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="supervised-learning.html#cb2-12" tabindex="-1"></a><span class="co">#Herein we will capture the deconstructed components for each model</span></span>
<span id="cb2-13"><a href="supervised-learning.html#cb2-13" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Var =</span> <span class="cn">NA</span>, <span class="at">Bias2 =</span> <span class="cn">NA</span>, <span class="at">Red_err =</span> <span class="cn">NA</span>, <span class="at">MSE =</span> <span class="cn">NA</span>)</span>
<span id="cb2-14"><a href="supervised-learning.html#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="supervised-learning.html#cb2-15" tabindex="-1"></a><span class="co">#Testing data</span></span>
<span id="cb2-16"><a href="supervised-learning.html#cb2-16" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-17"><a href="supervised-learning.html#cb2-17" tabindex="-1"></a>f_test <span class="ot">&lt;-</span> x_test <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">5</span><span class="sc">*</span>x_test) <span class="co">#This is the part we don&#39;t know outside sims!!</span></span>
<span id="cb2-18"><a href="supervised-learning.html#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="supervised-learning.html#cb2-19" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="dv">0</span> <span class="co">#To keep track of dof iterations, even when changing the range</span></span>
<span id="cb2-20"><a href="supervised-learning.html#cb2-20" tabindex="-1"></a></span>
<span id="cb2-21"><a href="supervised-learning.html#cb2-21" tabindex="-1"></a><span class="cf">for</span>(dof <span class="cf">in</span> dofs) { <span class="co">#Repeat over all model complexities</span></span>
<span id="cb2-22"><a href="supervised-learning.html#cb2-22" tabindex="-1"></a>  d <span class="ot">&lt;-</span> d<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb2-23"><a href="supervised-learning.html#cb2-23" tabindex="-1"></a>  <span class="cf">for</span>(iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_sims){ </span>
<span id="cb2-24"><a href="supervised-learning.html#cb2-24" tabindex="-1"></a>    </span>
<span id="cb2-25"><a href="supervised-learning.html#cb2-25" tabindex="-1"></a>    <span class="co">#Training data</span></span>
<span id="cb2-26"><a href="supervised-learning.html#cb2-26" tabindex="-1"></a>    x_train <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-27"><a href="supervised-learning.html#cb2-27" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> x_train <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">5</span><span class="sc">*</span>x_train) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fu">sqrt</span>(var_eps))</span>
<span id="cb2-28"><a href="supervised-learning.html#cb2-28" tabindex="-1"></a>    </span>
<span id="cb2-29"><a href="supervised-learning.html#cb2-29" tabindex="-1"></a>    <span class="co">#Add the noise</span></span>
<span id="cb2-30"><a href="supervised-learning.html#cb2-30" tabindex="-1"></a>    y_test <span class="ot">&lt;-</span> f_test <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fu">sqrt</span>(var_eps))</span>
<span id="cb2-31"><a href="supervised-learning.html#cb2-31" tabindex="-1"></a>    </span>
<span id="cb2-32"><a href="supervised-learning.html#cb2-32" tabindex="-1"></a>    <span class="co">#Fit cubic spline</span></span>
<span id="cb2-33"><a href="supervised-learning.html#cb2-33" tabindex="-1"></a>    spline_mod <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x_train, y_train, <span class="at">df =</span> dof)</span>
<span id="cb2-34"><a href="supervised-learning.html#cb2-34" tabindex="-1"></a>    </span>
<span id="cb2-35"><a href="supervised-learning.html#cb2-35" tabindex="-1"></a>    <span class="co">#Predict on OOS data</span></span>
<span id="cb2-36"><a href="supervised-learning.html#cb2-36" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(spline_mod, x_test)<span class="sc">$</span>y</span>
<span id="cb2-37"><a href="supervised-learning.html#cb2-37" tabindex="-1"></a>    </span>
<span id="cb2-38"><a href="supervised-learning.html#cb2-38" tabindex="-1"></a>    <span class="co">#And store</span></span>
<span id="cb2-39"><a href="supervised-learning.html#cb2-39" tabindex="-1"></a>    pred_mat[iter, ] <span class="ot">&lt;-</span> yhat</span>
<span id="cb2-40"><a href="supervised-learning.html#cb2-40" tabindex="-1"></a>    red_err[iter] <span class="ot">&lt;-</span> <span class="fu">mean</span>((f_test <span class="sc">-</span> yhat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-41"><a href="supervised-learning.html#cb2-41" tabindex="-1"></a>    mses[iter] <span class="ot">&lt;-</span> <span class="fu">mean</span>((y_test <span class="sc">-</span> yhat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-42"><a href="supervised-learning.html#cb2-42" tabindex="-1"></a>  }</span>
<span id="cb2-43"><a href="supervised-learning.html#cb2-43" tabindex="-1"></a>  </span>
<span id="cb2-44"><a href="supervised-learning.html#cb2-44" tabindex="-1"></a>  <span class="co">#Average each component over all iterations</span></span>
<span id="cb2-45"><a href="supervised-learning.html#cb2-45" tabindex="-1"></a>  var_fhat <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(pred_mat, <span class="dv">2</span>, var))           <span class="co">#E[\hat{f} - E(\hat{f})]^2</span></span>
<span id="cb2-46"><a href="supervised-learning.html#cb2-46" tabindex="-1"></a>  bias2_fhat <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">colMeans</span>(pred_mat) <span class="sc">-</span> f_test)<span class="sc">^</span><span class="dv">2</span>) <span class="co">#E[E(\hat{f}) - f]^2</span></span>
<span id="cb2-47"><a href="supervised-learning.html#cb2-47" tabindex="-1"></a>  reducible <span class="ot">&lt;-</span> <span class="fu">mean</span>(red_err)                          <span class="co">#E[f - \hat{f}]^2</span></span>
<span id="cb2-48"><a href="supervised-learning.html#cb2-48" tabindex="-1"></a>  MSE <span class="ot">&lt;-</span> <span class="fu">mean</span>(mses)                                   <span class="co">#E[y_0 - \hat{f}]^2</span></span>
<span id="cb2-49"><a href="supervised-learning.html#cb2-49" tabindex="-1"></a>  </span>
<span id="cb2-50"><a href="supervised-learning.html#cb2-50" tabindex="-1"></a>  results[d, ] <span class="ot">&lt;-</span> <span class="fu">c</span>(var_fhat, bias2_fhat, reducible, MSE)</span>
<span id="cb2-51"><a href="supervised-learning.html#cb2-51" tabindex="-1"></a>}</span>
<span id="cb2-52"><a href="supervised-learning.html#cb2-52" tabindex="-1"></a></span>
<span id="cb2-53"><a href="supervised-learning.html#cb2-53" tabindex="-1"></a><span class="co">#Plot the results</span></span>
<span id="cb2-54"><a href="supervised-learning.html#cb2-54" tabindex="-1"></a><span class="fu">plot</span>(dofs, results<span class="sc">$</span>MSE, <span class="st">&#39;l&#39;</span>, <span class="at">col =</span> <span class="st">&#39;darkred&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb2-55"><a href="supervised-learning.html#cb2-55" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&#39;Model complexity&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(results)))</span>
<span id="cb2-56"><a href="supervised-learning.html#cb2-56" tabindex="-1"></a><span class="fu">lines</span>(dofs, results<span class="sc">$</span>Bias2, <span class="st">&#39;l&#39;</span>, <span class="at">col =</span> <span class="st">&#39;lightblue&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-57"><a href="supervised-learning.html#cb2-57" tabindex="-1"></a><span class="fu">lines</span>(dofs, results<span class="sc">$</span>Var, <span class="st">&#39;l&#39;</span>, <span class="at">col =</span> <span class="st">&#39;orange&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-58"><a href="supervised-learning.html#cb2-58" tabindex="-1"></a><span class="fu">lines</span>(dofs, results<span class="sc">$</span>Red_err, <span class="st">&#39;l&#39;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-59"><a href="supervised-learning.html#cb2-59" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&#39;topright&#39;</span>, </span>
<span id="cb2-60"><a href="supervised-learning.html#cb2-60" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">&#39;MSE&#39;</span>, <span class="fu">expression</span>(Bias<span class="sc">^</span><span class="dv">2</span> <span class="sc">~</span> (<span class="fu">hat</span>(f))), <span class="fu">expression</span>(<span class="fu">Var</span>(<span class="fu">hat</span>(f))), <span class="st">&#39;Reducible Error&#39;</span>), </span>
<span id="cb2-61"><a href="supervised-learning.html#cb2-61" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;darkred&#39;</span>, <span class="st">&#39;lightblue&#39;</span>, <span class="st">&#39;orange&#39;</span>, <span class="st">&#39;black&#39;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">3</span>), <span class="dv">2</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-62"><a href="supervised-learning.html#cb2-62" tabindex="-1"></a></span>
<span id="cb2-63"><a href="supervised-learning.html#cb2-63" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> dofs[<span class="fu">which.min</span>(results<span class="sc">$</span>MSE)], <span class="at">lty =</span> <span class="dv">3</span>) <span class="co">#Complexity minimising MSE</span></span>
<span id="cb2-64"><a href="supervised-learning.html#cb2-64" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> var_eps, <span class="at">lty =</span> <span class="dv">3</span>)                      <span class="co">#MSE lower bound</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:b-v-trade"></span>
<img src="figs/b-v-trade-1.png" alt="Averaged error components over 1000 simulations of samples of $n=100$. The horizontal dashed line represent the minimum lower bound for the test MSE. The vertical dashed line indicates the point at which both the test MSE and reducible error are minimised." width="576" />
<p class="caption">
Figure 2.2: Averaged error components over 1000 simulations of samples of <span class="math inline">\(n=100\)</span>. The horizontal dashed line represent the minimum lower bound for the test MSE. The vertical dashed line indicates the point at which both the test MSE and reducible error are minimised.
</p>
</div>
<p>As a quick sanity check before interpreting this result, let us add up the components – which were calculated separately – and see whether we indeed observe that <span class="math inline">\(E\left[f - \hat{f} \right]^2 = Var\left[\hat{f}\right] + Bias^2\left[\hat{f}\right]\)</span> as per Equation <a href="supervised-learning.html#eq:test-mse-decomp">(2.1)</a>, and <span class="math inline">\(\text{Test MSE} = E\left[y_0 - \hat{f}\right]^2 = E\left[f - \hat{f} \right]^2 + Var(\epsilon)\)</span> as per Equation <a href="supervised-learning.html#eq:bias-var">(2.2)</a>. Note that we will need to have a small tolerance for discrepancy, since we have approximated the expected values by averaging over only 1000 realisations. This approximation will become more accurate as the number of iterations is increased.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="supervised-learning.html#cb3-1" tabindex="-1"></a><span class="co">#Is reducible error = var(fhat) + bias^2(fhat)?</span></span>
<span id="cb3-2"><a href="supervised-learning.html#cb3-2" tabindex="-1"></a><span class="fu">ifelse</span>(<span class="fu">isTRUE</span>(<span class="fu">all.equal</span>(results<span class="sc">$</span>Red_err, </span>
<span id="cb3-3"><a href="supervised-learning.html#cb3-3" tabindex="-1"></a>                        results<span class="sc">$</span>Var <span class="sc">+</span> results<span class="sc">$</span>Bias2, </span>
<span id="cb3-4"><a href="supervised-learning.html#cb3-4" tabindex="-1"></a>                        <span class="at">tolerance =</span> <span class="fl">0.001</span>)),</span>
<span id="cb3-5"><a href="supervised-learning.html#cb3-5" tabindex="-1"></a>       <span class="st">&#39;Happy days! :D&#39;</span>, </span>
<span id="cb3-6"><a href="supervised-learning.html#cb3-6" tabindex="-1"></a>       <span class="st">&#39;Haibo...&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Happy days! :D&quot;</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="supervised-learning.html#cb5-1" tabindex="-1"></a><span class="co">#Is Test MSE = var(fhat) + bias^2(fhat) + var(eps)?</span></span>
<span id="cb5-2"><a href="supervised-learning.html#cb5-2" tabindex="-1"></a><span class="fu">ifelse</span>(<span class="fu">isTRUE</span>(<span class="fu">all.equal</span>(results<span class="sc">$</span>MSE, </span>
<span id="cb5-3"><a href="supervised-learning.html#cb5-3" tabindex="-1"></a>                        results<span class="sc">$</span>Var <span class="sc">+</span> results<span class="sc">$</span>Bias2 <span class="sc">+</span> var_eps, </span>
<span id="cb5-4"><a href="supervised-learning.html#cb5-4" tabindex="-1"></a>                        <span class="at">tolerance =</span> <span class="fl">0.01</span>)),</span>
<span id="cb5-5"><a href="supervised-learning.html#cb5-5" tabindex="-1"></a>       <span class="st">&#39;Happy days! :D&#39;</span>, </span>
<span id="cb5-6"><a href="supervised-learning.html#cb5-6" tabindex="-1"></a>       <span class="st">&#39;Haibo...&#39;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Happy days! :D&quot;</code></pre>
<p>Figure <a href="supervised-learning.html#fig:b-v-trade">2.2</a> illustrates the general error pattern when learning from data: As model complexity/flexibility increases, the variance across multiple training samples increases, whilst the (squared) bias decreases as the estimated function gets closer to the true pattern on average. Note that <span class="math inline">\(E(\epsilon^2) = Var(\epsilon)\)</span> remains constant. This decrease in bias<span class="math inline">\(^2\)</span> initially offsets the increase in variance such that the test MSE initially decreases. However, from some complexity/flexibility of <span class="math inline">\(\hat{f}\)</span>, the decrease in bias<span class="math inline">\(^2\left(\hat{f}\right)\)</span> is offset by the increase in <span class="math inline">\(Var\left(\hat{f}\right)\)</span>, at which point the model starts to <strong>overfit</strong> and the test MSE starts increasing. This is the <strong>bias-variance trade-off</strong>. In this particular example, we see that of all the cubic splines, one with 13 degrees of freedom most closely captures the underlying pattern in the data, as measured by the test MSE.</p>
<p>The fundamental challenge in statistical learning is to postulate a model of the data that yields both a low bias and variance, whilst policing the model complexity such that the sum of these error components are minimised.</p>
<p>In the above example, we knew what the underlying function was as well as the residual variance. However, when modelling data generated in some real-world environment, we do not observe <span class="math inline">\(f\)</span> and therefore cannot explicitly compute the test MSE. In order to <em>estimate</em> the test MSE, we make use of model validation procedures.</p>
</div>
</div>
<div id="model-validation" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Model validation<a href="supervised-learning.html#model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine there are two students who have been subjected to the same set of lectures, notes, and homework exercises, which you can view as their training data used to learn the true subject knowledge. When studying for the test – which is designed to test this knowledge, i.e. the test set in our analogy – they take two different approaches: Student A, a model student, tries to master the subject matter by focusing on the course material, testing themself with new homework exercises after studying some completed ones first. Student B, however, managed to obtain a copy of the test in advance through some nefarious means, and plans to prove their knowledge of the subject matter by preparing only for this specific set of questions.</p>
<p>Even though student B’s test score will in all likelihood be better, does this mean that they have obtained and retained more knowledge? Certainly not! Suppose the lecturer catches wind of this cheating and swaps the initial test with a new set of randomised questions. Which of the two approaches would you expect to yield better results on such a test, on average?</p>
<p>When comparing different statistical models, we would like to select the one that we think will work best on unseen test data. But if we use the test data to make this decision, this will also be cheating, and we will be no better off for it. Like student A though, we can leave out some exercises in the training data and use these to <strong>validate</strong> our learning, i.e. gauge how well we would do in the test.</p>
<div id="validation-set" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Validation set<a href="supervised-learning.html#validation-set" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One way to create a validation set (or hold-out set) is to just leave aside, in a randomised way, a portion of the training data, say 30%. We then train models on the other 70% of the data only, test them on the validation set, and select the model that yields the lowest validation MSE, which serves as an estimate of test set performance.</p>
<p>Although there are some situations in which this approach is merited, it has two potential flaws:</p>
<ol style="list-style-type: decimal">
<li>Due to the single random split, the validation estimate of the test error can be highly variable.</li>
<li>Since we are reducing our training data, the model sees less information, generally leading to worse performance. Therefore, the validation error may overestimate the test error.</li>
</ol>
<p>We will not go into any more detail than this on the validation set approach, but rather focus on <strong>cross-validation (CV)</strong> strategies, which addresses these two issues.</p>
</div>
<div id="k-fold-cv" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> <span class="math inline">\(k\)</span>-fold CV<a href="supervised-learning.html#k-fold-cv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With this approach, the training set is randomly divided into <span class="math inline">\(k\)</span> groups, or folds, of (approximately) the same size. Each fold gets a turn to act as the validation set, with the model trained on the remaining <span class="math inline">\(k-1\)</span> folds. Therefore, the training process is repeated <span class="math inline">\(k\)</span> times, each yielding an estimate of the test error, denoted as <span class="math inline">\(MSE_1,\ MSE_2,\ldots,\ MSE_k\)</span>. These values are then averaged over all <span class="math inline">\(k\)</span> folds to yield the <strong><span class="math inline">\(k\)</span>-fold CV estimate</strong>:</p>
<p><span class="math display">\[CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i\]</span>
The next obvious question is: What value of <span class="math inline">\(k\)</span> should we choose? Start by considering the lowest value, <span class="math inline">\(k=2\)</span>. This would be the same as the validation set approach with a 50% split, except that each half of the data will get a chance to act as training and validation set. Therefore, we still expect the validation error to overestimate the test error, or in other words, there will be some bias. As we increase <span class="math inline">\(k\)</span>, the estimated error will become more unbiased, since each fold will allow the model to capture more of the underlying pattern.</p>
<p>However, just as with model complexity we also need to consider the variance aspect. Consider now the other extreme, when <span class="math inline">\(k = n\)</span> (the number of observations in the training set). Here we have what is referred to as <strong>Leave-one-out cross-validation (LOOCV)</strong>, since we have <span class="math inline">\(n\)</span> folds, each leaving out just one observation for validation. Each of these <span class="math inline">\(n\)</span> training sets will be almost identical, such that there will be very high correlation between them. Now, remember that when we add correlated random variables (note that averaging involves summation), then the correlation affects the resulting variance:</p>
<p><span class="math display">\[Var(X+Y) = \sigma^2_X + \sigma^2_Y + 2\frac{\rho_{XY}}{\sigma_X\sigma_Y}\]</span>
Therefore, larger <span class="math inline">\(k\)</span> implies larger variation of the estimated error. This means that the same bias-variance trade-off applies to <span class="math inline">\(k\)</span>-fold CV! In practice, it has been shown that <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(k = 10\)</span> yields a good balance such that the test error estimate does not suffer from excessively high bias nor variance.</p>
<p>Also note that as <span class="math inline">\(k\)</span> increases, the computational cost increases proportionally, since <span class="math inline">\(k\)</span> separate models must be fitted to <span class="math inline">\(k\)</span> different data splits. This could cause unnecessarily long training times for large datasets/complicated models, such that a smaller <span class="math inline">\(k\)</span> might be preferable.</p>
<p>To illustrate the implementation, let us return to the earlier example, where we will pretend that we do not know the underlying relationship we are trying to estimate.</p>
</div>
<div id="example-1-simulation-continued" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Example 1 – Simulation (continued)<a href="supervised-learning.html#example-1-simulation-continued" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before using the CV error to determine the ideal model complexity, let us first illustrate the concept of cross-validation for a single model, say a cubic spline with 8 degrees of freedom, with <span class="math inline">\(k = 10\)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="supervised-learning.html#cb7-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4026</span>)</span>
<span id="cb7-2"><a href="supervised-learning.html#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="supervised-learning.html#cb7-3" tabindex="-1"></a><span class="co">#Simulated data</span></span>
<span id="cb7-4"><a href="supervised-learning.html#cb7-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> </span>
<span id="cb7-5"><a href="supervised-learning.html#cb7-5" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span>  <span class="co">#We will apply 10-fold CV</span></span>
<span id="cb7-6"><a href="supervised-learning.html#cb7-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb7-7"><a href="supervised-learning.html#cb7-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">5</span><span class="sc">*</span>x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">2</span>))</span>
<span id="cb7-8"><a href="supervised-learning.html#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="supervised-learning.html#cb7-9" tabindex="-1"></a><span class="co">#Here we will collect the out-of-sample and in-sample errors</span></span>
<span id="cb7-10"><a href="supervised-learning.html#cb7-10" tabindex="-1"></a>cv_k <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb7-11"><a href="supervised-learning.html#cb7-11" tabindex="-1"></a>train_err <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb7-12"><a href="supervised-learning.html#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a href="supervised-learning.html#cb7-13" tabindex="-1"></a><span class="co"># We don&#39;t actually need to randomise, since x&#39;s are random, but one should in general</span></span>
<span id="cb7-14"><a href="supervised-learning.html#cb7-14" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(n) </span>
<span id="cb7-15"><a href="supervised-learning.html#cb7-15" tabindex="-1"></a>x <span class="ot">&lt;-</span> x[ind]</span>
<span id="cb7-16"><a href="supervised-learning.html#cb7-16" tabindex="-1"></a>y <span class="ot">&lt;-</span> y[ind]</span>
<span id="cb7-17"><a href="supervised-learning.html#cb7-17" tabindex="-1"></a></span>
<span id="cb7-18"><a href="supervised-learning.html#cb7-18" tabindex="-1"></a>folds <span class="ot">&lt;-</span> <span class="fu">cut</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="at">breaks =</span> k, <span class="at">labels =</span> F)   <span class="co">#Create indices for k folds</span></span>
<span id="cb7-19"><a href="supervised-learning.html#cb7-19" tabindex="-1"></a></span>
<span id="cb7-20"><a href="supervised-learning.html#cb7-20" tabindex="-1"></a><span class="co">#10-fold CV</span></span>
<span id="cb7-21"><a href="supervised-learning.html#cb7-21" tabindex="-1"></a><span class="cf">for</span>(fld <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb7-22"><a href="supervised-learning.html#cb7-22" tabindex="-1"></a>  x_train <span class="ot">&lt;-</span> x[folds <span class="sc">!=</span> fld]                    <span class="co">#Could streamline this code, (see next block)</span></span>
<span id="cb7-23"><a href="supervised-learning.html#cb7-23" tabindex="-1"></a>  x_valid <span class="ot">&lt;-</span> x[folds <span class="sc">==</span> fld]                    <span class="co">#but this is easier to follow   </span></span>
<span id="cb7-24"><a href="supervised-learning.html#cb7-24" tabindex="-1"></a>  y_train <span class="ot">&lt;-</span> y[folds <span class="sc">!=</span> fld]                    </span>
<span id="cb7-25"><a href="supervised-learning.html#cb7-25" tabindex="-1"></a>  y_valid <span class="ot">&lt;-</span> y[folds <span class="sc">==</span> fld]</span>
<span id="cb7-26"><a href="supervised-learning.html#cb7-26" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x_train, y_train, <span class="at">df =</span> <span class="dv">8</span>)</span>
<span id="cb7-27"><a href="supervised-learning.html#cb7-27" tabindex="-1"></a>  valid_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, x_valid)<span class="sc">$</span>y</span>
<span id="cb7-28"><a href="supervised-learning.html#cb7-28" tabindex="-1"></a>  train_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, x_train)<span class="sc">$</span>y</span>
<span id="cb7-29"><a href="supervised-learning.html#cb7-29" tabindex="-1"></a>  cv_k <span class="ot">&lt;-</span> <span class="fu">c</span>(cv_k, <span class="fu">mean</span>((valid_pred <span class="sc">-</span> y_valid)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb7-30"><a href="supervised-learning.html#cb7-30" tabindex="-1"></a>  train_err <span class="ot">&lt;-</span> <span class="fu">c</span>(train_err, <span class="fu">mean</span>((train_pred <span class="sc">-</span> y_train)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb7-31"><a href="supervised-learning.html#cb7-31" tabindex="-1"></a>  </span>
<span id="cb7-32"><a href="supervised-learning.html#cb7-32" tabindex="-1"></a>  <span class="co">#One should rather write the above into a function... </span></span>
<span id="cb7-33"><a href="supervised-learning.html#cb7-33" tabindex="-1"></a>  <span class="co">#But the plotting needs to be inside the loop for the notes&#39; rendering</span></span>
<span id="cb7-34"><a href="supervised-learning.html#cb7-34" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb7-35"><a href="supervised-learning.html#cb7-35" tabindex="-1"></a>  <span class="fu">plot</span>(x_train, y_train, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">xlab =</span> <span class="st">&#39;x&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;y&#39;</span>, </span>
<span id="cb7-36"><a href="supervised-learning.html#cb7-36" tabindex="-1"></a>       <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x)), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fu">min</span>(y), <span class="fu">max</span>(y)))</span>
<span id="cb7-37"><a href="supervised-learning.html#cb7-37" tabindex="-1"></a>  <span class="fu">points</span>(x_valid, y_valid, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&#39;gray&#39;</span>)</span>
<span id="cb7-38"><a href="supervised-learning.html#cb7-38" tabindex="-1"></a>  <span class="fu">segments</span>(x_valid, y_valid, x_valid, valid_pred,</span>
<span id="cb7-39"><a href="supervised-learning.html#cb7-39" tabindex="-1"></a>           <span class="at">col =</span> <span class="st">&#39;red&#39;</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb7-40"><a href="supervised-learning.html#cb7-40" tabindex="-1"></a>  <span class="fu">lines</span>(fit, <span class="at">col =</span> <span class="st">&#39;blue&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb7-41"><a href="supervised-learning.html#cb7-41" tabindex="-1"></a>  <span class="fu">title</span>(<span class="at">main =</span> <span class="fu">paste</span>(<span class="st">&#39;Fold:&#39;</span>, fld))</span>
<span id="cb7-42"><a href="supervised-learning.html#cb7-42" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&#39;bottomright&#39;</span>, <span class="fu">c</span>(<span class="st">&#39;Training&#39;</span>, <span class="st">&#39;Validation&#39;</span>, <span class="st">&#39;Errors&#39;</span>), </span>
<span id="cb7-43"><a href="supervised-learning.html#cb7-43" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;black&#39;</span>, <span class="st">&#39;gray&#39;</span>, <span class="st">&#39;red&#39;</span>), </span>
<span id="cb7-44"><a href="supervised-learning.html#cb7-44" tabindex="-1"></a>         <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="cn">NA</span>),</span>
<span id="cb7-45"><a href="supervised-learning.html#cb7-45" tabindex="-1"></a>         <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">3</span>),</span>
<span id="cb7-46"><a href="supervised-learning.html#cb7-46" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">2</span>))</span>
<span id="cb7-47"><a href="supervised-learning.html#cb7-47" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>fld, cv_k, <span class="st">&#39;b&#39;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&#39;red&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="st">&#39;Fold&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;MSE&#39;</span>, </span>
<span id="cb7-48"><a href="supervised-learning.html#cb7-48" tabindex="-1"></a>       <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">10</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">5.5</span>))</span>
<span id="cb7-49"><a href="supervised-learning.html#cb7-49" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>fld, train_err, <span class="st">&#39;b&#39;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb7-50"><a href="supervised-learning.html#cb7-50" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&#39;topright&#39;</span>, <span class="fu">c</span>(<span class="st">&#39;Training&#39;</span>, <span class="st">&#39;Validation&#39;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;black&#39;</span>, <span class="st">&#39;red&#39;</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb7-51"><a href="supervised-learning.html#cb7-51" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CV"></span>
<img src="figs/CV-.gif" alt="Left: The training (black) and validation (grey) portions of the dataset across 10 folds, with the fitted cubic spline with 8 degrees of freedom in blue. Right: The resulting training (black) and validation (red) MSEs across 10 folds." width="150%" />
<p class="caption">
Figure 2.3: Left: The training (black) and validation (grey) portions of the dataset across 10 folds, with the fitted cubic spline with 8 degrees of freedom in blue. Right: The resulting training (black) and validation (red) MSEs across 10 folds.
</p>
</div>
<p>Here we see that, as expected, the validation error is noticeably more variable than the training error across the folds. We can calculate<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> the CV MSE as 2.77, although on its own this value is not particularly insightful. When comparing it to that of other models, though, we can determine which model complexity is estimated to yield the lowest test error.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="supervised-learning.html#cb8-1" tabindex="-1"></a><span class="co">#Keep track of MSE per fold, per model</span></span>
<span id="cb8-2"><a href="supervised-learning.html#cb8-2" tabindex="-1"></a>fold_mses <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">10</span>, <span class="at">ncol =</span> <span class="fu">length</span>(dofs))</span>
<span id="cb8-3"><a href="supervised-learning.html#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="supervised-learning.html#cb8-4" tabindex="-1"></a><span class="cf">for</span>(fld <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb8-5"><a href="supervised-learning.html#cb8-5" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb8-6"><a href="supervised-learning.html#cb8-6" tabindex="-1"></a>  <span class="cf">for</span>(dof <span class="cf">in</span> dofs){ <span class="co">#Using the same dofs as earlier</span></span>
<span id="cb8-7"><a href="supervised-learning.html#cb8-7" tabindex="-1"></a>    d <span class="ot">&lt;-</span> d <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb8-8"><a href="supervised-learning.html#cb8-8" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x[folds <span class="sc">!=</span> fld], y[folds <span class="sc">!=</span> fld], <span class="at">df =</span> dof) </span>
<span id="cb8-9"><a href="supervised-learning.html#cb8-9" tabindex="-1"></a>    valid_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, x[folds <span class="sc">==</span> fld])<span class="sc">$</span>y</span>
<span id="cb8-10"><a href="supervised-learning.html#cb8-10" tabindex="-1"></a>    fold_mses[fld, d] <span class="ot">&lt;-</span> <span class="fu">mean</span>((valid_pred <span class="sc">-</span> y[folds <span class="sc">==</span> fld])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-11"><a href="supervised-learning.html#cb8-11" tabindex="-1"></a>  }</span>
<span id="cb8-12"><a href="supervised-learning.html#cb8-12" tabindex="-1"></a>}</span>
<span id="cb8-13"><a href="supervised-learning.html#cb8-13" tabindex="-1"></a><span class="co">#Average over all folds</span></span>
<span id="cb8-14"><a href="supervised-learning.html#cb8-14" tabindex="-1"></a>cv_mses <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(fold_mses)</span>
<span id="cb8-15"><a href="supervised-learning.html#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="supervised-learning.html#cb8-16" tabindex="-1"></a><span class="co"># Compare the true MSE from earlier</span></span>
<span id="cb8-17"><a href="supervised-learning.html#cb8-17" tabindex="-1"></a><span class="fu">plot</span>(dofs, results<span class="sc">$</span>MSE, <span class="st">&#39;l&#39;</span>, <span class="at">col =</span> <span class="st">&#39;darkred&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb8-18"><a href="supervised-learning.html#cb8-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&#39;Model complexity&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(cv_mses)))</span>
<span id="cb8-19"><a href="supervised-learning.html#cb8-19" tabindex="-1"></a><span class="fu">lines</span>(dofs, cv_mses, <span class="st">&#39;l&#39;</span>, <span class="at">col =</span> <span class="st">&#39;grey&#39;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb8-20"><a href="supervised-learning.html#cb8-20" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&#39;bottomright&#39;</span>, <span class="fu">c</span>(<span class="st">&#39;CV MSE&#39;</span>, <span class="st">&#39;True test MSE&#39;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;gray&#39;</span>, <span class="st">&#39;darkred&#39;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb8-21"><a href="supervised-learning.html#cb8-21" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> dofs[<span class="fu">which.min</span>(results<span class="sc">$</span>MSE)], <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb8-22"><a href="supervised-learning.html#cb8-22" tabindex="-1"></a><span class="fu">points</span>(dofs[<span class="fu">which.min</span>(cv_mses)], <span class="fu">min</span>(cv_mses), <span class="at">pch =</span> <span class="dv">13</span>, <span class="at">cex =</span> <span class="fl">2.5</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CV-min"></span>
<img src="figs/CV-min-1.png" alt="10-fold cross-validation error curve (grey) for cubic splines with varying degrees of freedom, with the minimum point indicated by the crossed circle. The red line indicates the true test MSE being estimated." width="672" />
<p class="caption">
Figure 2.4: 10-fold cross-validation error curve (grey) for cubic splines with varying degrees of freedom, with the minimum point indicated by the crossed circle. The red line indicates the true test MSE being estimated.
</p>
</div>
<p>Because we simulated these data, we know that the cubic spline yielding the lowest expected test MSE is one with 13 degrees of freedom. Applying 10-fold CV to our 100 training data points resulted in an estimated optimal model with 12 degrees of freedom, indicated by the crossed square in Figure <a href="supervised-learning.html#fig:CV-min">2.4</a>. It is interesting to note that the CV error consistently <em>overestimated</em> the true error. This is likely due to the relatively small dataset; remember that we only tested on 10 observations per fold! The shape of the true MSE curve was captured relatively well by the CV process in this example.</p>
<p>This section provided a succinct illustration of model validation. For a detailed discussion, see Section 5.1 of <span class="citation">James et al. (2013)</span>. In the following chapters we will move beyond simulated data and apply these methods to various datasets as we encounter different classes of models and other techniques. Although there is much value in coding the CV procedure from scratch, it is built into various R packages, which we will leverage going forward.</p>
</div>
</div>
<div id="side-note-statistical-learning-vs-machine-learning" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Side note: Statistical learning vs machine learning<a href="supervised-learning.html#side-note-statistical-learning-vs-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It may seem that we use the terms “statistical learning” and “machine learning” interchangeably, so is there a difference? The distinction between these two concepts can sometimes be blurred with the paradigms largely overlapping, and some might argue that the difference is mostly semantic.</p>
<p>In essence, statistical learning often focuses on understanding the probabilistic relationships between variables, while machine learning places greater emphasis on developing algorithms that can learn patterns directly from data, sometimes sacrificing interpretability for predictive performance. However, the principles discussed in this chapter form the core of both approaches – both are concerned with extracting insights from data and making predictions, although they may approach these goals with slightly different philosophical and methodological perspectives.</p>
<p>Statistical learning places a strong emphasis on understanding the underlying data-generating process and making inferences about population characteristics based on sample data. While understanding the underlying data-generating process is still important in machine learning, the focus is often more on achieving optimal predictive performance. Statistical learning approaches are also characterised by explicit assumptions about the underlying statistical distributions and relationships between variables, whereas machine learning methods often work in a more agnostic manner and may not rely heavily on explicit statistical assumptions.</p>
<p>For the purposes of our study throughout this course, these distinctions are not of consequence and we will adopt both perspectives throughout.</p>
</div>
<div id="homework-exercises" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Homework exercises<a href="supervised-learning.html#homework-exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>For Example 1, edit the provided code such that LOOCV is applied. Does this method suggest a different model complexity?</li>
<li>Now do the same for 5-fold CV. What changes in the CV curve do you observe as <span class="math inline">\(k\)</span> changes?</li>
</ol>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Often referred to as <strong>features</strong> in the learning context, or <strong>predictors</strong> in supervised learning specifically.<a href="supervised-learning.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Since there were 100 observations and 10 folds, each fold had an equal number of observations and we can calculate the CV MSE as the average of the 10 folds’ average errors. However, in cases where there are unequal numbers of observations across the folds, it would be more accurate to average over all of the individual observations’ squared errors.<a href="supervised-learning.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-model-selection-regularisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/SSBritz/STA4026S-Analytics-SecB/edit/main/02-SL.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/SSBritz/STA4026S-Analytics-SecB/blob/main/02-SL.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
